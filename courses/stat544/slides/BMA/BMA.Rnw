\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Bayesian model averaging}

\begin{document}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=6, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
# library(reshape2)
# library(plyr)
library(dplyr)
library(ggplot2)
library(xtable)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}



\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Posterior model probability
  \begin{itemize}
  \item 
  \end{itemize}
\item 
\end{itemize}

\end{frame}


\section{Bayesian Model Averaging}
\begin{frame}
\frametitle{Bayesian Model Averaging}

The posterior predictive distribution 
\[ 
p(\tilde{y}|y) = \int p(\tilde{y}|\theta) p(\theta|y) d\theta
\]
assumes there is a true model $p(y|\theta)$ and accounts for the uncertainty in 
$\theta$.

\vspace{0.1in} \pause

If you want to account for model uncertainty amongst some set of models 
$M_1,\ldots,M_h$, 
you can use the Bayesian model averaged posterior predictive distribution
\[ 
p(\tilde{y}|y) = \sum_{h=1}^H p(\tilde{y}|M_h, y) p(M_h|y)
\]
\pause
where 
\begin{itemize}[<+->]
\item $p(M_h|y)$ is the posterior model probability and 
\item $p(\tilde{y}|M_h,y)$ is the predictive distribution under model $M_h$. 
\end{itemize}

\end{frame}


\subsection{Normal example}
\begin{frame}
\frametitle{Normal example}

Suppose we have two models:
\[ \begin{array}{rl}
Y_i|M_0\phantom{,\mu} &\ind N(0,1) \\
Y_i|M_1,\mu &\ind N(\mu,1),\,\mu|M_1 \sim N(0,1) 
\end{array} \]

\vspace{0.1in} \pause

Thus, we have the following posterior predictive distributions \pause
\[ \begin{array}{rl}
\tilde{y}|y,M_0 &\sim N(0,1) \\
\tilde{y}|y,M_1 &\sim N\!\left(n\overline{y}[n+1]^{-1},[n+1]^{-1}+1\right)
\end{array} \]

\pause
and the following posterior model probabilities: \pause
\[ \begin{array}{rl}
p(M_0|y) &\propto N(\overline{y};0,1/n)p(M_0) \\
p(M_1|y) &\propto N(\overline{y};0,2/n)p(M_0) \\
\end{array} \]

\end{frame}



\begin{frame}
<<>>=
n <- 10
ybar_max <- 3

post_predictive <- function(d) {
  ybar <- d$ybar
  
  # Posterior model probabilities
  pM0t <- dnorm(ybar, 0, sqrt(1/n))
  pM1t <- dnorm(ybar, 0, sqrt(2/n))
  pM0 <- pM0t/(pM0t+pM1t)
  pM1 <- 1-pM0
  
  # Posterior predictive distributions
  x <- seq(-3, ybar_max+3, by=0.1)
  ppM0 <- dnorm(x, 0, 1)
  ppM1 <- dnorm(x, n*ybar/(n+1), sqrt(1/(n+1)))
  
  data.frame(x = x,
             density = pM0*ppM0 + pM1*ppM1)
}

d <- data.frame(ybar = 0:ybar_max) %>%
  group_by(ybar) %>%
  do(post_predictive(.)) %>%
  ungroup() %>%
  mutate(ybar = factor(ybar))

ggplot(d, aes(x = x, y = density, color = ybar, linetype = ybar, group = ybar)) +
  geom_line() + 
  theme_bw() +
  labs(title = paste0("Posterior predictive distribution (n=",n,")"))
@
\end{frame}







\subsection{AIC/BIC model averaging}
\begin{frame}
\frametitle{AIC/BIC model averaging}

The generic structure for model averaging is 
\[ 
p(\tilde{y}|y) = \sum_{h=1}^H p(\tilde{y}|M_h, y) w_h
\]
where $w_h$ is the \alert{weight} for model $h$. 

\vspace{0.1in} \pause

Here are some possible weights:
\begin{itemize}
\item Bayesian model averaging: $w_h = p(M_h|y)$ 
\item AIC model averaging: $w_h = e^{-\Delta_h/2}$ where $\Delta_h = AIC_h - \min AIC$
\item AICc model averaging: $w_h = e^{-\Delta_h/2}$ where $\Delta_h = AICc_h - \min AICc$
\item BIC model averaging: $w_h = e^{-\Delta_h/2}$ where $\Delta_h = BIC_h - \min BIC$
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Information criterion}

Recall that information criteria have the form:
\[ 
IC = -2 \log L(\hat\theta) + P
\]
where $P$ is a penalty.
\pause 
So if you take 
\[ 
w_h = e^{-\Delta_h/2} = e^{-(IC_h - \min IC)/2} \propto 
e^{-IC_h/2} = L_h(\hat\theta)e^{P}.
\]

where, if $p$ is the number of parameters, 
\pause 
the penalty $P$ is 
\begin{itemize}
\item AIC: $2p$
\item AICc: $2p + 2p(p+1)/(n-p-1)$
\item BIC: $p \log(n)$ 
\end{itemize}

\pause

The BIC is a large sample approximation to the marginal likelihood:
\[
-2 \log p(y) \approx -2 \log p(y|\theta) + p \log(n) + C
\]


\end{frame}



\subsection{Regression BMA}
\begin{frame}
\frametitle{Regression BMA}

A common place to perform Bayesian Model Averaging is in the
regression framework:
\[
y \sim N(X_\gamma \beta_\gamma, \sigma_{\gamma}^2\I)
\]
where $\gamma$ is a vector indicator of what explanatory variables are included 
in the model, e.g. 
\[ 
\gamma = (1,1,0,\ldots,0,1,0).
\]


\end{frame}



\begin{frame}[fragile]
\frametitle{BIC model averaging in R}

<<bic_ma, echo=TRUE>>=
library(BMA)
library(MASS)
data(UScrime)
x<- UScrime[,-16]
y<- log(UScrime[,16])
x[,-2]<- log(x[,-2])
lma<- bicreg(x, y, strict = FALSE, OR = 20) 
@
\end{frame}

\begin{frame}[fragile]
<<summary_lma, dependson="bic_ma", echo=TRUE>>=
summary(lma)
@
\end{frame}

\begin{frame}[fragile]
<<imageplot_lma, dependson="bic_ma", echo=TRUE>>=
imageplot.bma(lma)
@
\end{frame}




\subsection{}



\section{Posterior model probability}
\begin{frame}
\frametitle{Posterior model probability}

Consider a set of models 
\[ M_1,\ldots,M_K \] 
where we believe one of these models to be true \pause and that each $M_k$ corresponds to a prior predictive distribution 
\[ p(y|M_k) = \int p(y|\theta,M_k) p(\theta|M_k) d\theta. \]
\pause
The uncertainty here is which model is true \pause and thus we assign a \alert{prior model probability}
\[ p(M_k) \quad \mbox{s.t.} \quad \sum_{k=1}^K p(M_k)= 1. \]
\pause 
Then we obtain the \alert{posterior model probability} via Bayes' rule
\[ p(M_k|y) = \frac{p(y|M_k)p(M_k)}{p(y)} \pause = \frac{p(y|M_k)p(M_k)}{\sum_{j=1}^K p(y|M_j)p(M_j)} \pause \propto p(y|M_k)p(M_k).\]
\end{frame}


\begin{frame}

\frametitle{Binomial model example}

Suppose $Y\sim Bin(n,\theta)$ and we have models $M_k: \theta = (k-1)/10$ for $k=1,\ldots,K=11$. \pause We assume the prior model probability is 
\[ p(M_k) = p_k. \]
\pause 
If we observe $y$ successes out of $n$ attempts, then our posterior model probability is 
\[ p(M_k|y) \propto {n\choose y} \left( \frac{k-1}{10} \right)^y \left( 1 - \frac{k-1}{10} \right)^{n-y} p_k. \]
and the normalizing constant is 
\[ p(y) = \sum_{k=1}^K {n\choose y} \left( \frac{k-1}{10} \right)^y \left( 1 - \frac{k-1}{10} \right)^{n-y} p_k. \]
\end{frame}


\begin{frame}[fragile]
<<>>=
y = 3; n = 10
theta = seq(0,1,by=0.1)
prior = rep(1/length(theta),length(theta)) # uniform over the models
posterior = dbinom(y,n,theta)*prior
posterior = posterior/sum(posterior)
ggplot(rbind(data.frame(belief = "prior", probability=prior, theta=theta),
             data.frame(belief = "posterior", probability=posterior, theta=theta)),
       aes(x=theta, y=probability, color=belief)) + 
  geom_point() + 
  labs(title=paste(y,"successes out of",n,"attempts with uniform prior."))
@
\end{frame}


\begin{frame}
\frametitle{Prior predictive distributions}
Recall that a prior predictive distribution is the statistical model integrated over the prior distribution, i.e. 
\[ p(y|M_k) = \int p(y|\theta)p(\theta|M_k) d\theta \]
\pause
evaluation of this pdf (or pmf) at the observed data $y$ is the \alert{marginal likelihood}.

\vspace{0.2in}

Thus, we can write the posterior model probability as 
\[ p(M_k|y) \propto p(y|M_k) p(M_k) = \left[ \int p(y|\theta)p(\theta|M_k) d\theta \right] p(M_k). \]
\pause
Recall that making $p(\theta|M_k)$ too diffuse will cause $p(y|\theta)$ to decrease.
\end{frame}


\begin{frame}
\frametitle{Binomial model example}
Suppose $Y\sim Bin(n,\theta)$ and $\theta \sim Be(a,b)$, then 
\[ p(y) = \int p(y|\theta) p(\theta) d\theta  = {n\choose y}\frac{B(a+y,b+n-y)}{B(a,b)},\]
\pause
i.e. a \href{http://en.wikipedia.org/wiki/Beta-binomial_distribution}{beta-binomial distribution}.

\vspace{0.2in} \pause

If we take $a=b\to 0$, we have $p(y)\to 0$ for $y\ne 0$ and $y\ne n$. 
\end{frame}

\begin{frame}[fragile]
<<>>=
beta_binomial = function(y,n,a,b) exp(lchoose(n,y)+lbeta(a+y,b+n-y)-lbeta(a,b))
d = ddply(expand.grid(y=0:(n/2),a=10^seq(-5,0)), 
          .(y,a), 
          function(x) data.frame(prior_predictive = with(x, beta_binomial(y,n,a,a))))
ggplot(d, aes(x=a,y=prior_predictive, color=factor(y))) + 
  geom_line() +
  scale_x_log10()
@
\end{frame}



\begin{frame}
\frametitle{Impact of diffuse priors on posterior model probabilities}

Suppose $Y\sim Bin(n,\theta)$ and we consider the two models $M_0: \theta=0.5$ and $M_1: \theta\ne 0.5$. \pause Under $M_1$, we choose $\theta\sim Be(a,b)$. \pause Then the prior predictive distributions are 
\[ \begin{array}{rl}
p(y|M_0) &= {n\choose y} 0.5^y(1-0.5)^{n-y} = {n\choose y} 0.5^n \\
p(y|M_1) &= {n\choose y} \frac{B(a+y,b+n-y)}{B(a,b)} \\
\end{array} \]
\pause
Thus the posterior model probability for $M_1$ is
\[ p(M_1|y) = \frac{p(y|M_1)p(M_1)}{p(y|M_0)p(M_0) + p(y|M_1)p(M_1)} = \frac{\frac{B(a+y,b+n-y)}{B(a,b)}}{\frac{B(a+y,b+n-y)}{B(a,b)}+0.5^n},\]
\pause
For a fixed $y\ne 0$ and $y\ne n$, $p(M_1|y)\to 0$ as $a=b\to 0$. 

\end{frame}


\section{Bayesian model averaging}

\begin{frame}
\frametitle{Bayesian model averaging}

Bayesian model averaging accounts for model uncertainty. \pause Suppose $\Delta$ is an unknown quantity of interest, e.g. prediction, effect size, utility, etc. \pause Then 
\[ p(\Delta|y) = \sum_{k=1}^K p(\Delta|M_k,y) p(M_k|y) \]
\pause 
where $M_1,\ldots,M_K$ are the models under consideration. \pause This is just a weighted average of the posterior distributions for the quantity $\Delta$ under each model, $p(\Delta|M_k,y)$, weighted by the posterior probability of the models.

\end{frame}



\begin{frame}[fragile]
\frametitle{Binomial model with discrete models for $\theta$}

Let $Y\sim Bin(n,\theta)$ and $M_k:\theta=[k-1]/10$ for $k=1,\ldots,K=11$. \pause Suppose we are interested in predicting the next observation $\tilde{Y}\sim Ber(\theta)$ (conditionally independent of the previous observations). \pause Then 
\[ p(\tilde{y}|y) = \sum_{k=1}^K p(\tilde{y}|M_k,y) p(M_k|y) \]
\pause 
in particular,
\[ \begin{array}{rl}
P(\tilde{Y}=1|y) &= \sum_{k=1}^K P(\tilde{Y}=1|M_k,y) \times P(M_k\mbox{ TRUE}|y) \\
&= \sum_{k=1}^K \frac{k-1}{10} \times \frac{{n\choose y} \left(\frac{k-1}{10}\right)^{y} \left(1-\frac{k-1}{10}\right)^{n-y}}{\sum_{j=1}^K {n\choose y} \left(\frac{j-1}{10}\right)^{y} \left(1-\frac{j-1}{10}\right)^{n-y}}. 
\end{array} \]
\pause
<<echo=TRUE>>=
c(y,n); sum(posterior*theta)
@

\end{frame}



\begin{frame}
\frametitle{Binomial model with discrete and continuous models for $\theta$}

Let $Y\sim Bin(n,\theta)$ and $M_0:\theta=0.5$ and $M_1:\theta \sim Be(a,b)$. \pause Again, we are interested in predicting the next observation 
$\tilde{Y}\sim Ber(\theta)$ (conditionally independent of the previous observations). \pause Then
\[ p(\tilde{y}|y) = p(\tilde{y}|M_0,y) p(M_0|y) + p(\tilde{y}|M_1,y) p(M_1|y) \]
where $p(\tilde{y}|M_1,y)$ is the posterior predictive distribution under model $M_1$, i.e. 
\[ p(\tilde{y}|M_1,y) = \int p(\tilde{y}|\theta)p(\theta|M_1,y) d\theta = \frac{B(a+y+\tilde{y},b+n-y+1-\tilde{y})}{B(a+y,b+n-y)}. \]
\pause
Thus {\tiny
\[ P(\tilde{Y}=1|y) = 0.5 \times \frac{{n\choose y} 0.5^n}{{n\choose y} 0.5^n+{n\choose y} \frac{B(a+y,b+n-y)}{B(a,b)}} + \frac{B(a+y+1,b+n-y)}{B(a+y,b+n-y)} \times \frac{{n\choose y} \frac{B(a+y,b+n-y)}{B(a,b)}}{{n\choose y} 0.5^n+{n\choose y} \frac{B(a+y,b+n-y)}{B(a,b)}} \]
}
\end{frame}


\subsection{Normal model}
\begin{frame}
\frametitle{Normal model with discrete and continuous models for $\theta$}

Let $Y_i\stackrel{ind}{\sim} N(\theta,1)$ \pause and $M_0:\theta=0$ and $M_1:\theta\sim N(0,1)$. \pause Again, we are interested in predicting the next observation $\tilde{Y} \sim N(\theta,1)$ (conditionally independent of the first observations). Then 
\[ p(\tilde{y}|y) = p(\tilde{y}|M_0,y)p(M_0|y) + p(\tilde{y}|M_1,y)p(M_1|y)\]
\pause
where the posterior model probabilities are
\[ \begin{array}{rl}
p(M_0|y) \propto \left[ \prod_{i=1}^n N(y_i;0,1) \right] p(M_0|y) \\
p(M_1|y) \propto \left[ \prod_{i=1}^n N(y_i;0,2) \right] p(M_1|y) 
\end{array} \]
\pause
and the posterior predictive distributions are 
\[ \begin{array}{rl}
\tilde{y}|M_0,y &\sim N(0,1) \\
\tilde{y}|M_1,y & \sim N([1+n]^{-1}[n\overline{y}],[1+n]^{-1}+1) 
\end{array} \]

\end{frame}



\begin{frame}[fragile]
\frametitle{}

<<echo=TRUE>>=
n = 10
y = rnorm(n); y - mean(y)
prior_model_probability = c(0.5,0.5)
log_likelihood = c(sum(dnorm(y,log=TRUE)), sum(dnorm(y,0,sqrt(2),log=TRUE)))
posterior_model_probability = prior_model_probability + log_likelihood


# d = ddply(data.frame(theta=c(0,2), .(theta), function(x) {
#   
# })
@

\end{frame}




\end{document}
