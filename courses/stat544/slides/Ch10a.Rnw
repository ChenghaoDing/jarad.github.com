\documentclass[handout]{beamer}


\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title{Introduction to Bayesian computation}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\I}{\mathrm{I}}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=6, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
@

<<set_seed, echo=FALSE>>=
set.seed(1)
@

\frame{\maketitle}


\begin{frame}
\frametitle{Outline}

Bayesian computation
\begin{itemize}
\item Goal: $p(y) = \int p(y|\theta)p(\theta) d\theta$ or $E[h(\theta)|y] = \int h(\theta) p(\theta|y) d\theta$. 
\item Deterministic approximation
\item Monte Carlo approximation
  \begin{itemize}
  \item Gridding
  \item Accept-reject
  \item Importance sampling
  \end{itemize}
\end{itemize}

\end{frame}


\section{Introduction to Bayesian computation}
\frame{\frametitle{Notation}
  \begin{itemize}
  \item Target distribution: $p(\theta|y)$
  \item Unnormalized target distribution: $q(\theta|y)$, i.e.
  \[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \propto q(\theta|y) \]
  \item Proposal distribution: $g(\theta)$ which may depend on $y$
  \end{itemize}
}



\subsection{Example: Normal-Cauchy model}
\frame{\frametitle{Example: Normal-Cauchy model}
  Let $Y\sim N(\theta,1)$ with $\theta\sim Ca(0,1)$. \pause The posterior is 
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2} \]
  \pause which is not a known distribution. \pause We might be interested in
  \begin{enumerate}
  \item  normalizing this posterior, i.e. calculating
  \[ p(y) = \int p(y|\theta)p(\theta) d\theta \]
  \item or in calculating the posterior mean, i.e. 
  \[ E[\theta|y] = \int \theta p(\theta|y) d\theta. \]
  \end{enumerate}
}


\subsection{Numerical integration}
\frame{\frametitle{Numerical integration}
  \begin{itemize}[<+->]
  \item Simulation methods:
  \[ E[h(\theta)|y] = \int h(\theta)p(\theta|y) d\theta \approx \frac{1}{S} \sum_{S=1}^S h\left(\theta^{(s)}\right) \]
  where 
    \begin{itemize}
    \item $\theta^{(s)} \stackrel{iid}{\sim} p(\theta|y)$ \pause 
    \item and we have SLLN and CLT.
    \end{itemize}
  \item Deterministic methods:
  \[ E[h(\theta)|y] = \int h(\theta)p(\theta|y) d\theta \approx \sum_{S=1}^S w_s h\left(\theta^{(s)}\right)p\left(\left.\theta^{(s)}\right|y\right) \]
  where 
    \begin{itemize}
    \item $\theta^{(s)}$ are selected points, 
    \item $w_s$ is the weight given to the point $\theta^{(s)}$, and
    \item the error can be bounded. 
    \end{itemize}
	\end{itemize}
}


\begin{frame}[fragile]
\frametitle{Marginal likelihood }
<<normal_cauchy_posterior, echo=TRUE>>=
y = 1 # Data

q = function(theta,y,log=FALSE) {
  out = -(y-theta)^2/2-log(1+theta^2)
  if (log) return(out)
  return(exp(out))
}

# Find marginal likelihood for y
w = 0.1
theta = seq(-5,5,by=w)+y
(py = sum(q(theta,y)*w))
integrate(function(x) q(x,y), -Inf, Inf)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<normal_cauchy_posterior_plot, echo=TRUE, fig.width=8>>=
curve(q(x,y), -5, 5, n=1001)
points(theta,rep(0,length(theta)), cex=0.5, pch=19)
segments(theta,0,theta,q(theta,y))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior expectation }
\[ E[h(\theta)|y) \approx \sum_{s=1}^S w_s h\left(\theta^{(s)}\right)p\left(\left.\theta^{(s)}\right|y\right) = \sum_{s=1}^S w_s h\left(\theta^{(s)}\right) \frac{p\left(\left.\theta^{(s)}\right|y\right)}{p(y)} \]
<<normal_cauchy_posterior_expectation, echo=TRUE>>=
h = function(theta) theta
sum(w*h(theta)*q(theta,y)/py)
@

\end{frame}



<<normal_cauchy_posterior_mean, echo=FALSE>>=
# Calculate marginal likelihood, p(y)
py = function(y,log=FALSE) {
  int = integrate(function(x) q(x,y,log=FALSE),-Inf,Inf)
  if (int$message!="OK") {
    warning(paste("Could not compute marginal likelihood for y=", y, "\n"))
    return(NA)
  }
  int$value 
}

# Find posterior expectation when y=0
post_expectation = function(y) {
  py = py(y)
  int = integrate(function(x) x*q(x,y)/py, -Inf, Inf)
    if (int$message!="OK") {
    warning(paste("Could not compute posterior expectation for y=", y, "\n"))
    return(NA)
  }
  int$value
}

res = ddply(data.frame(y=seq(-5,5, by=0.1)), 
            .(y), 
            function(x) { data.frame(Cauchy = post_expectation(x$y))})
res = mutate(res, normal = y/2, "improper uniform" = y)
@




\begin{frame}[fragile]
\frametitle{Posterior expectation as a function of observed data}
<<normal_cauchy_posterior_mean_plot, fig.width=8>>=
ggplot(melt(res, id.var='y', variable.name = "prior"),
       aes(y,value,color=prior))+
  geom_line() + 
  labs(y="Posterior expectation")
@
\end{frame}


\subsection{Gridding}
\frame{\frametitle{Monte Carlo approximation via gridding}
  Rather than approximate $p(y)$ and then $E[\theta|y]$ via deterministic gridding (all $w_i$ are equal), we can use the grid as a discrete approximation to the posterior, i.e. 
  \[ p(\theta|y) \approx \sum_{i=1}^N p_i \delta_{\theta_i}(\theta) \qquad p_i = \frac{q(\theta_i|y)}{\sum_{j=1}^N q(\theta_j|y)}  \]
  \pause where $\delta_{\theta_i}(\theta)$ is the Dirac delta function, i.e. 
  \[ \delta_{\theta_i}(\theta) = 0 \,\forall\, \theta\ne \theta_i \qquad \int \delta_{\theta_i}(\theta) d\theta=1. \]
\pause  This discrete approximation to $p(\theta|y)$ can be used to approximate the expectation $E[h(\theta)|y]$ deterministically or via simulation, \pause i.e. 
  \[ E[h(\theta)|y]\approx \sum_{i=1}^N p_i h(\theta_i) \pause \qquad E[h(\theta)|y]\approx \frac{1}{S} \sum_{s=1}^S h\left(\theta^{(s)}\right) \] 
\pause  where $\theta^{(s)} \sim \sum_{i=1}^N p_i \delta_{\theta_i}(\theta)$ (with replacement).
}

\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model }
<<normal_cauchy_posterior_mean_grid, echo=TRUE>>=
# Small number of grid locations
theta = seq(-5,5,length=1e2+1)+y; p = q(theta,y)/sum(q(theta,y)); sum(p*theta)
mean(sample(theta,prob=p,replace=TRUE))

# Large number of grid locations
theta = seq(-5,5,length=1e6+1)+y; p = q(theta,y)/sum(q(theta,y)); sum(p*theta) 
mean(sample(theta,1e2,prob=p,replace=TRUE)) # But small MC sample

# Truth
post_expectation(1)
@
\end{frame}



\subsection{Inverse CDF method}
\frame{\frametitle{Inverse cumulative distribution function}
  \begin{definition}
	The \alert{cumulative distribution function} of a random variable $X$ is defined by 
	\[ F_X(x) = P_X(X\le x) \qquad\mbox{for all }x. \]
	\end{definition}
	
	\vspace{0.2in} \pause
	
  Suppose you want to sample $X\sim f(x)$ and you have access to the inverse cdf of X, $F^{-1}(x)$, then 
	\begin{lemma}
	If $U\sim Unif(0,1)$, then $X=F^{-1}(U)$ is a simulation from $f(x)$. 	\end{lemma}
}


\begin{frame}[fragile]
\frametitle{Inverse CDF}
<<>>=
curve(pnorm, -3, 3, lwd=2, main="Standard normal CDF")

plot_arrows = function(u, ...) {
  arrows(-100, u, qnorm(u,...), u, col='blue')
  arrows(qnorm(u,...), u, qnorm(u,...), 0, col='blue') 
}

for (u in c(.1,.95)) plot_arrows(u)

@
\end{frame}



\begin{frame}[fragile]
\frametitle{Exponential example}
	For example, to sample $X\sim Exp(1)$, 
	\begin{enumerate}
	\item Sample $U\sim Unif(0,1)$. \pause
	\item Set $X = -\log(1-U)$, \pause or $X=-\log(U)$.  
	\end{enumerate}

\pause

<<exponential, message=FALSE, fig.width=8>>=
n.reps = 1e4
d = data.frame(x = -log(runif(n.reps)))
ggplot(d, aes(x=x))+geom_histogram(aes(y=..density..))+stat_function(fun=dexp, color="red")
@
\end{frame}



\frame{\frametitle{Sampling from a univariate truncated distribution}
	Suppose you wish to sample from $X\sim N(\mu,\sigma^2)\mathrm{I}(a<X<b)$, i.e. a normal random variable with untruncated mean $\mu$ and variance $\sigma^2$, but truncated to the interval $(a,b)$. \pause Suppose the untruncated cdf is $F$ and inverse cdf is $F^{-1}$.  \pause 
	
	\begin{enumerate}[\,1.]
	\item Calculate endpoints $p_a = F(a)$ and $p_b = F(b)$. \pause
	\item Sample $U\sim Unif(p_a,p_b)$. \pause
	\item Set $X=\mathrm{F}^{-1}(U)$. \pause
	\end{enumerate}	
  
  \vspace{0.2in} \pause
  
  This just avoids having to recalculate the normalizing constant for the pdf, i.e. $1/(F^{-1}(b)-F^{-1}(a))$, which rescales 
}


\begin{frame}[fragile]
\frametitle{Normal}
$X\sim N(5,9)\mathrm{I}(1\le X \le 6)$
<<>>=
mean = 5; sd = 9
curve(pnorm(x,mean,sd), mean-2*sd, mean+2*sd, lwd=2, main="N(5,9^2) cdf")

plot_segments = function(e) {
  pe = pnorm(e, mean, sd)
  segments(-100, pe, e, pe, col='red')
  segments(e, pe, e, 0, col='red') 
}

for (endpoint in c(1,6)) plot_segments(endpoint)
plot_arrows(.5, mean=mean, sd=sd)
@
\end{frame}




\begin{frame}[fragile]
$X\sim N(5,9)\mathrm{I}(1\le X \le 6)$
<<truncated_normal, message=FALSE, fig.width=8>>=
d = data.frame(x= 5+3*qnorm(runif(n.reps, pnorm((1-5)/3), pnorm((6-5)/3))))
dtnorm = function(x,mu=5,sigma=3,a=1,b=6) {
  dnorm(x,mu,sigma)/diff(pnorm(c(a,b),mu,sigma))
} 
ggplot(d, aes(x=x))+geom_histogram(aes(y=..density..))+stat_function(fun=dtnorm, color="red")
@
\end{frame}






\subsection{Rejection sampling}
\frame{\frametitle{Rejection sampling}
  Suppose you wish to obtain samples $\theta\sim p(\theta|y)$, \pause rejection sampling performs the following \pause 
  \begin{enumerate}[\,1.]
	\item Sample a proposal $\theta^*\sim g(\theta)$ \pause and $U\sim Unif(0,1)$. \pause 
	\item Accept $\theta=\theta^*$ as a draw from $p(\theta|y)$ if $U\le p(\theta^*|y)/Mg(\theta^*)$\pause, otherwise return to step 1. \pause
	\end{enumerate}
	where $M$ satisfies $M\, g(\theta)\ge p(\theta|y)$ for all $\theta$. 
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item For a given proposal distribution $g(\theta)$, the optimal $M$ is $M=\sup_\theta p(\theta|y)/g(\theta)$. 
	\item The probability of acceptance is $1/M$.
	\end{itemize}
  \pause The accept-reject idea is to create an envelope, $M\, g(\theta)$, above $p(\theta|y)$.
}


\frame{\frametitle{Rejection sampling with unnormalized density}
  Suppose you wish to obtain samples $\theta\sim p(\theta|y)\propto q(\theta|y)$, \pause rejection sampling performs the following  
  \begin{enumerate}[\,1.]
	\item Sample a proposal $\theta^*\sim g(\theta)$ \pause and $U\sim Unif(0,1)$.  
	\item Accept $\theta=\theta^*$ as a draw from $p(\theta|y)$ if $U\le q(\theta^*|y)/M^*g(\theta^*)$, otherwise return to step 1. 
	\end{enumerate}
	where $M^*$ satisfies $M^*\, g(\theta)\ge q(\theta|y)$ for all $\theta$. 
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item For a given proposal distribution $g(\theta)$, the optimal $M^*$ is $M*=\sup_\theta q(\theta|y)/g(\theta)$. 
  \item The acceptance probability is $1/M = p(y)/M^*$.
	\end{itemize}
  \pause The accept-reject idea is to create an envelope, $M\, g(\theta)$, above $q(\theta|y)$.
}

\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  If $Y \sim N(\theta,1)$ and $\theta\sim Ca(0,1)$, then 
  \[ p(\theta|y) \propto e^{-(y-\theta)^2/2} \frac{1}{(1+\theta^2)} \]
  for $\theta\in\mathbb{R}$. 
  
  \vspace{0.2in} \pause 
  
  Choose a $N(y,1)$ as a proposal distribution, i.e. 
  \[ g(\theta)=\frac{1}{\sqrt{2\pi}} e^{-(\theta-y)^2/2} \]
  with 
  \[ M^* = \sup_\theta \frac{q(\theta|y)}{g(\theta)} \pause 
  = \sup_\theta \frac{e^{-(y-\theta)^2/2} \frac{1}{(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}  e^{-(\theta-y)^2/2} } \pause 
  = \frac{\sqrt{2\pi}}{(1+\theta^2)} \le \sqrt{2\pi} \]
  The acceptance rate is $1/M = p(y)/M^* = 1.643545 / \sqrt{2\pi} = 0.656$.
\end{frame}



\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<rejection_sampling_plot, echo=FALSE, fig.width=8>>=
M = sqrt(2*pi)
g = function(theta,y,log=FALSE) dnorm(theta,y,log=log)

n = 1e2
d = data.frame(x = rnorm(n), u = runif(n))
d$u_scaled = d$u*M*g(d$x,0)
d$accept = d$u_scaled < q(d$x,0)

xx = seq(-3,3,by=0.01)
dd = data.frame(x=xx,
                y=c(M*g(xx,0),q(xx,0)),
                distribution = c(rep("envelope",length(xx)),
                                 rep("target",length(xx))))

gg = ggplot(d, aes(x=x,y=u_scaled,col=accept)) +geom_point()
clrs = unique(ggplot_build(gg)$data[[1]]$colour)
gg + stat_function(fun=function(x) M*g(x,0), col=clrs[2]) +
  stat_function(fun=function(x) q(x,0), col=clrs[1]) + labs(x="sample",y=expression(paste("u M g(",theta,")")))
cat("Acceptance rate was",mean(d$accept))
@
\end{frame}


\frame{\frametitle{Heavy-tailed proposals}
  Suppose our target is a standard Cauchy and our (proposed) proposal is a standard normal, then
  \[ \frac{p(\theta|y)}{g(\theta)} = 
  \frac{\frac{1}{\pi(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}}  \]
  \pause and 
  \[
  \frac{\frac{1}{\pi(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}} \pause 
  \stackrel{\theta\to\infty}{\longrightarrow} \infty  \]
  since $e^{-a}$ converges to zero faster than $1/(1+a)$. 
  \pause Thus, there is no value $M$ such that $M \, g(\theta) \ge p(\theta|y)$ for all $\theta$.
  
  \vspace{0.2in} \pause
  
  Bottom line: the condition $M\, g(\theta)\ge p(\theta|y)$ requires the proposal to have tails at least as thick (heavy) as the target.
}


\end{document}
