\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Model checking}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\I}{\mathrm{I}}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=6, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
library(rstan)
library(GGally)
@

<<set_seed, echo=FALSE>>=
set.seed(1)
@



\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

We assume $p(y|\theta)$ and $p(\theta)$, so it would be prudent to determine if these assumptions are reasonable. 

\vspace{0.2in} \pause

\begin{itemize}
\item (Prior) sensitivity analysis 
\item Posterior predictive checks
  \begin{itemize}
  \item Graphical checks
  \item Posterior predictive pvalues
  \end{itemize}
\end{itemize}
\end{frame}




\section{Prior sensitivity analysis}
\begin{frame}
\frametitle{Prior sensitivity analysis}

Since a prior specifies our prior belief, we may want to check to determine whether our conclusions would change it we held different prior beliefs. \pause Suppose a particular scientific question can be boiled down to 
\[ Y_i \stackrel{ind}{\sim} Ber(\theta) \]
\pause 
and that there is wide disagreement about the value for $\theta$ such that following might actually characterize different individual beliefs before the experiment is run: \pause
\begin{itemize}
\item Skeptic: $\theta\sim Be(1,100)$ 
\item Agnostic: $\theta\sim Be(1,1)$
\item Believer: $\theta\sim Be(100,1)$
\end{itemize}
\pause
An experiment is run and the posterior under these different priors are compared. 

\end{frame}


\begin{frame}
\frametitle{Ten data points}

<<>>=
n = 10
xx = seq(0,1,by=0.01)
d = ddply(data.frame(a=c(1,1,100), b=c(100,1,1), 
                     type=factor(c("skeptic","agnostic","believer"),
                                 c("skeptic","agnostic","believer"))), 
          .(type), function(x) {
  data.frame(xx=xx, yy=dbeta(xx, x$a+.9*n, x$b+.1*n))
})

ggplot(d, aes(x=xx,y=yy,col=type)) + geom_line() +
  labs(x=expression(theta), y=expression(paste("p(",theta,"|y)")))
@
\end{frame}


\begin{frame}
\frametitle{A million data points}

<<>>=
n = 1000000
xx = seq(0.89,.91,by=0.0001)
d = ddply(data.frame(a=c(1,1,100), b=c(100,1,1), 
                     type=factor(c("skeptic","agnostic","believer"),
                                 c("skeptic","agnostic","believer"))), 
          .(type), function(x) {
  data.frame(xx=xx, yy=dbeta(xx, x$a+.9*n, x$b+.1*n))
})

ggplot(d, aes(x=xx,y=yy,col=type)) + geom_line() +
  labs(x=expression(theta), y=expression(paste("p(",theta,"|y)")))
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Hierarchical variance prior}

Recall the normal hierarchical model
\[ y_i \stackrel{ind}{\sim} N(\theta_i,s_i^2), \quad \theta_i\stackrel{ind}{\sim} N(\mu,\tau^2) \]
which results in the posterior distribution for $\tau^2$ of 
\[ p(\tau|y) \propto p(\tau) V_\mu^{1/2} \prod_{i=1}^\I (s_i^2+\tau^2)^{-1/2} \exp\left( -\frac{(y_{i}-\hat{\mu})^2}{2(s_i^2+\tau^2)} \right) \]

 \pause

As an attempt to be non-informative, consider an $IG(\epsilon,\epsilon)$ prior for $\tau^2$. \pause As an alternative consider $\tau \sim Unif(0,C)$ or $\tau\sim Ca^+(0,C)$ where $C$ problem specific, but is chosen to be relatively large for the particular problem. 

 \pause

The 8-schools example has the following data:

<<results='asis'>>=
d = data.frame(y = c(28,  8, -3,  7, -1,  1, 18, 12),
               s = c(15, 10, 16, 11,  9, 11, 10, 18))
print(xtable(t(d)))
@


\end{frame}

\begin{frame}
<<analysis, echo=FALSE, fig.width=8, warning=FALSE>>=
dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
dsqrtinvgamma = function(x, a, b) dinvgamma(x^2, a, b)*2*x

tau_log_posterior = function(tau, y, si2) 
{
  spt = si2+tau^2
  Vmu = 1/sum(1/spt)
  mu = sum(y/spt)*Vmu
  0.5*log(Vmu)+sum(-0.5*log(spt)-(y-mu)^2/(2*spt))
}

V_tau_log_posterior = Vectorize(tau_log_posterior,"tau")



h = .001
tmp =data.frame(x = seq(0,30-2*h, by=2*h)+h)
post = rbind(mutate(tmp, 
                    prior = 1/30, 
                    f = exp(V_tau_log_posterior(x, d$y, d$s^2)), 
                    prior_type="uniform"),
             mutate(tmp, 
                    prior = dsqrtinvgamma(x, 1, 1), 
                    f= exp(V_tau_log_posterior(x, d$y, d$s^2) + log(dsqrtinvgamma(x, 1, 1))), 
                    prior_type="IG(1,1)"),
             mutate(tmp, 
                    prior = dsqrtinvgamma(x, .001, .001),
                    f = exp(V_tau_log_posterior(x, d$y, d$s^2) + log(dsqrtinvgamma(x, .001, .001 ))), 
                    prior_type="IG(.001,001)"))
post2 = ddply(post, .(prior_type), function(d) 
  data.frame(x=d$x, posterior = d$f/(sum(d$f)*2*h), prior=d$prior/(sum(d$prior)*2*h))
)
m = melt(post2, id.var=c("prior_type","x"))
names(m) = c("prior","x","density","y")
m$density = factor(m$density, c("prior","posterior"))
ggplot(m, aes(x=x, y=y, color=prior)) + geom_line() + 
  labs(x=expression(tau), y='density') + 
  xlim(0,10) + facet_wrap(~density)
@
\end{frame}



\section{Posterior predictive checks}
\begin{frame}
\frametitle{Posterior predictive checks}
  Let $y^{rep}$ be a replication of $y$, \pause then 
  \[ p(y^{rep}|y) = \int p(y^{rep}|\theta,y) p(\theta|y) d\theta \pause = \int p(y^{rep}|\theta) p(\theta|y) d\theta. \]
  \pause where $y$ is the observed data and $\theta$ are the model parameters.
  
  \vspace{0.2in} \pause
  
  To simulate a full replication:
  \begin{enumerate}[<+->]
  \item Simulate $\theta^{(j)}\sim p(\theta|y)$ and 
  \item Simulate $y^{rep,j} \sim p(y|\theta^{(j)})$.
  \end{enumerate}
  
  \vspace{0.2in} \pause 
  
  To assess model adequacy:
  \begin{itemize}
  \item Compare plots of replicated data to the observed data. \pause
  \item Calculate posterior predictive pvalues.
  \end{itemize}
\end{frame}



\subsection{Airline example{}}
\begin{frame}[fragile]
\frametitle{Airline accident data}
  Let 
  \begin{itemize}
  \item $y_i$ be the number of fatal accidents in year $i$
  \item $x_i$ be the number of 100 million miles flown in year $i$
  \end{itemize}
  
  Consider the model 
  \[ Y_i \stackrel{ind}{\sim} Po(x_i\lambda) \qquad p(\lambda)\propto 1/\sqrt{\lambda}. \]
  
\footnotesize
<<airline_accident_data, results='asis'>>=
d = data.frame(year=1976:1985, 
              fatal_accidents = c(24,25,31,31,22,21,26,20,16,22),
              passenger_deaths = c(734,516,754,877,814,362,764,809,223,1066),
              death_rate = c(0.19,0.12,0.15,0.16,0.14,0.06,0.13,0.13,0.03,0.15))
d$miles_flown = d$passenger_deaths/d$death_rate # 100 million miles
print(xtable(d, digits=0, include.rownames=FALSE))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Posterior replications of the data}
  Under Jeffreys prior, the posterior is 
  \[ \lambda|y \sim Ga(0.5+n\overline{y}, n\overline{x}). \]
  So 
  \begin{enumerate}
  \item $\lambda^{(j)} \sim Ga(0.5+n\overline{y}, n\overline{x})$ and 
  \item $y^{rep,j}_i \stackrel{ind}{\sim} Po(x_i\lambda^{(j)})$ for $i=1,\ldots,n$.
  \end{enumerate}

<<airline_fatalities>>=
# Jeffreys prior
a = 0.5 + sum(d$fatal_accidents)
b = 0   + sum(d$miles_flown)

# Replicate data
rep = rdply(19, { 
  tmp=d
  lambda = rgamma(1,a,b)
  tmp$fatal_accidents = rpois(nrow(tmp), lambda*tmp$miles_flown)
  tmp 
})
id = sample(20,1) # Choose a random spot to insert observed data set
if (id==20) {
  d$.n = 20
  rep = rbind(rep,d)
} else {
  rep$.n[rep$.n==id] = 20
  d$.n = id
  rep = rbind(rep,d) 
}
@
\end{frame}



\begin{frame}[fragile]
<<histograms, message=FALSE>>=
ggplot(rep, aes(x=fatal_accidents, fill=factor(1))) + 
  geom_histogram(binwidth=1) +
  facet_wrap(~.n) +
  theme(legend.position="none")
@
\end{frame}



\begin{frame}[fragile]
<<histograms_observed, message=FALSE>>=
ggplot(rep, aes(x=fatal_accidents, fill=.n==id)) + 
  geom_histogram(binwidth=1) +
  facet_wrap(~.n) +
  theme(legend.position="none")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{How might this model not accurately represent the data?}
  Let 
  \begin{itemize}
  \item $y_i$ be the number of fatal accidents in year $i$
  \item $x_i$ be the number of 100 million miles flown in year $i$
  \end{itemize}
  
  Consider the model 
  \[ Y_i \stackrel{ind}{\sim} Po(x_i\lambda) \qquad p(\lambda)\propto 1/\sqrt{\lambda}. \]
  
\footnotesize
<<results='asis'>>=
print(xtable(d, digits=0, include.rownames=FALSE))
@
\end{frame}



\begin{frame}[fragile]
<<scatterplot>>=
g = ggplot(rep, aes(x=year, y=fatal_accidents, col=factor(1))) + 
  geom_point() + 
  facet_wrap(~.n) +
  theme(legend.position="none")
g
@
\end{frame}


\begin{frame}[fragile]
<<scatterplot_with_regression_line>>=
g = g + stat_smooth(method="lm", formula=y~x)
g
@
\end{frame}


\begin{frame}[fragile]
<<scatterplot_with_observed>>=
g + aes(col=.n==id)
@
\end{frame}



\subsection{Posterior predictive pvalues}
\begin{frame}
\frametitle{Posterior predictive pvalues}
  To quantify the discrepancy between observed and replicated data: \pause
  \begin{enumerate}
  \item Define a test statistic $T(y,\theta)$. \pause
  \item Define the posterior predictive pvalue
  \[ p_B = P(T(y^{rep},\theta)\ge T(y,\theta)|y) \]
  \pause where $y^{rep}$ and $\theta$ are treated as random. \pause Typically this pvalue is calculated via simulation, \pause i.e. 
  \[ \begin{array}{rl}
  p_B
  &= E_{y^{rep},\theta}[\mathrm{I}(T(y^{rep},\theta)\ge T(y,\theta)) p(y^{rep}|\theta) p(\theta|y)|y] \pause \\
  &= \int \int \mathrm{I}(T(y^{rep},\theta)\ge T(y,\theta)) p(y^{rep}|\theta) p(\theta|y) dy^{rep} d\theta \pause \\
  &\approx \frac{1}{J} \sum_{j=1}^J \mathrm{I}(T(y^{rep,j},\theta^{(j)})\ge T(y,\theta^{(j)})) 
  \end{array} \]
  \pause where $\theta^{(j)} \sim p(\theta|y)$ and $y^{rep,j} \sim p(y|\theta^{(r)})$. 
   
  \end{enumerate}
  \pause Large or small pvalues are (possible) cause for concern. 
\end{frame}






\begin{frame}[fragile]
\frametitle{Posterior predictive pvalue for slope}
  Let 
  \[ Y_i^{obs} = \beta_0^{obs}+\beta_1^{obs} i \]
  where 
  \begin{itemize}
  \item $Y_i^{obs}$ is the observed number of fatal accidents in year $i$ \pause and 
  \item $\beta_1^{obs}$ be the test statistic. 
  \end{itemize}
  
  \vspace{0.2in} \pause 

  Now, generate replicate data $y^{rep}$ and fit 
  \[ Y_i^{rep} = \beta_0^{rep} + \beta_1^{rep} i.\]
  \pause
  Now compare $\beta_1^{obs}$ to the distribution of $\beta_1^{rep}$.

<<slope, cache=TRUE>>=
rep = rdply(1000, { 
  tmp=d
  lambda = rgamma(1,a,b)
  tmp$fatal_accidents = rpois(nrow(d), lambda*d$miles_flown)
  data.frame(beta1 = coefficients(lm(fatal_accidents~year, tmp))[2])
})
observed_slope = coefficients(lm(fatal_accidents~year,d))[2]
@
\end{frame}



\begin{frame}[fragile]
<<slope_plot, dependson="slope", fig.width=8>>=
mean(rep$beta1<observed_slope) 
ggplot(rep, aes(x=beta1)) + geom_histogram(binwidth=0.1) + geom_vline(xintercept=observed_slope, color="red")
@
\end{frame}


\subsection{Update model}
\begin{frame}
\frametitle{Consider a linear model for the $\lambda_i$}
  Consider the model 
  \[ \begin{array}{rl}
  Y_i &\stackrel{ind}{\sim} Po(x_i \lambda_i) \pause  \\
  \log(\lambda_i) &= \beta_0+\beta_1 i
  \end{array} \]
  \pause where
  \begin{itemize}[<+->]
  \item $Y_i$ is the number of fatal accidents in year $i$
  \item $x_i$ is the number of 100 million miles flown in year $i$
  \item $\lambda_i$ is the accident rate in year $i$
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Here the $\lambda_i$ are a deterministic function of year, but (possibly) different each year.
\end{frame}




\begin{frame}[fragile]
\frametitle{Stan linear model for accident rate}
<<stan, echo=TRUE, results='hide', cache=TRUE>>=
model = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  vector<lower=0>[n] x;
}
transformed data {
  vector[n] logx;
  logx <- log(x); # both x and logx need to be vectors 
}
parameters {
  real beta[2];
}
transformed parameters {
  vector[n] lambda;
  for (i in 1:n) lambda[i] <- beta[1] + beta[2]*i;
}
model {
  y ~ poisson_log(logx + lambda); # _log indicates mean on log scale
}
"

m = stan_model(model_code = model)
r = sampling(m, list(n=nrow(d), y=d$fatal_accidents, x=d$miles_flown))
@
\end{frame}

\begin{frame}[fragile]
<<stan_posteriors, dependson='stan'>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior predictive pvalue: slope}
<<stan_rep, dependson='stan', cache=TRUE>>=
lambda = extract(r, "lambda")[["lambda"]]
reps = adply(lambda, 1, function(a) {
  d = data.frame(yrep = rpois(nrow(d), d$miles_flown * exp(a)),
             year = 1:10)
})

rep_slopes = daply(reps, .(iterations), function(d) {
  slopes=coefficients(lm(yrep~year, d))[2]
})
@

<<echo=TRUE, dependson='stan_rep'>>=
# Posterior predictive pvalue: slope
mean(rep_slopes>observed_slope)
@

<<fig.width=8, dependson='stan_rep'>>=
ggplot(data.frame(slopes=rep_slopes), aes(x=slopes)) + 
  geom_histogram(binwidth=0.1) + 
  geom_vline(xintercept = observed_slope, color='red')
@
\end{frame}




\end{document}
