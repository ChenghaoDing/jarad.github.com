\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Multiparameter models}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\titlepage}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Independent beta-binomial
  \begin{itemize}
  \item Independent posteriors
  \item Comparison of parameters
  \item JAGS
  \end{itemize}
\item Probability theory results
  \begin{itemize}
  \item Scaled Inv-$\chi^2$ distribution
  \item $t$-distribution
  \item Normal-Inv$\chi^2$ distribution
  \end{itemize}
\item Normal model with unknown mean and variance
  \begin{itemize}
  \item Jeffreys prior
  \item Natural conjugate prior
  \end{itemize}
\item Theoretical justification for simulation
  \begin{itemize}
  \item Strong Law of Large Numbers
  \item Central limit theorem
  \end{itemize}
\end{itemize}
\end{frame}






\section{Independent binomials}
\subsection{3-point success in basketball}
\frame{\frametitle{Motivating example}
  	Is Andre Dawkins 3-point percentage higher in 2013-2014 than past years?
		
		\vspace{0.2in} \pause
		
		\begin{center}
		\begin{tabular}{lrr}
		Season & Made & Attempts \\
		\hline
		2009-2010 & 36 & 95  \\
		2010-2011 & 64 & 150 \\
    2011-2012 & 67 & 171 \\
  	2013-2014 & 64 & 152 \\
		\hline
		\end{tabular}
		\end{center}
}

\subsection{Binomial model}
\frame{\frametitle{Binomial model}
	Assume an independent binomial model,
	\[ Y_s \stackrel{ind}{\sim} Bin(n_s,\theta_s), \uncover<6->{\mbox{ i.e. },\,    p(y|\theta) = \prod_{s=1}^S p(y_s|\theta_s) = \prod_{s=1}^S  {n_s\choose y_s} \theta_s^{y_s} (1-\theta_s)^{n_s-y_s}}  \]
	\pause where 
	\begin{itemize}
	\item $y_s$ is the number of 3-pointers made in season $s$ \pause
	\item $n_s$ is the number of 3-pointers attempted in season $s$ \pause 
	\item $\theta_s$ is the unknown 3-pointer success probability in season $s$ \pause
	\item $S$ is the number of seasons \pause \pause
	\item $\theta = (\theta_1,\theta_2,\theta_3,\theta_4)'$ and $y=(y_1,y_2,y_3,y_4)$
	\end{itemize}
	\pause and assume independent beta priors distribution:
	\[ p(\theta) = \prod_{s=1}^S p(\theta_s) \pause = \prod_{s=1}^S \frac{\theta_s^{a _s-1}(1-\theta_s)^{b _s-1}}{B(a _s,b _s)} \mathrm{I}(0<\theta_s<1).  \]
}

\frame{\frametitle{Joint posterior}
	Derive the posterior according to Bayes rule: \pause 
	\[ \begin{array}{ll}
	p(\theta|y) &\pause \propto p(y|\theta)p(\theta) \pause \\
	&= \prod_{s=1}^S p(y_s|\theta_s) \prod_{s=1}^S p(\theta_s)  \pause \\
	&= \prod_{s=1}^S p(y_s|\theta_s) p(\theta_s) \pause \\
	&\propto \prod_{s=1}^S \mbox{Beta}(\theta_s|a _s+y_s, b _s+n_s-y_s) \pause 
	\end{array} \]
	So the posterior for each $\theta_s$ is exactly the same as if we treated each season independently. 
}

\begin{frame}[fragile]
\frametitle{Joint posterior}
<<joint_posterior>>=
a = b = 1
d = data.frame(year=1:4, y=c(36,64,67,64), n=c(95,150,171,152))
d$a = a + d$y
d$b = b + d$n - d$y
plot(0, 0, type="n", main="Andre Dawkins's 3-point percentage", xlab=expression(theta), ylab="Posterior",
     xlim=c(0,1), ylim=c(0,max(dbeta(d$y/d$n,d$a,d$b))))
for (i in 1:nrow(d)) curve(dbeta(x, d$a[i], d$b[i]), col=i, lwd=2, lty=i, add=TRUE)
legend("topright", paste("Season", 1:nrow(d)), col=1:4, lwd=2, lty=1:4)
@
\end{frame}

\section{Monte Carlo estimates}
\begin{frame}[fragile]
\frametitle{Monte Carlo estimates}
Estimated means, medians, and quantiles.
<<theta_sims>>=
sim = ddply(d, .(year), 
            function(x) data.frame(theta=rbeta(1e3, x$a, x$b),
                                   a = x$a, b = x$b))

# hpd 
hpd = function(theta,a,b,p=.95) {
  h = dbeta((a-1)/(a+b-2),a,b)
  ftheta = dbeta(theta,a,b)
  r = uniroot(function(x) mean(ftheta>x)-p,c(0,h))
  range(theta[which(ftheta>r$root)])
}

# expectations
ddply(sim, .(year), summarize,
      mean = mean(theta),
      median = median(theta),
      ciL  = quantile(theta, c(.025,.975))[1],
      ciU  = quantile(theta, c(.025,.975))[2],
      hpdL = hpd(theta,a[1],b[1])[1],
      hpdU = hpd(theta,a[1],b[1])[2])
@
\end{frame}



\frame{\frametitle{Comparing probabilities across years}
	The scientific question of interest here is whether Dawkins's 3-point percentage is higher in 2013-2014 than previously. \pause In probability notation this is 
	\[ P(\theta_4>\theta_s|y)\mbox{ for } s=1,2,3.\]
	\pause which can be approximated via Monte Carlo as 
	\[ P(\theta_4>\theta_s|y) = E_{\theta|y}[\mathrm{I}(\theta_4>\theta_s)]\approx \frac{1}{J} \sum_{j=1}^J \mathrm{I}\left(\theta_4^{(j)} > \theta_s^{(j)}\right) \]
	\pause where
	\begin{itemize}
	\item $\theta_s^{(j)} \stackrel{ind}{\sim} Be(a_s + y_s,b_s+n_s-y_s)$ \pause 
	\item $\mathrm{I}(A)$ is in indicator function that is 1 if $A$ is true and zero otherwise. 
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Estimated probabilities}
<<theta_results, dependson='theta_sim', echo=TRUE>>=
# Should be able to use dcast
d = data.frame(theta_1 = sim$theta[sim$year==1],
               theta_2 = sim$theta[sim$year==2],
               theta_3 = sim$theta[sim$year==3],
               theta_4 = sim$theta[sim$year==4])

# Probabilities that season 4 percentage is higher than other seasons
mean(d$theta_4 > d$theta_1)
mean(d$theta_4 > d$theta_2)
mean(d$theta_4 > d$theta_3)
@
\end{frame}


\subsection{JAGS}
\begin{frame}[fragile]
\frametitle{Using JAGS}
<<jags, echo=TRUE>>=
library(rjags)
independent_binomials = "
model {
  for (i in 1:N) { 
    y[i] ~ dbin(theta[i],n[i]) 
    theta[i] ~ dbeta(1,1)
  }
}
"

d = list(y=c(36,64,67,64), n=c(95,150,171,152), N=4)
m = jags.model(textConnection(independent_binomials), d)
res = coda.samples(m, "theta", 1000)
@
\end{frame}

\begin{frame}[fragile]
<<jags_summary, dependson='jags', echo=TRUE>>=
summary(res)
@
\end{frame}


\begin{frame}[fragile]
<<jags_results, dependson='jags', echo=TRUE>>=
# Extract sampled theta values
theta = as.matrix(res[[1]]) # with only 1 chain, all values are in the first list element

# Calculate probabilities that season 4 percentage is higher than other seasons
mean(theta[,4] > theta[,1])
mean(theta[,4] > theta[,2])
mean(theta[,4] > theta[,3])
@
\end{frame}






\section{Probability theory}
\begin{frame}
\frametitle{Background probability theory}

\begin{itemize}
\item Scaled Inv-$\chi^2$ distribution
\item Location-scale $t$-distribution
\item Normal-Inv-$\chi^2$ distribution
\end{itemize}

\end{frame}


\subsection{Scaled-inverse $\chi$-square}
\begin{frame}
\frametitle{Scaled-inverse $\chi^2$-distribution}
  If $\sigma^2\sim IG(a,b)$, then $\sigma^2\sim \mbox{Inv-}\chi^2(v, s^2)$ \pause with 
  \begin{itemize}[<+->]
  \item $a=v/2$ and $b=vs^2/2$, \pause or, equivalently,
  \item $v=2a$ and $s^2=b/a$. 
  \end{itemize}
   
  \vspace{0.2in} \pause 
  
  Deriving from the inverse gamma, the scaled-inverse $\chi^2$ has 
  \begin{itemize}[<+->]
  \item Mean: $vs^2/(v-2)$ for $v>2$
  \item Mode: $vs^2/(v+2)$
  \item Variance: $2v^2(s^2)^2/[(v-2)^2(v-4)]$ for $v>4$
  \end{itemize}
   
  \vspace{0.2in} \pause 
  
  So $s^2$ is a point estimate and $v\to \infty$ means the variance decreases, since, for large $v$, 
  \[ \frac{2v^2(s^2)^2}{(v-2)^2(v-4)} \approx \frac{2v^2(s^2)^2}{v^3} = \frac{2(s^2)^2}{v}.   \]
\end{frame}


\begin{frame}[fragile]
\frametitle{Scaled-inverse $\chi^2$-distribution}
<<scaled_inverse_chi_square, echo=TRUE>>=
dinvgamma = function(x, a, b, ...) dgamma(1/x, a, b,...)/x^2
dsichisq = function(x, v, s2, ...) dinvgamma(x, v/2, v*s2/2, ...)
@

<<scaled_inverse_chi_square_plot, dependson='scaled_inverse_chi_square', out.width='0.7\\textwidth'>>=
d = ddply(expand.grid(v=c(10,5,1), s2=c(1,2,3), x=seq(.01,5,.01)), .(v,s2,x), function(x) {
  data.frame(density = dsichisq(x$x,x$v,x$s2))
})

d$v_f = as.factor(d$v)
d$s2_f = as.factor(d$s2)
levels(d$s2_f) = paste('s2=',levels(d$s2_f))

ggplot(d, aes(x,density, color=v_f, linetype=v_f, group=v_f)) + 
  geom_line() + 
  facet_grid(s2_f~.)
@
\end{frame}



\subsection{$t$-distribution}
\begin{frame}
\frametitle{Location-scale $t$-distribution}
  The $t$-distribution is a location-scale family (Casella \& Berger Thm 3.5.6), \pause i.e. if $T_v$ has a standard $t$-distribution with $v$ degrees of freedom and pdf
  \[ f_t(t) = \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}}\left(1+t^2/v\right)^{-(v+1)/2}, \]
  \pause then $X=m+sT_v$ has pdf 
  \[ f_X(x) = f_t([x-m]/s)/s \pause = \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}s}\left(1+\frac{1}{v}\left[\frac{x-m}{s}\right]^2\right)^{-(v+1)/2}. \]
  \pause This is referred to as a $t$ distribution with $v$ degrees of freedom, location $m$, and scale $s$; it is written as $t_{v}(m,s^2)$. \pause Also,
\[ 
t_v(m,s^2) \stackrel{v\to\infty}{\longrightarrow} N(m,s^2).
\]
\end{frame}



\begin{frame}[fragile]
\frametitle{$t$ distribution as $v$ changes}
<<t_distribution, fig.height=4.5, warning=FALSE>>=
dtms = function(x,v,m=0,s=1) dt((x-m)/s,v)/s
#opar = par(mar=c(5,4,0,0)+.1, mfrow=c(1,2))

d = ddply(data.frame(v=c(1,10,100)), .(v), function(x) {
  data.frame(x = seq(-3,6,by=.1)) %>%
    mutate(density = dtms(x, x$v))
})
d$v = factor(d$v)

ggplot(d, aes(x=x,y=density, group=v, linetype=v, color=v)) +
  geom_line() + 
  theme(legend.position='bottom')
@
\end{frame}






\begin{frame}
\frametitle{Normal-Inv-$\chi^2$ distribution}

Let $\mu|\sigma^2 \sim N(m,\sigma^2/k)$ and $\sigma^2 \sim \mbox{Inv-}\chi^2(v,s^2)$, \pause then the kernel of this joint density is

\[ \begin{array}{rl}
p(\mu,\sigma^2) &= p(\mu|\sigma^2)p(\sigma^2) \pause \\
&\propto (\sigma^2)^{-1/2} e^{-\frac{1}{2\sigma^2/k}(\mu-m)^2} (\sigma^2)^{-\frac{v}{2}-1} e^{-\frac{vs^2}{2\sigma^2}} \pause \\
&= (\sigma^2)^{-(v+3)/2} e^{-\frac{1}{2\sigma^2}\left[ k(\mu-m)^2 + vs^2 \right]}
\end{array} \]

\vspace{0.2in} \pause

In addition, the marginal distribution for $\mu$ is 
  \[ \begin{array}{rl}
  p(\mu) &= \int p(\mu|\sigma^2) p(\sigma^2) d\sigma^2 \pause \\
  &= \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}s/\sqrt{k}}\left(1+\frac{1}{v}\left[\frac{\mu-m}{s/\sqrt{k}}\right]^2\right)^{-(v+1)/2}. \pause 
  \end{array} \]
Thus $\mu \sim t_{v}(m, s^2/k)$. 
  
\end{frame}




\section{Univariate normal}
\begin{frame}[fragile]
\frametitle{Univariate normal model}
Suppose $Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2)$. 

<<normal_model>>=
curve(dnorm,-3,3,lwd=2, axes=F, frame=TRUE, 
      xlab="y", ylab=expression(p(y~"|"~mu,sigma^2)), main="Normal model")
abline(v=0, lty=2)
text(0,0,expression(mu),pos=4)
x = 1
arrows(-x,dnorm(-x),x,dnorm(x),code=3, lty=2)
text(x/2,dnorm(x), expression(sigma^2), pos=3)
@
\end{frame}

\subsection{Confidence interval}
\begin{frame}
\frametitle{Confidence interval for $\mu$}
  Let \pause 
  \[ \overline{y} = \frac{1}{n} \sum_{i=1}^n y_i \qquad \mbox{and} \qquad S^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i-\overline{y})^2.\]
  
  \pause Then, 
  \[ T = \frac{\overline{Y}-\mu}{S/\sqrt{n}} \pause \sim t_{n-1} \]
  
  \pause and an equal-tail $100(1-\alpha)$\% confidence interval can be constructed via 
  
  \[ 1-\alpha = P\left(-c \le T\le c\right) = P\left( \overline{Y} -\frac{cS}{\sqrt{n}} \le \mu \le \overline{Y} +\frac{cS}{\sqrt{n}} \right) \]
  
  \pause and thus $\overline{y} \pm c S/\sqrt{n}$ is an equal-tail 95\% confidence interval where $c=t_{n-1}(1-\alpha/2)$ is the t-critical value.
\end{frame}
  

\subsection{Priors}
\begin{frame}
\frametitle{Default priors}

Jeffreys prior can be shown to be $p(\mu,\sigma^2) \propto (1/\sigma^2)^{3/2}$. \pause But alternative methods, e.g. reference prior, find that $p(\mu,\sigma^2) \propto 1/\sigma^2$ is a more appropriate prior. 

\vspace{0.2in} \pause

The posterior under the reference prior is 
\[ \begin{array}{rl}
p(\mu,\sigma^2|y) &\propto (\sigma^2)^{-n/2}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i\phantom{{}-\overline{y}{}+\overline{y}}-\mu)^2 \right) \times \frac{1}{\sigma^2} \pause \\
&= (\sigma^2)^{-n/2}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\overline{y}+\overline{y}-\mu)^2 \right) \times \frac{1}{\sigma^2} \pause \\
& \vdots \\
&= (\sigma^2)^{-(n-1+3)/2} \exp\left( -\frac{1}{2\sigma^2} \left[ n(\mu-\overline{y})^2 + (n-1) S^2 \right]\right)  
\end{array} \]
\pause Thus
\[ \mu|\sigma^2,y \sim N(\overline{y}, \sigma^2/n) \qquad \sigma^2 \sim \mbox{Inv-}\chi^2(n-1,S^2). \]
\end{frame}


\begin{frame}
\frametitle{Marginal posterior for $\mu$}
  The marginal posterior for $\mu$ is 
  \[ \mu|y \sim t_{n-1}(\overline{y}, S^2/n). \]
  
  \vspace{0.2in} \pause
  
  An equal-tailed $100(1-\alpha)$\% credible inteval can be obtained via 
  \[ \overline{y} \pm c S/\sqrt{n} \]
  where $c = t_{n-1}(1-\alpha)$ is the t-critical value from before. \pause This is exactly the same as the $100(1-\alpha)$\% confidence inteval.
\end{frame}


\begin{frame}
\frametitle{Predictive distribution}
  Let $\tilde{y} \sim N(\mu,\sigma^2)$. \pause The predictive distribution is 
  \[ \int \int p(\tilde{y}|\mu,\sigma^2)p(\mu|\sigma^2,y) p(\sigma^2|y) d\mu d\sigma^2 \]
  \pause
  The easiest way to derive this is to write $\tilde{y} = \mu + \epsilon$ with 
  \[ \mu|\sigma^2 \sim N(\overline{y}, \sigma^2/n) \qquad \epsilon|\sigma^2 \sim N(0,\sigma^2) \]
  independent of each other. \pause Thus
  \[ \tilde{y}|\sigma^2 \sim N(\overline{y}, \sigma^2[1+1/n]). \]
  \pause
  with $\sigma^2 \sim \mbox{Inv-}\chi^2(n-1,S^2)$. \pause Now, we can use the Normal-Inv-$\chi^2$ theory, to find that 
  \[ \tilde{y} \sim t_{n-1}(\overline{y}, S^2[1+1/n]). \]
\end{frame}



\begin{frame}
\frametitle{Conjugate prior for $\mu$ and $\sigma^2$}
  The joint conjugate prior for $\mu$ and $\sigma^2$ is 
  \[ \mu|\sigma^2\phantom{,y} \sim N(m,\sigma^2/k) \qquad \sigma^2\phantom{,y} \sim \mbox{Inv-}\chi^2(v,s^2) \]
  \pause where $s^2$ serves as a prior guess about $\sigma^2$ and $v$ controls how certain we are about that guess. 
  
  \vspace{0.2in} \pause 

  The posterior under this prior is 
  \[ \mu|\sigma^2,y \sim N(m',\sigma^2/k') \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v',(s')^2) \]
  \pause where 
  \[ \begin{array}{rl}
  k' &= k+n \\
  m' &= [km + n\overline{y}]/k' \\
  v' &= v+n \\
  v'(s')^2 &= vs^2 + (n-1)S^2 + \frac{kn}{k'}(\overline{y}-m)^2
  \end{array} \]
\end{frame}



\begin{frame}
\frametitle{Marginal posterior for $\mu$}
  The marginal posterior for $\mu$ is 
  \[ \mu|y \sim t_{v'}(m', (s')^2/k'). \]
  
  \vspace{0.2in} \pause
  
  An equal-tailed $100(1-\alpha)$\% credible inteval can be obtained via 
  \[ m' \pm c s'/\sqrt{k'} \]
  where $c = t_{v'}(1-\alpha)$ is the t-critical value from before. 
\end{frame}



\begin{frame}
\frametitle{Marginal posterior via simulation}

An alternative to deriving the closed form posterior for $\mu$ is to simulate from the distribution. \pause Recall that 
\[  \mu|\sigma^2,y \sim N(m',\sigma^2/k') \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v',(s')^2) \]
\pause
To obtain a simulation from the posterior distribution $p(\mu,\sigma^2|y)$, do the following
\begin{enumerate}[<+->]
\item Calculate $m', k', v',$ and $s'$. 
\item Simulate $\sigma^2 \sim \mbox{Inv-}\chi^2(v',(s')^2)$.
\item Using this value, simulate $\mu\sim  N(m',\sigma^2/k')$. 
\end{enumerate}
\pause
Not only does this provide a sample from the joint distribution for $\mu,\sigma$ but it also (therefore) provides a sample from the marginal distribution for $\mu$. \pause The integral was suggestive:
\[  p(\mu|y) = \int p(\mu|\sigma^2,y) p(\sigma^2|y) d\sigma^2 \]
\end{frame}



\begin{frame}
\frametitle{Predictive distribution via simulation}

Similarly, we can obtain the predictive distribution via simulation. \pause Recall that 
\[  p(\tilde{y}|y) = \int \int p(\tilde{y}|\mu,\sigma^2) p(\mu|\sigma^2,y) p(\sigma^2|y) d\mu d\sigma^2 \]
\pause
To obtain a simulation from the predictive distribution $p(\tilde{y}|y)$, do the following
\begin{enumerate}[<+->]
\item Calculate $m', k', v',$ and $s'$. 
\item Simulate $\sigma^2 \sim \mbox{Inv-}\chi^2(v',(s')^2)$.
\item Using this value, simulate $\mu\sim  N(m',\sigma^2/k')$.
\item Using $\mu$ and $\sigma^2$ from above, simulate $\tilde{y}\sim N(\mu,\sigma^2)$.
\end{enumerate}
\end{frame}




\begin{frame}
\frametitle{Summary of normal inference}
\begin{itemize}
\item Default analysis
  \begin{itemize}
  \item Prior: (think $\mu\sim N(0,\infty)$ and $\sigma^2 \sim \mbox{Inv-}\chi^2(0,0)$)
  \[ p(\mu,\sigma^2) \propto 1/\sigma^2 \] 
  \item Posterior:
  \[ \mu|\sigma^2,y \sim N(\overline{y}, \sigma^2/n) ,\,  \sigma^2|y \sim  \mbox{Inv-}\chi^2(n-1,S^2) ,\,  \mu|y \sim t_{n-1}(\overline{y}, S^2/n) \]
  \end{itemize}
\item Conjugate analysis
  \begin{itemize}
  \item Prior: 
  \[ \mu|\sigma^2 \sim N(m, \sigma^2/k) ,\,  \sigma^2 \sim  \mbox{Inv-}\chi^2(v,s^2) ,\,  \mu \sim t_{v}(m, s^2/k) \] 
  \item Posterior:
  \[ \mu|\sigma^2,y \sim N(m', \sigma^2/k') ,\,  \sigma^2|y \sim  \mbox{Inv-}\chi^2(v',(s')^2) ,\,  \mu|y \sim t_{v'}(m', (s')^2/k') \]
  with 
  \[ k' = k+n, m' = [km+n\overline{y}]/k', v'=v+n, \]
  \[ v'(s')^2 = vs^2 + (n-1)S^2 + \frac{kn}{k'}(\overline{y}-m)^2\]
  \end{itemize}
\end{itemize}
\end{frame}





\end{document}
