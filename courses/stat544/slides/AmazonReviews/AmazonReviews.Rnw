\documentclass{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Amazon Reviews}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\I}{\mathrm{I}}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=6, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
library(rstan)
@

<<set_seed, echo=FALSE>>=
set.seed(1)
@



\frame{\maketitle}



\section{Amazon Reviews}
\begin{frame}
\frametitle{Amazon Reviews}

<<>>=
tmp = read.csv("reviews.csv",header=FALSE)[,c(3,4)]
names(tmp) = c("product_id","stars")

product_ids = read.csv("upright_bagless_cyclonic_vacuum_cleaners_amazon_ids.csv", 
                       stringsAsFactors = FALSE)$ids

d = tmp[tmp$product_id %in% product_ids & 
            tmp$stars %in% c("1.0","2.0","3.0","4.0","5.0"),]
d$product_id = factor(d$product_id)
d$stars      = as.numeric(factor(d$stars))
@

\small
<<results='asis'>>=
for_table = ddply(d, .(product_id), summarise, 
          n1 = sum(stars==1),
          n2 = sum(stars==2),
          n3 = sum(stars==3),
          n4 = sum(stars==4),
          n5 = sum(stars==5), 
          n_total = length(stars),
          mean = mean(stars), 
          sd   = sd(stars),
          .drop=FALSE)

print(xtable(for_table, 
             digits = c(NA,NA,1,1,1,1,1,1,2,2), 
             align  = "c|r|rrrrr|rrr|"), 
      include.rownames=FALSE,
      add.to.row = list(pos = list(-1),
                        command = "& \\multicolumn{5}{c|}{Number of ratings} &&& \\\\"))
@

\end{frame}



\begin{frame}
\frametitle{}
\end{frame}

\begin{frame}
\frametitle{}
\end{frame}




\subsection{Normal model}
\begin{frame}
\frametitle{Model for Amazon Reviews}

Let $y_{ij}$ be the $j$th review for the $i$th product. \pause Assume 
\[ y_{ij} \stackrel{ind}{\sim} N(\theta_i, \sigma^2) \]
\pause
and
\[ \theta_i \stackrel{ind}{\sim} N(\mu,\tau^2) \]
\pause
and 
\[ p(\mu,\tau, \sigma) \propto Ca^+(\sigma;0,1) Ca^+(\tau;0,1) \]

\end{frame}



\begin{frame}[fragile]
\frametitle{Normal hierarchical model in Stan}

<<echo=TRUE>>=
normal_model = "
data {
  int <lower=1> n;
  int <lower=1> n_products;
  int <lower=1,upper=5> stars[n];
  int <lower=1,upper=n_products> id[n];
}

parameters {
  real theta[n_products];
  real sigma;
  real mu;
  real<lower=0> tau;
}

model {
  sigma ~ cauchy(0,1);
  tau ~ cauchy(0,1);
  for (i in 1:n) stars[i] ~ normal(theta[id[i]], sigma);
  for (p in 1:n_products) theta[p] ~ normal(mu,tau);
}
"
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Fit model}

<<run_normal_model, echo=TRUE, cache=TRUE, dependson='normal_model'>>=
m = stan_model(model_code = "normal_model")
dat = list(n = nrow(d),
           n_products = nlevels(d$product_id),
           stars = d$stars,
           id = as.numeric(d$product_id))
r = sampling(m, dat)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Tabular summary}

<<dependson='run_normal_model'>>=
r
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Movie mean posteriors ($\theta_i$)}

<<dependson='run_normal_model'>>=
draws = extract(r)
theta = draws$theta
colnames(theta) = paste("theta_",1:ncol(theta), sep="")
theta_m = melt(theta, varnames=c("iterations", "theta"))
theta_m$product = levels(d$product_id)[theta_m$theta]
ggplot(theta_m, aes(x=value, fill=product)) +  geom_density(alpha=0.5)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Other parameter posteriors}

<<dependson='normal_model', message=FALSE>>=
other = rbind(data.frame(parameter = "sigma", value = draws$sigma),
              data.frame(parameter = "mu",    value = draws$mu   ),
              data.frame(parameter = "tau",   value = draws$tau))
prior = rbind(data.frame(x = seq(1.3,1.5,by=0.001), 
                         parameter = "sigma", 
                         y = 2*dcauchy(seq(1.3,1.5,by=0.001),0,1)),
              data.frame(x = seq(0,3,by=0.01), 
                         parameter = "tau",   
                         y = 2*dcauchy(seq(0,3,by=0.01))))
ggplot(other, aes(x=value)) +  
  geom_histogram(aes(y=..density..)) + 
  geom_line(data=prior, aes(x=x,y=y, color='red')) +
  facet_wrap(~parameter, scales="free")  +
  theme(legend.position="none")
@

\end{frame}



\begin{frame}
\frametitle{A quick rating}

Suppose a new vacuum cleaner comes on the market and there are two Amazon reviews both with 5 stars. \pause What do you think the averaging star rating will be (in the future) for this new product? 

\vspace{0.2in} \pause 

Let $n^*$ be the number of new ratings and $\overline{y}^*$ be the average of those ratings, \pause then
\[ \begin{array}{rl} 
E[\theta^*|y,\overline{y}^*,n^*,\sigma,\mu,\tau] &= \pause \frac{\frac{n^*}{\sigma^2}}{\frac{n^*}{\sigma^2} + \frac{1}{\tau^2}}\overline{y}^* + \frac{\frac{1}{\tau^2}}{\frac{n^*}{\sigma^2} + \frac{1}{\tau^2}} \mu \pause \\
&= \frac{n^*}{n^* + \frac{\sigma^2}{\tau^2}}\overline{y}^* + \frac{\frac{\sigma^2}{\tau^2}}{n^* + \frac{\sigma^2}{\tau^2}} \mu \pause \\ 
&= \frac{n^*}{n^* + m}\overline{y}^* + \frac{m}{n^* + m} \mu 
\end{array} \]
\pause
where $m=\sigma^2/\tau^2$ is a measure of how many \emph{prior} samples there are.


\end{frame}



\begin{frame}[containsverbatim]
\frametitle{IMDB rating}

From \url{http://www.imdb.com/chart/top.html}:

{\small
\begin{verbatim}
weighted rating (WR) = (v / (v+m)) × R + (m / (v+m)) × C

Where:

R = average for the movie (mean) = (Rating)
v = number of votes for the movie = (votes)
m = minimum votes required to be listed in the Top 250 
    (currently 25000)
C = the mean vote across the whole report (currently 7.1)
\end{verbatim}
}

\vspace{0.2in} 

Thus IMDB uses a Bayesian estimate for the rating for each movie where $m=\sigma^2/\tau^2 = 25,000$. \pause IMDB has enough data that the uncertainty in $\mu (C)$, $\sigma^2,$ and $\tau^2$ is pretty minimal.

\end{frame}



\subsection{Binomial model}
\begin{frame}
\frametitle{Clearly incorrect model}

We assumed 
\[ y_{ij} \stackrel{ind}{\sim} N(\theta_i,\sigma^2) \]
for the $j$th star rating of product $i$. \pause Clearly this model is incorrect since $y_{ij} \in \{1,2,3,4,5\}$. 

\vspace{0.2in} 

An alternative model is 
\[ y_{ij} \stackrel{ind}{\sim} Bin(4, \theta_i) \]
where $y_{ij}$ is the $j$th star rating minus 1 of product $i$ \pause and 
\[ \theta_i \sim Be(\alpha,\beta) \pause \qquad \mbox{and} \qquad p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}. \]
\pause
The idea behind this model would be that product $i$ has an independent, but common probability ($\theta_i$) of earning each star.

\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial hierarchical model in Stan}

<<binomial_model, echo=TRUE, cache=TRUE>>=
binomial_model = "
data {
  int <lower=1> n;
  int <lower=1> n_products;
  int <lower=0,upper=4> stars[n];
  int <lower=1,upper=n_products> id[n];
}

parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  real<lower=0,upper=1> theta[n_products];
}

model {
  increment_log_prob(-5*log(alpha+beta)/2); # improper prior

  for (i in 1:n) stars[i] ~ binomial(4, theta[id[i]]);
  for (p in 1:n_products) theta[p] ~ beta(alpha,beta);
}
"
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Fit model}

<<run_binomial_model, echo=TRUE, cache=TRUE, dependson='normal_model'>>=
m = stan_model(model_code = "binomial_model")
dat = list(n = nrow(d),
           n_products = nlevels(d$product_id),
           stars = d$stars-1,
           id = as.numeric(d$product_id))
r = sampling(m, dat)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Tabular summary}

<<dependson='run_binomial_model'>>=
r
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Movie mean posteriors ($\theta_i$)}

<<dependson='run_binomial_model'>>=
draws = extract(r)
theta = draws$theta
colnames(theta) = paste("theta_",1:ncol(theta), sep="")
theta_m = melt(theta, varnames=c("iterations", "theta"))
theta_m$product = levels(d$product_id)[theta_m$theta]
ggplot(theta_m, aes(x=value, fill=product)) +  geom_density(alpha=0.5)
@

\end{frame}


\begin{frame}
\frametitle{Other parameter posteriors}

Recall that 
\begin{itemize}
\item $\alpha$ is the prior success
\item $\beta$ is the prior failures
\end{itemize}

\vspace{0.2in} \pause

So 
\begin{itemize}
\item $\alpha+\beta$ is the prior sample size
\item $E[\theta_i|\alpha,\beta] = \frac{\alpha}{\alpha+\beta}$ is the prior expectation for the probability
\end{itemize}

\vspace{0.2in} \pause

But we might want to show results on the original scale (stars), \pause so the expected number of stars for a new product is 
\[  \begin{array}{rl}
E[\mbox{stars}_{*j}|\alpha,\beta] &= E[\mbox{y}_{*j}+1|\alpha,\beta] \pause = E[\mbox{y}_{*j}|\alpha,\beta] + 1 \pause \\
&= E[E[y_{*j}|\theta^*]|\alpha,\beta]+1 \pause = E[4\theta^*|\alpha,\beta]+1 \pause \\
&= 4\frac{\alpha}{\alpha+\beta} + 1
\end{array} \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Other parameter posteriors}

<<dependson='run_binomial_model', warning=FALSE, message=FALSE>>=
other = rbind(data.frame(parameter = "alpha", value = as.numeric(draws$alpha)),
              data.frame(parameter = "beta",    value = as.numeric(draws$beta)   ),
              data.frame(parameter = "prior_sample_size",   value = as.numeric(draws$alpha+draws$beta)),
              data.frame(parameter = "prior_mean",   value = as.numeric(draws$alpha/(draws$alpha+draws$beta))),
              data.frame(parameter = "prior_stars",   value = 1+4*as.numeric(draws$alpha/(draws$alpha+draws$beta))))
ggplot(other, aes(x=value)) +  
  geom_histogram(aes(y=..density..)) + 
  facet_wrap(~parameter, scales="free")  +
  theme(legend.position="none")
@

\end{frame}



\subsection{Posterior predictive pvalues}
\begin{frame}
\frametitle{Uniform use of star ratings}

This binomial model has the proper support $\{0,1,2,3,4\}$ for stars minus 1, \pause but does it have the correct proportion of observations in each star category? 

\vspace{0.2in} \pause

As an example, $\hat{\theta}_2 = 0.81$ (and a 95\% CI is (0.79, 0.83)). \pause Thus, we would expect 

<<results='asis'>>=
tmp = data.frame(stars = 1:5,
           theoretical = dbinom(0:4, 4, .81),
           actual = as.numeric(for_table[2,2:6]/for_table[2,7]))
print(xtable(tmp, digits=c(NA, 0, 3, 3)), include.rownames=FALSE)
@

\pause
But this ignores the uncertainty in $\theta_2$, so perhaps this difference is due to this uncertainty. 

\end{frame}





\begin{frame}
\frametitle{Posterior predictive pvalue}

\small

To assess this model fit, we will simulate posterior predictive star ratings for product 2 and compare to the observed ratings:
<<results='asis'>>=
print(xtable(for_table[2,1:7]), include.rownames=FALSE)
@
\pause
Let $\tilde{y}_2$ be all the predictive data for product 2, i.e. $\tilde{y}_2 = (\tilde{y}_{21},\ldots,\tilde{y}_{2J})$ with $J=535$ where $\tilde{y}_{2j}$ is the $j$th predictive star rating minus 1 for review $j$ of product 2. \pause Then 

\[ p(\tilde{y}_2|y) = \int \left[\prod_{j=1}^J p(\tilde{y}_{2j}|\theta_2)\right] p(\theta_2|y) d\theta_2 \]

\pause Thus the following procedure will simulation from the joint distribution for the predictive ratings:
\begin{enumerate}
\item $\theta_2 \sim p(\theta_2|y)$, 
\item For $j=1,\ldots,535$, $y_{2j} \stackrel{ind}{\sim} Bin(4,\theta_2)$, and
\item star$_{2j} = y_{2j}+1$. 
\end{enumerate}

\end{frame}




\begin{frame}[fragile]
\frametitle{Posterior predictive distribution in R}

<<echo=TRUE>>=
theta2 = as.numeric(draws$theta[,2])
ytilde2 = adply(theta2, 1, function(x) {
  ytilde = rbinom(535, 4, x) + 1
  data.frame(n1 = sum(ytilde==1),
             n2 = sum(ytilde==2),
             n3 = sum(ytilde==3),
             n4 = sum(ytilde==4),
             n5 = sum(ytilde==5))
})
head(ytilde2)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior predictive distribution in R}

<<>>=
ytilde2_m = melt(ytilde2, id.vars="X1")
truth = data.frame(variable = paste("n",1:5,sep=""),
                   observed = as.numeric(for_table[2,2:6]))
ggplot(ytilde2_m, aes(x=value)) + 
  geom_histogram(aes(y=..density..), binwidth=1) + 
  facet_wrap(~variable, scales="free") + 
  geom_vline(data=truth, aes(xintercept=observed), color="red")
  
@

\end{frame}



\subsection{Ordinal data model}
\begin{frame}
\frametitle{Ordinal data model}

Let $y_i = (y_{i1},\ldots,y_{i5})$ be the vector of 1-star to 5-star ratings, \pause assume
\begin{align*}
Y_{i} \stackrel{ind}{\sim} Mult(n_i,\theta_i)
\end{align*}
where $\theta_i$ is a probability vector \pause
\[ \theta_{ik} = \int_{\alpha_{k-1}}^{\alpha_k} N(x|\mu_i,1) dx = \Phi(\alpha_k-\mu_i) - \Phi(\alpha_{k-1}-\mu_i) \]
\pause 
where $\alpha_0=-\infty$, $\alpha_1=0$, and $\alpha_5=\infty$, \pause and $\Phi$ is the standard normal cumulative distribution function (cdf). 

\end{frame}


\begin{frame}
\frametitle{Visualizing the model}

<<cuts, echo=FALSE>>=
mu = 1.7
curve(dnorm(x,mu,1), -1, 5, lwd=2, xlab="", axes=F, frame=T, ylab="")
abline(v=0:3,col="red")
axis(1, at=0:3, 
     labels=paste("alpha_",1:4,sep=""), 
     col="red")

abline(v=mu, lty=2, col="blue")
axis(3, at=mu, labels=expression(mu[j]), col="blue")
text((-1:3)+.5, 0, 1:5, col="seagreen")
@
\end{frame}

\begin{frame}
\frametitle{Hierarchical model}
So each product has its own mean $\mu_i$. \pause The larger $\mu_i$ is the more 5-star ratings the product will receive and the fewer 1-star ratings the product will review. 

\vspace{0.2in} \pause

In order to borrow information across different products, we might assume a hierarchical model for the $\mu_i$\pause, e.g. 

\[ \mu_i \stackrel{ind}{\sim} N(\eta,\tau^2) \]
\pause 
with a prior 
\[ p(\eta,\tau) \propto Ca(\tau;0,1). \]
\end{frame}





\end{document}
