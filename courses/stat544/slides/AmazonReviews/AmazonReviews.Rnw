\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Amazon Reviews}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
library(rstan)
@

<<set_seed>>=
set.seed(1)
@



\frame{\maketitle}



\section{Amazon Reviews}
\begin{frame}
\frametitle{Amazon Reviews - Upright, bagless, cyclonic vacuum cleaners}

<<reviews>>=
tmp = read.csv("reviews.csv",header=FALSE)[,c(3,4)]
names(tmp) = c("product_id","stars")

product_ids = read.csv("upright_bagless_cyclonic_vacuum_cleaners_amazon_ids.csv", 
                       stringsAsFactors = FALSE)$ids

d = tmp[tmp$product_id %in% product_ids & 
            tmp$stars %in% c("1.0","2.0","3.0","4.0","5.0"),]
d$product_id = factor(d$product_id)
d$stars      = as.numeric(factor(d$stars))
@

\small
<<table, results='asis', dependson='reviews'>>=
for_table = ddply(d, .(product_id), summarise, 
          n1 = sum(stars==1),
          n2 = sum(stars==2),
          n3 = sum(stars==3),
          n4 = sum(stars==4),
          n5 = sum(stars==5), 
          n_total = length(stars),
          mean = mean(stars), 
          sd   = sd(stars),
          .drop=FALSE)

print(xtable(for_table, 
             digits = c(NA,NA,1,1,1,1,1,1,2,2), 
             align  = "c|r|rrrrr|rrr|"), 
      include.rownames=FALSE,
      add.to.row = list(pos = list(-1),
                        command = "& \\multicolumn{5}{c|}{Number of ratings} &&& \\\\"))
@

\end{frame}


\subsection{Normal model}
\begin{frame}
\frametitle{Model for Amazon Reviews}

Let $y_{ij}$ be the $j$th review for the $i$th product. \pause Assume 
\[ y_{ij} \stackrel{ind}{\sim} N(\theta_i, \sigma^2) \]
\pause
and
\[ \theta_i \stackrel{ind}{\sim} N(\mu,\tau^2) \]
\pause
and 
\[ p(\mu,\tau, \sigma) \propto Ca^+(\sigma;0,1) Ca^+(\tau;0,1) \]

\end{frame}



\begin{frame}[fragile]
\frametitle{Normal hierarchical model in Stan}

<<normal_model, echo=TRUE>>=
normal_model = "
data {
  int <lower=1> n;
  int <lower=1> n_products;
  int <lower=1,upper=5> stars[n];
  int <lower=1,upper=n_products> id[n];
}

parameters {
  real mu;                // implied uniform prior
  real<lower=0> sigma;
  real<lower=0> tau;
  real theta[n_products];
}

model {
  // Prior 
  sigma ~ cauchy(0,1);
  tau   ~ cauchy(0,1);

  // Hierarchial model
  theta ~ normal(mu,tau);

  // Data model
  for (i in 1:n) stars[i] ~ normal(theta[id[i]], sigma);
}
"
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Fit model}

<<run_normal_model, echo=TRUE, dependson=c('normal_model','reviews')>>=
m = stan_model(model_code = normal_model)
dat = list(n = nrow(d),
           n_products = nlevels(d$product_id),
           stars = d$stars,
           id = as.numeric(d$product_id))
r = sampling(m, dat)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Tabular summary}

<<dependson='run_normal_model'>>=
r
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Vacuum cleaner mean posteriors ($\theta_i$)}

<<dependson='run_normal_model'>>=
draws = extract(r)
theta = draws$theta
colnames(theta) = paste("theta_",1:ncol(theta), sep="")
theta_m = melt(theta, varnames=c("iterations", "theta"))
theta_m$product = levels(d$product_id)[theta_m$theta]

ggplot(theta_m, aes(x=value, fill=product)) +  
  geom_density(alpha=0.5) +
  theme_bw()
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Other parameter posteriors}

<<dependson='run_normal_model', message=FALSE>>=
other = rbind(data.frame(parameter = "sigma", value = draws$sigma),
              data.frame(parameter = "mu",    value = draws$mu   ),
              data.frame(parameter = "tau",   value = draws$tau))
prior = rbind(data.frame(x = seq(1.3,1.5,by=0.001), 
                         parameter = "sigma", 
                         y = 2*dcauchy(seq(1.3,1.5,by=0.001),0,1)),
              data.frame(x = seq(0,3,by=0.01), 
                         parameter = "tau",   
                         y = 2*dcauchy(seq(0,3,by=0.01))))

ggplot(other, aes(x=value)) +  
  geom_histogram(aes(y=..density..)) + 
  geom_line(data=prior, aes(x=x,y=y), color='red') +
  facet_wrap(~parameter, scales="free") +
  theme_bw() +
  theme(legend.position="none") 
@

\end{frame}



\begin{frame}
\frametitle{A quick rating}

Suppose a new vacuum cleaner comes on the market and there are two Amazon 
reviews both with 5 stars. 
\pause 
What do you think the average star rating will be (in the future) for this new 
product? 

\vspace{0.2in} \pause 

Let $n^*$ be the number of new ratings and $\overline{y}^*$ be the average of those ratings, \pause then
\[ \begin{array}{rl} 
E[\theta^*|\overline{y}^*,n^*,\sigma,\mu,\tau] &= \pause \frac{\frac{n^*}{\sigma^2}}{\frac{n^*}{\sigma^2} + \frac{1}{\tau^2}}\overline{y}^* + \frac{\frac{1}{\tau^2}}{\frac{n^*}{\sigma^2} + \frac{1}{\tau^2}} \mu \pause \\
&= \frac{n^*}{n^* + \frac{\sigma^2}{\tau^2}}\overline{y}^* + \frac{\frac{\sigma^2}{\tau^2}}{n^* + \frac{\sigma^2}{\tau^2}} \mu \pause \\ 
&= \frac{n^*}{n^* + m}\overline{y}^* + \frac{m}{n^* + m} \mu 
\end{array} \]
\pause
where $m=\sigma^2/\tau^2$ is a measure of how many \emph{prior} samples there are.


\end{frame}



\begin{frame}[containsverbatim]
\frametitle{IMDB rating}

From \url{http://www.imdb.com/chart/top.html}:

{\small
\begin{verbatim}
weighted rating (WR) = (v / (v+m)) × R + (m / (v+m)) × C

Where:

R = average for the movie (mean) = (Rating)
v = number of votes for the movie = (votes)
m = minimum votes required to be listed in the Top 250 
    (currently 25000)
C = the mean vote across the whole report (currently 7.1)
\end{verbatim}
}

\vspace{0.2in} 

Thus IMDB uses a Bayesian estimate for the rating for each movie where $m=\sigma^2/\tau^2 = 25,000$. \pause IMDB has enough data that the uncertainty in $\mu (C)$, $\sigma^2,$ and $\tau^2$ is pretty minimal.

\end{frame}



\subsection{Binomial model}
\begin{frame}
\frametitle{Clearly incorrect model}

We assumed 
\[ y_{ij} \stackrel{ind}{\sim} N(\theta_i,\sigma^2) \]
for the $j$th star rating of product $i$. \pause Clearly this model is incorrect since $y_{ij} \in \{1,2,3,4,5\}$. 

\vspace{0.2in} 

An alternative model is 
\[ z_{ij} \stackrel{ind}{\sim} Bin(4, \theta_i) \]
where $z_{ij}=y_{ij}-1$ is the $j$th star rating minus 1 of product $i$ \pause and 
\[ \theta_i \sim Be(\alpha,\beta) \pause \qquad \mbox{and} \qquad p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}. \]
\pause
The idea behind this model would be that product $i$ the probability of earning
each star is $\theta_i$ and each star is independent.

\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial hierarchical model in Stan}

<<binomial_model, echo=TRUE>>=
binomial_model = "
data {
  int <lower=1> n;
  int <lower=1> n_products;
  int <lower=0,upper=4> z[n];
  int <lower=1,upper=n_products> id[n];
}

parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  real<lower=0,upper=1> theta[n_products];
}

model {
  // Prior
  target += -5*log(alpha+beta)/2; // improper prior

  // Hierarchical model
  theta ~ beta(alpha,beta);

  // Data model
  for (i in 1:n) z[i] ~ binomial(4, theta[id[i]]);
}
"
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Fit model}

<<run_binomial_model, echo=TRUE, dependson=c('binomial_model','reviews')>>=
m = stan_model(model_code = binomial_model)
dat = list(n = nrow(d),
           n_products = nlevels(d$product_id),
           z = d$stars-1,
           id = as.numeric(d$product_id))
r = sampling(m, dat)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Tabular summary}

<<dependson='run_binomial_model'>>=
r
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Review mean posteriors ($\theta_i$)}

<<dependson='run_binomial_model'>>=
draws = extract(r)
theta = draws$theta
colnames(theta) = paste("theta_",1:ncol(theta), sep="")
theta_m = melt(theta, varnames=c("iterations", "theta"))
theta_m$product = levels(d$product_id)[theta_m$theta]

ggplot(theta_m, aes(x=value, fill=product)) +  
  geom_density(alpha=0.5) +
  theme_bw()
@

\end{frame}


\begin{frame}
\frametitle{Other parameter posteriors}

Recall that 
\begin{itemize}
\item $\alpha$ is the prior success
\item $\beta$ is the prior failures
\end{itemize}

\vspace{0.2in} \pause

So 
\begin{itemize}
\item $\alpha+\beta$ is the prior sample size
\item $E[\theta_i|\alpha,\beta] = \frac{\alpha}{\alpha+\beta}$ is the prior expectation for the probability
\end{itemize}

\vspace{0.2in} \pause

But we might want to show results on the original scale (stars), \pause so the expected number of stars for a new product is 
\[  \begin{array}{rl}
E[\mbox{stars}_{*j}|\alpha,\beta] &= E[\mbox{y}_{*j}+1|\alpha,\beta] \pause = E[\mbox{y}_{*j}|\alpha,\beta] + 1 \pause \\
&= E[E[y_{*j}|\theta^*]|\alpha,\beta]+1 \pause = E[4\theta^*|\alpha,\beta]+1 \pause \\
&= 4\frac{\alpha}{\alpha+\beta} + 1
\end{array} \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Other parameter posteriors}

<<dependson='run_binomial_model', warning=FALSE, message=FALSE>>=
other = rbind(data.frame(parameter = "alpha", value = as.numeric(draws$alpha)),
              data.frame(parameter = "beta",    value = as.numeric(draws$beta)   ),
              data.frame(parameter = "prior_sample_size",   value = as.numeric(draws$alpha+draws$beta)),
              data.frame(parameter = "prior_mean",   value = as.numeric(draws$alpha/(draws$alpha+draws$beta))),
              data.frame(parameter = "prior_stars",   value = 1+4*as.numeric(draws$alpha/(draws$alpha+draws$beta))))

ggplot(other, aes(x=value)) +  
  geom_histogram(aes(y=..density..)) + 
  facet_wrap(~parameter, scales="free")  +
  theme(legend.position="none") +
  theme_bw()
@

\end{frame}



\subsection{Posterior predictive pvalues}
\begin{frame}
\frametitle{Uniform use of star ratings}

This binomial model has the proper support $\{0,1,2,3,4\}$ for stars minus 1, \pause but does it have the correct proportion of observations in each star category? 

\vspace{0.2in} \pause

As an example, $\hat{\theta}_2 = 0.81$. 
\pause 
Thus, we would expect if we used $\hat{\theta}_2$

<<results='asis'>>=
tmp = data.frame(stars = 1:5,
           theoretical = dbinom(0:4, 4, .81),
           observed = as.numeric(for_table[2,2:6]/for_table[2,7]))
print(xtable(tmp, digits=c(NA, 0, 3, 3)), include.rownames=FALSE)
@

\pause
But this ignores the uncertainty in $\theta_2$ (95\% CI is (0.79, 0.83)), 
so perhaps this difference is due to this uncertainty. 

\end{frame}





\begin{frame}
\frametitle{Posterior predictive pvalue}

\small

To assess this model fit, we will simulate posterior predictive star ratings for product 2 and compare to the observed ratings:
<<results='asis'>>=
print(xtable(for_table[2,1:7]), include.rownames=FALSE)
@
\pause
Let $\tilde{y}_2$ be all the predictive data for product 2, i.e. $\tilde{y}_2 = (\tilde{y}_{21},\ldots,\tilde{y}_{2J})$ with $J=526$ where $\tilde{y}_{2j}$ is the $j$th predictive star rating minus 1 for review $j$ of product 2. \pause Then 

\[ p(\tilde{y}_2|y) = \int \left[\prod_{j=1}^J p(\tilde{y}_{2j}|\theta_2)\right] p(\theta_2|y) d\theta_2 \]

\pause Thus the following procedure will simulation from the joint distribution for the predictive ratings:
\begin{enumerate}
\item $\theta_2 \sim p(\theta_2|y)$, 
\item For $j=1,\ldots,526$, $y_{2j} \stackrel{ind}{\sim} Bin(4,\theta_2)$, and
\item star$_{2j} = y_{2j}+1$. 
\end{enumerate}

\end{frame}




\begin{frame}[fragile]
\frametitle{Posterior predictive distribution in R}

<<posterior_predictive_binomial, dependson='run_binomial_model', echo=TRUE>>=
theta2 = as.numeric(draws$theta[,2])
ytilde2 = adply(theta2, 1, function(x) {
  ytilde = rbinom(526, 4, x) + 1
  data.frame(n1 = sum(ytilde==1),
             n2 = sum(ytilde==2),
             n3 = sum(ytilde==3),
             n4 = sum(ytilde==4),
             n5 = sum(ytilde==5))
})
head(ytilde2)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior predictive distribution in R}

<<>>=
ytilde2_m = melt(ytilde2, id.vars="X1")
truth = data.frame(variable = paste("n",1:5,sep=""),
                   observed = as.numeric(for_table[2,2:6]))

ggplot(ytilde2_m, aes(x=value)) + 
  geom_histogram(aes(y=..density..), binwidth=1) + 
  facet_wrap(~variable, scales="free") + 
  geom_vline(data=truth, aes(xintercept=observed), color="red") +
  theme_bw()
@

\end{frame}



\subsection{Ordinal data model}
\begin{frame}
\frametitle{Ordinal data model}

Let $y_i = (y_{i1},\ldots,y_{i5})$ be the vector of 1-star to 5-star ratings, \pause assume
\begin{align*}
Y_{i} \stackrel{ind}{\sim} Mult(n_i,\theta_i)
\end{align*}
where $\theta_i$ is a probability vector \pause
\[ \theta_{ik} = \int_{\alpha_{k-1}}^{\alpha_k} N(x|\mu_i,1) dx = \mathrm{\Phi}(\alpha_k-\mu_i) - \mathrm{\Phi}(\alpha_{k-1}-\mu_i) \]
\pause 
where $\alpha_0=-\infty$, $\alpha_1=0$, and $\alpha_5=\infty$, \pause and $\mathrm{\Phi}$ is the standard normal cumulative distribution function (cdf). 

\end{frame}


\begin{frame}
\frametitle{Visualizing the model}

<<cuts>>=
mu = 1.7
curve(dnorm(x,mu,1), -1, 5, lwd=2, xlab="", axes=F, frame=T, ylab="")
abline(v=0:3,col="red")
axis(1, at=0:3, 
     labels=parse(text=paste("alpha[",1:4,"]",sep='')), 
     col="red")

abline(v=mu, lty=2, col="blue")
axis(3, at=mu, labels=expression(mu[i]), col="blue")
text((-1:3)+.5, 0, 1:5, col="seagreen")
@
\end{frame}

\begin{frame}
\frametitle{Hierarchical model}
So each product has its own mean $\mu_i$. \pause The larger $\mu_i$ is the more 5-star ratings the product will receive and the fewer 1-star ratings the product will review. 

\vspace{0.2in} \pause

In order to borrow information across different products, we might assume a hierarchical model for the $\mu_i$\pause, e.g. 

\[ \mu_i \stackrel{ind}{\sim} N(\eta,\tau^2) \]
\pause 
with a prior 
\[ p(\eta,\tau) \propto Ca(\tau;0,1). \]
\end{frame}



\begin{frame}[fragile]
\frametitle{}

<<ordinal_model, echo=TRUE>>=
ordinal_model = "
data {
  int <lower=1> n_products;
  int <lower=0> y[n_products,5]; // summarized count by product
}

parameters {
  real<lower=0> alpha_diff[3];
  real mu[n_products];
  real eta;
  real<lower=0> tau;
}

transformed parameters {
  ordered[4] alpha;             // cut points
  simplex[5] theta[n_products]; // each theta vector sums to 1

  alpha[1] = 0; for (i in 1:3) alpha[i+1] = alpha[i] + alpha_diff[i];

  for (p in 1:n_products) {
    theta[p,1] = Phi(-mu[p]);
    for (j in 2:4) 
      theta[p,j] = Phi(alpha[j]-mu[p]) - Phi(alpha[j-1]-mu[p]);
    theta[p,5] = 1-Phi(alpha[4]-mu[p]);
  }
}

model {
  tau ~ cauchy(0,1);
  mu ~ normal(eta, tau);
  for (p in 1:n_products) y[p] ~ multinomial(theta[p]); // n_reviews[p] is implicit
}
"
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Fit model}

<<run_ordinal_model, echo=TRUE, dependson='ordinal_model'>>=
m = stan_model(model_code = ordinal_model)
dat = list(n_products = nrow(for_table),
           y = as.matrix(for_table[,2:6]))
r = sampling(m, dat, pars = c("alpha","eta","tau","mu"))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Fit model}

<<echo=TRUE, dependson='run_ordinal_model'>>=
r
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Review mean posteriors ($\theta_i$)}

<<dependson='run_ordinal_model'>>=
draws = extract(r)
mu = draws$mu
colnames(mu) = paste("mu_",1:ncol(mu), sep="")
mu_m = melt(mu, varnames=c("iterations", "mu"))
mu_m$product = levels(d$product_id)[mu_m$mu]

ggplot(mu_m, aes(x=value, fill=product)) +  
  geom_density(alpha=0.5) +
  theme_bw()
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Other parameter posteriors}

<<dependson='run_ordinal_model', warning=FALSE, message=FALSE>>=
other = data.frame("alpha" = as.matrix(draws$alpha),
              "eta" = as.numeric(draws$eta),
              "tau" = as.numeric(draws$tau))

ggplot(melt(other), aes(x=value)) +  
  geom_histogram(aes(y=..density..)) + 
  facet_wrap(~variable, scales="free")  +
  theme(legend.position="none") +
  theme_bw()
@

\end{frame}


\begin{frame}
\frametitle{Visualizing the model}

<<cuts2, dependson='run_ordinal_model'>>=
alpha = c(0,.36,.6,1.11)
mu = c(1.49,-.38)
curve(dnorm(x,mu[1],1), -3, 5, lwd=2, xlab="", axes=F, frame=T, ylab="", col='gray')
curve(dnorm(x,mu[2],1), lwd=2, col='gray', add=TRUE)
abline(v=alpha,col="red")
axis(1, at=alpha, 
     labels=parse(text=paste("hat(alpha)[",1:4,"]",sep='')), 
     col="red")

#abline(v=mu, lty=2, col="blue")
axis(3, at=mu, labels=parse(text=paste("hat(mu)[",c(2,10),"]",sep='')), col="blue")
#text((-1:3)+.5, 0, 1:5, col="seagreen")
@
\end{frame}


\end{document}
