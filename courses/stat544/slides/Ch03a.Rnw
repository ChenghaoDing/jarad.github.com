\documentclass[handout]{beamer}

%\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}


\title{Multiparameter models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\begin{document}

%\section{Temp??} \begin{comment}

<<chunk_options, echo=FALSE>>=
opts_chunk$set(fig.width=6, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny')
library(plyr)
@

\frame{\titlepage}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Theoretical justification for simulation
  \begin{itemize}
  \item Strong Law of Large Numbers
  \item Central limit theorem
  \end{itemize}
\item Independent beta-binomial
  \begin{itemize}
  \item Independent posteriors
  \item Comparison of parameters
  \item JAGS
  \end{itemize}
\item Probability theory results
  \begin{itemize}
  \item Scaled Inv-$\chi^2$ distribution
  \item $t$-distribution
  \item Normal-Inv$\chi^2$ distribution
  \end{itemize}
\item Normal model with unknown mean and variance
  \begin{itemize}
  \item Jeffreys prior
  \item Natural conjugate prior
  \end{itemize}
\end{itemize}
\end{frame}

\section{Monte Carlo methods}
\frame{\frametitle{Monte Carlo integration}
\small
  Consider evaluating the integral
	\[ E_X[h(X)] = \int_{\mathcal{X}} h(x) f(x) dx \]
	\pause using the Monte Carlo estimate
	\[ \hat{h}_J = \frac{1}{J} \sum_{j=1}^J h\left(x^{(j)}\right) \]
	\pause where $x^{(j)} \stackrel{ind}{\sim} f(x)$.  \pause We know
	\begin{itemize}
	\item SLLN: $\hat{h}_J$ converges almost surely to $E_X[h(X)]$. \pause 
	\item CLT: if $h^2$ has finite expectation, \pause then 
	\[ \hat{h}_J \stackrel{d}{\to} N(E_X[h(X)], v_J) \pause \]
	where 
	\[ v_J = \frac{1}{J} \widehat{V_X[h(X)]} \pause \approx \frac{1}{J^2} \sum_{s=1}^J \left[ h\left(x^{(j)}\right) - \hat{h}_J \right]^2. \]
	\end{itemize}
}

\subsection{Definite integral}
\frame{\frametitle{Definite integral}
	Suppose you are interested in evaluating
	\[ \mathrm{I} = \int_0^1 e^{-x^2/2} dx. \]
	\pause Then set
	\begin{itemize}
	\item $h(x) = e^{-x^2/2}$ \pause and
	\item $f(x) = 1$\pause, i.e. $x\sim \mbox{Unif}(0,1)$. \pause
	\end{itemize}
  \pause
  and approximate by a Monte Carlo estimate via
  \begin{enumerate}[<+->]
  \item For $j=1,\ldots,J$, 
    \begin{enumerate}
    \item sample $x^{(j)} \sim Unif(0,1)$ and 
    \item calculate $h\left(x^{(j)}\right)$.
    \end{enumerate}
  \item Calculate 
  \[ \mathrm{I} \approx \frac{1}{J} \sum_{j=1}^J h(x^{(j)}). \]
  \end{enumerate}
}



\begin{frame}[fragile]
\frametitle{Strong law of large numbers}
<<uniform, echo=FALSE>>=
f = function(x) exp(-x^2/2)

# Monte Carlo integration
set.seed(1)
n = 1e3
x = runif(n)
fx = f(x)

par(mfrow=c(1,2))
n_points = 20; curve(f, main=paste(n_points, "samples"))
segments(x[1:n_points], 0, x[1:n_points], fx[1:n_points])
n_points = 200; curve(f, main=paste(n_points, "samples"))
segments(x[1:n_points], 0, x[1:n_points], fx[1:n_points])
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Strong law of large numbers}
<<uniform2, echo=FALSE>>=

# Monte Carlo integration
cummean = function(x) return(cumsum(x)/(1:length(x)))
plot(cmfx<-cummean(fx), type='l', ylab="Estimate", main="Monte Carlo estimate", xlab="Number of samples")
abline(h=h<-diff(pnorm(c(0,1)))*sqrt(2*pi), col="red")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Central limit theorem}
<<uniform3, echo=FALSE>>=
cumvar = function(x) { 
  J = length(x)
  cumvar = numeric(J)
  for (j in 1:J) { cumvar[j] = var(x[1:j])/j }
  return(cumvar)
}

cvfx = cumvar(fx)
uci  = cmfx+qnorm(.975)*sqrt(cvfx)
lci  = cmfx-qnorm(.975)*sqrt(cvfx)
plot(cmfx, main="Monte Carlo estimate", type="n", 
     ylim=range(uci,lci,na.rm=T), xlab="Number of samples", ylab="Estimate")
abline(h=h, col="red",lwd=2)
lines(cmfx,lwd=2)
lines(uci, lty=2)
lines(lci, lty=2)
legend("bottomright", c("Truth","Estimate","95% CI"), 
       lwd=c(2,2,1), lty=c(1,1,2), col=c("red","black","black"))
@
\end{frame}




\subsection{Infinite bounds}
\frame{\frametitle{Infinite bounds}
	Suppose $X\sim N(0,1)$ and you are interested in evaluating
	\[ E_X[X] \pause = \int_{-\infty}^\infty x \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx \]
	
	\pause Then set
	\begin{itemize}
	\item $h(x) = x$ \pause and
	\item $f(x) = \phi(x)$\pause, i.e. $x\sim N(0,1)$. \pause
	\end{itemize}
  and approximate by a Monte Carlo estimate via
  \begin{enumerate}[<+->]
  \item For $j=1,\ldots,J$, 
    \begin{enumerate}
    \item sample $x^{(j)} \sim N(0,1)$ and 
    \item calculate $h\left(x^{(j)}\right)$.
    \end{enumerate}
  \item Calculate 
  \[ E_X[X] \approx \frac{1}{J} \sum_{j=1}^J h(x^{(j)}). \]
  \end{enumerate}
}

\begin{frame}[fragile]
\frametitle{Non-uniform sampling}
<<echo=FALSE>>=
opar = par(mfrow=c(1,2))
x = rnorm(100)
y = x[1:20]
rng = c(-3,3)
plot(y,y, type='l',
     main="n=20", xlab="locations, x", ylab='h(x)=x', 
     xlim=rng, ylim=rng)
segments(y,0,y,y)
abline(0,1,lwd=2)
abline(h=0)
plot(x, x, type='l',
     main="n=100", xlab="locations, x", ylab='h(x)=x',
     xlim=rng, ylim=rng)
segments(x,0,x,x)
abline(0,1,lwd=2)
abline(h=0)
par(opar)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Monte Carlo estimate}
<<infinite, echo=FALSE>>=
# Normal expectation
set.seed(1)
x = rnorm(1e3)
cmfx = cummean(x)
cvfx = cumvar(x)
uci  = cmfx+qnorm(.975)*sqrt(cvfx)
lci  = cmfx-qnorm(.975)*sqrt(cvfx)
plot(cmfx, main="Monte Carlo estimate", type="n", 
     ylim=range(uci,lci,na.rm=T), xlab="J", ylab="Estimate")
abline(h=0, col="red",lwd=2)
lines(cmfx,lwd=2)
lines(uci, lty=2)
lines(lci, lty=2)
legend("bottomright", c("Truth","Estimate","95% CI"), 
       lwd=c(2,2,1), lty=c(1,1,2), col=c("red","black","black"))
@
\end{frame}




\section{Independent binomials}
\subsection{3-point success in basketball}
\frame{\frametitle{Motivating example}
  	Is Andre Dawkins 3-point percentage higher in 2013-2014 than past years?
		
		\vspace{0.2in} \pause
		
		\begin{center}
		\begin{tabular}{lrr}
		Season & Made & Attempts \\
		\hline
		2009-2010 & 36 & 95  \\
		2010-2011 & 64 & 150 \\
    2011-2012 & 67 & 171 \\
  	2013-2014 & 64 & 152 \\
		\hline
		\end{tabular}
		\end{center}
}

\subsection{Binomial model}
\frame{\frametitle{Binomial model}
	Assume an independent binomial model,
	\[ Y_s \stackrel{ind}{\sim} Bin(n_s,\theta_s), \uncover<6->{\mbox{ i.e. }\quad   p(y|\theta) = \prod_{s=1}^S p(y_s|\theta_s) = \prod_{s=1}^S  {n_s\choose y_s} \theta_s^{y_s} (1-\theta_s)^{n_s-y_s}}  \]
	\pause where 
	\begin{itemize}
	\item $y_s$ is the number of 3-pointers made in season $s$ \pause
	\item $n_s$ is the number of 3-pointers attempted in season $s$ \pause 
	\item $\theta_s$ is the unknown 3-pointer success probability in season $s$ \pause
	\item $S$ is the number of seasons \pause \pause
	\item $\theta = (\theta_1,\theta_2,\theta_3,\theta_4)'$ and $y=(y_1,y_2,y_3,y_4)$
	\end{itemize}
	\pause and assume independent beta priors distribution:
	\[ p(\theta) = \prod_{s=1}^S p(\theta_s) \pause = \prod_{s=1}^S \frac{\theta_s^{a _s-1}(1-\theta_s)^{b _s-1}}{B(a _s,b _s)} \mathrm{I}(0<\theta_s<1).  \]
}

\frame{\frametitle{Joint posterior}
	Derive the posterior according to Bayes rule: \pause 
	\[ \begin{array}{ll}
	p(\theta|y) &\pause \propto p(y|\theta)p(\theta) \pause \\
	&= \prod_{s=1}^S p(y_s|\theta_s) \prod_{s=1}^S p(\theta_s)  \pause \\
	&= \prod_{s=1}^S p(y_s|\theta_s) p(\theta_s) \pause \\
	&\propto \prod_{s=1}^S \mbox{Beta}(\theta_s|a _s+y_s, b _s+n_s-y_s) \pause 
	\end{array} \]
	So the posterior for each $\theta_s$ is exactly the same as if we treated each season independently. 
}

\begin{frame}[fragile]
\frametitle{Joint posterior}
<<joint_posterior, echo=FALSE>>=
a = b = 1
d = data.frame(year=1:4, y=c(36,64,67,64), n=c(95,150,171,152))
d$a = a + d$y
d$b = b + d$n - d$y
plot(0, 0, type="n", main="Andre Dawkins's 3-point percentage", xlab=expression(theta), ylab="Posterior",
     xlim=c(0,1), ylim=c(0,max(dbeta(d$y/d$n,d$a,d$b))))
for (i in 1:nrow(d)) curve(dbeta(x, d$a[i], d$b[i]), col=i, lwd=2, lty=i, add=TRUE)
legend("topright", paste("Season", 1:nrow(d)), col=1:4, lwd=2, lty=1:4)
@
\end{frame}

\section{Monte Carlo estimates}
\begin{frame}[fragile]
\frametitle{Monte Carlo estimates}
Estimated means, medians, and quantiles.
<<echo=FALSE>>=
sim = ddply(d, .(year), 
            function(x) data.frame(theta=rbeta(1e3, x$a, x$b),
                                   a = x$a, b = x$b))

# hpd 
hpd = function(theta,a,b,p=.95) {
  h = dbeta((a-1)/(a+b-2),a,b)
  ftheta = dbeta(theta,a,b)
  r = uniroot(function(x) mean(ftheta>x)-p,c(0,h))
  range(theta[which(ftheta>r$root)])
}

# expectations
ddply(sim, .(year), summarize,
      mean = mean(theta),
      median = median(theta),
      ciL  = quantile(theta, c(.025,.975))[1],
      ciU  = quantile(theta, c(.025,.975))[2],
      hpdL = hpd(theta,a[1],b[1])[1],
      hpdU = hpd(theta,a[1],b[1])[2])
@
\end{frame}



\frame{\frametitle{Comparing probabilities across years}
	The scientific question of interest here is whether Dawkins's 3-point percentage is higher 2013-2014 than previously. \pause In probability notation this is 
	\[ P(\theta_4>\theta_s|y)\mbox{ for } s=1,2,3.\]
	\pause which can be approximated via Monte Carlo as 
	\[ P(\theta_4>\theta_s|y) = E_{\theta|y}[\mathrm{I}(\theta_4>\theta_s)]\approx \frac{1}{J} \sum_{j=1}^J \mathrm{I}\left(\theta_4^{(j)} > \theta_s^{(j)}\right) \]
	\pause where
	\begin{itemize}
	\item $\theta_s^{(j)} \stackrel{ind}{\sim} Be(a_s + y_s,b_s+n_s-y_s)$ \pause 
	\item $\mathrm{I}(A)$ is in indicator function that is 1 if $A$ is true and zero otherwise. 
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Estimated probabilities}
<<>>=
# Should be able to use dcast
d = data.frame(theta_1 = sim$theta[sim$year==1],
               theta_2 = sim$theta[sim$year==2],
               theta_3 = sim$theta[sim$year==3],
               theta_4 = sim$theta[sim$year==4])

# Probabilities that season 4 percentage is higher than other seasons
mean(d$theta_4 > d$theta_1)
mean(d$theta_4 > d$theta_2)
mean(d$theta_4 > d$theta_3)
@
\end{frame}


\subsection{JAGS}
\begin{frame}[fragile]
\frametitle{Using JAGS}
<<jags>>=
library(rjags)
independent_binomials = "
model {
  for (i in 1:N) { 
    y[i] ~ dbin(theta[i],n[i]) 
    theta[i] ~ dbeta(1,1)
  }
}
"

d = list(y=c(36,64,67,64), n=c(95,150,171,152), N=4)
m = jags.model(textConnection(independent_binomials), d)
res = coda.samples(m, "theta", 1000)
@
\end{frame}

\begin{frame}[fragile]
<<jags2>>=
summary(res)
@
\end{frame}


\begin{frame}[fragile]
<<jags3>>=
# Extract sampled theta values
theta = as.matrix(res[[1]]) # with only 1 chain, all values are in the first list element

# Calculate probabilities that season 4 percentage is higher than other seasons
mean(theta[,4] > theta[,1])
mean(theta[,4] > theta[,2])
mean(theta[,4] > theta[,3])
@
\end{frame}






\section{Probability theory}
\begin{frame}
\frametitle{Background probability theory}

\begin{itemize}
\item Scaled Inv-$\chi^2$ distribution
\item Location-scale $t$-distribution
\item Normal-Inv-$\chi^2$ distribution
\end{itemize}

\end{frame}


\subsection{Scaled-inverse $\chi$-square}
\begin{frame}
\frametitle{Scaled-inverse $\chi^2$-distribution}
  If $\sigma^2\sim IG(a,b)$, then $\sigma^2\sim \mbox{Inv-}\chi^2(v, s^2)$ \pause with 
  \begin{itemize}[<+->]
  \item $a=v/2$ and $b=vs^2/2$, \pause or, equivalently,
  \item $v=2a$ and $s^2=b/a$. 
  \end{itemize}
   
  \vspace{0.2in} \pause 
  
  Deriving from the inverse gamma, the scaled-inverse $\chi^2$ has 
  \begin{itemize}[<+->]
  \item Mean: $vs^2/(v-2)$ for $v>2$
  \item Mode: $vs^2/(v+2)$
  \item Variance: $2v^2(s^2)^2/[(v-2)^2(v-4)]$ for $v>4$
  \end{itemize}
   
  \vspace{0.2in} \pause 
  
  So $s^2$ is a point estimate and $v\to \infty$ means the variance decreases, since, for large $v$, 
  \[ \frac{2v^2(s^2)^2}{(v-2)^2(v-4)} \approx \frac{2v^2(s^2)^2}{v^3} = \frac{2(s^2)^2}{v}.   \]
\end{frame}


\begin{frame}[fragile]
\frametitle{Scaled-inverse $\chi^2$-distribution}
<<scaled_inverse_chi_square, echo=FALSE, fig.height=4.5>>=
dinvgamma = function(x, a, b,...) dgamma(1/x, a, b,...)/x^2
dsichisq = function(x, v, s2, ...) dinvgamma(x, v/2, v*s2/2, ...)
d = expand.grid(v=c(10,5,1), s2=c(1,2,3))
xmax = 5
par(mfcol=c(3,1), mar=c(2,2,0,0)+0.1)
d_ply(d, .(s2), function(x) {
  v = x$v; s2 = x$s2
  curve(dsichisq(x, v[1], s2), 0 , xmax, lwd=2, ylab="", main="", ylim=c(0,1))
  curve(dsichisq(x, v[2], s2), 0 , xmax, lwd=2, col=2, add=TRUE)
  curve(dsichisq(x, v[3], s2), 0 , xmax, lwd=2, col=3, add=TRUE)
  abline(v=s2,lty=2,col=8)
  legend("topright", c(paste("v=",v),"s2"), lwd=c(2,2,2,1), lty=c(1,1,1,2), col=c(1:3,8))
})
@
\end{frame}



\subsection{$t$-distribution}
\begin{frame}
\frametitle{Location-scale $t$-distribution}
  The $t$-distribution is a location-scale family (Casella \& Berger Thm 3.5.6), \pause i.e. if $T_v$ has a standard $t$-distribution with $v$ degrees of freedom and pdf
  \[ f_t(t) = \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}}\left(1+t^2/v\right)^{-(v+1)/2}, \]
  \pause then $X=m+sT_v$ has pdf 
  \[ f_X(x) = f_t([x-m]/s)/s \pause = \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}s}\left(1+\frac{1}{v}\left[\frac{x-m}{s}\right]^2\right)^{-(v+1)/2}. \]
  \pause This is referred to as a $t$ distribution with $v$ degrees of freedom, location $m$, and scale $s$; it is written as $t_{v}(m,s^2)$. \pause As $v\to \infty$, this family converges to a normal distribution with mean $m$ and variance $s^2$. 
\end{frame}



\begin{frame}[fragile]
\frametitle{$t$ distribution as $v$ changes}
<<scaled_inverse_chi_square2, echo=FALSE, fig.height=4.5>>=
dtms = function(x,v,m,s) dt((x-m)/s,v)/s
opar = par(mar=c(5,4,0,0)+.1, mfrow=c(1,2))
curve(dtms(x, 1, 0, 1), -3, 3, lwd=2, ylim=c(0,.45),
      ylab='f(x)')
curve(dtms(x, 10, 0, 1), add=TRUE, lty=2, col=2, lwd=2)
curve(dtms(x, 100, 0, 1), add=TRUE, lty=3, col=3, lwd=2)

curve(dtms(x, 1, 0, 1), 4, 6, lwd=2, ylim=c(0,.02), ylab='')
curve(dtms(x, 10, 0, 1), add=TRUE, lty=2, col=2, lwd=2)
curve(dtms(x, 100, 0, 1), add=TRUE, lty=3, col=3, lwd=2)
legend("topright", paste("v=",c(1,10,100)), lwd=2, lty=1:3, col=1:3)
par(opar)
@
\end{frame}






\begin{frame}
\frametitle{Normal-Inv-$\chi^2$ distribution}

Let $\mu|\sigma^2 \sim N(m,\sigma^2/k)$ and $\sigma^2 \sim \mbox{Inv-}\chi^2(v,s^2)$, \pause then the kernel of this joint density is

\[ \begin{array}{rl}
p(\mu,\sigma^2) &= p(\mu|\sigma^2)p(\sigma^2) \pause \\
&\propto (\sigma^2)^{-1/2} e^{-\frac{1}{2\sigma^2/k}(\mu-m)^2} (\sigma^2)^{-\frac{v}{2}-1} e^{-\frac{vs^2}{2\sigma^2}} \pause \\
&= (\sigma^2)^{-(v+3)/2} e^{-\frac{1}{2\sigma^2}\left[ k(\mu-m)^2 + vs^2 \right]}
\end{array} \]

\vspace{0.2in} \pause

In addition, the marginal distribution for $\mu$ is 
  \[ \begin{array}{rl}
  p(\mu) &= \int p(\mu|\sigma^2) p(\sigma^2) d\sigma^2 \pause \\
  &= \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}s/\sqrt{k}}\left(1+\frac{1}{v}\left[\frac{\mu-m}{s/\sqrt{k}}\right]^2\right)^{-(v+1)/2} \pause \\
  &\stackrel{d}{=} t_{v}(m, s^2/k)
  \end{array} \]
  
\end{frame}




\section{Univariate normal}
\begin{frame}[fragile]
\frametitle{Univariate normal model}
Suppose $Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2)$. 

<<normal_model, echo=FALSE>>=
curve(dnorm,-3,3,lwd=2, axes=F, frame=TRUE, 
      xlab="y", ylab=expression(p(y~"|"~mu,sigma^2)), main="Normal model")
abline(v=0, lty=2)
text(0,0,expression(mu),pos=4)
x = 1
arrows(-x,dnorm(-x),x,dnorm(x),code=3, lty=2)
text(x/2,dnorm(x), expression(sigma^2), pos=3)
@
\end{frame}

\subsection{Confidence interval}
\begin{frame}
\frametitle{Confidence interval for $\mu$}
  Let \pause 
  \[ \overline{y} = \frac{1}{n} \sum_{i=1}^n y_i \qquad \mbox{and} \qquad S^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i-\overline{y})^2.\]
  
  \pause Then, 
  \[ T = \frac{\overline{Y}-\mu}{S/\sqrt{n}} \pause \sim t_{n-1} \]
  
  \pause and an equal-tail $100(1-\alpha)$\% confidence interval can be constructed via 
  
  \[ 1-\alpha = P\left(-c \le T\le c\right) = P\left( \overline{Y} -\frac{cS}{\sqrt{n}} \le \mu \le \overline{Y} +\frac{cS}{\sqrt{n}} \right) \]
  
  \pause and thus $\overline{y} \pm c S/\sqrt{n}$ is an equal-tail 95\% confidence interval where $c=t_{n-1}(1-\alpha/2)$ is the t-critical value.
\end{frame}
  

\subsection{Priors}
\begin{frame}
\frametitle{Default priors}

Jeffreys prior can be shown to be $p(\mu,\sigma^2) \propto (1/\sigma^2)^{3/2}$. \pause But alternative methods, e.g. reference prior, find that $p(\mu,\sigma^2) \propto 1/\sigma^2$ is a more appropriate prior. 

\vspace{0.2in} \pause

The posterior under the reference prior is 
\[ \begin{array}{rl}
p(\mu,\sigma^2|y) &\propto (\sigma^2)^{-n/2}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i\phantom{{}-\overline{y}{}+\overline{y}}-\mu)^2 \right) \times \frac{1}{\sigma^2} \pause \\
&= (\sigma^2)^{-n/2}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\overline{y}+\overline{y}-\mu)^2 \right) \times \frac{1}{\sigma^2} \pause \\
& \vdots \\
&= (\sigma^2)^{-(n-1+3)/2} \exp\left( -\frac{1}{2\sigma^2} \left[ n(\mu-\overline{y})^2 + (n-1) S^2 \right]\right)  
\end{array} \]
\pause Thus
\[ \mu|\sigma^2,y \sim N(\overline{y}, \sigma^2/n) \qquad \sigma^2 \sim \mbox{Inv-}\chi^2(n-1,S^2). \]
\end{frame}


\begin{frame}
\frametitle{Marginal posterior for $\mu$}
  The marginal posterior for $\mu$ is 
  \[ \mu|y \sim t_{n-1}(\overline{y}, S^2/n). \]
  
  \vspace{0.2in} \pause
  
  An equal-tailed $100(1-\alpha)$\% credible inteval can be obtained via 
  \[ \overline{y} \pm c S/\sqrt{n} \]
  where $c = t_{n-1}(1-\alpha)$ is the t-critical value from before. \pause This is exactly the same as the $100(1-\alpha)$\% confidence inteval.
\end{frame}


\begin{frame}
\frametitle{Predictive distribution}
  Let $\tilde{y} \sim N(\mu,\sigma^2)$. \pause The predictive distribution is 
  \[ \int \int p(\tilde{y}|\mu,\sigma^2)p(\mu|\sigma^2,y) p(\sigma^2|y) d\mu d\sigma^2 \]
  \pause
  The easiest way to derive this is to write $\tilde{y} = \mu + \epsilon$ with 
  \[ \mu|\sigma^2 \sim N(\overline{y}, \sigma^2/n) \qquad \epsilon|\sigma^2 \sim N(0,\sigma^2) \]
  independent of each other. \pause Thus
  \[ \tilde{y}|\sigma^2 \sim N(\overline{y}, \sigma^2[1+1/n]). \]
  \pause
  with $\sigma^2 \sim \mbox{Inv-}\chi^2(n-1,S^2)$. \pause Now, we can use the Normal-Inv-$\chi^2$ theory, to find that 
  \[ \tilde{y} \sim t_{n-1}(\overline{y}, S^2[1+1/n]). \]
\end{frame}



\begin{frame}
\frametitle{Conjugate prior for $\mu$ and $\sigma^2$}
  The joint conjugate prior for $\mu$ and $\sigma^2$ is 
  \[ \mu|\sigma^2\phantom{,y} \sim N(m,\sigma^2/k) \qquad \sigma^2\phantom{,y} \sim \mbox{Inv-}\chi^2(v,s^2) \]
  \pause where $s^2$ serves as a prior guess about $\sigma^2$ and $v$ controls how certain we are about that guess. 
  
  \vspace{0.2in} \pause 

  The posterior under this prior is 
  \[ \mu|\sigma^2,y \sim N(m',\sigma^2/k') \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v',(s')^2) \]
  \pause where 
  \[ \begin{array}{rl}
  k' &= k+n \\
  m' &= [km + n\overline{y}]/k' \\
  v' &= v+n \\
  v'(s')^2 &= vs^2 + (n-1)S^2 + \frac{kn}{k'}(\overline{y}-m)^2
  \end{array} \]
\end{frame}



\begin{frame}
\frametitle{Marginal posterior for $\mu$}
  The marginal posterior for $\mu$ is 
  \[ \mu|y \sim t_{v'}(m', (s')^2/k'). \]
  
  \vspace{0.2in} \pause
  
  An equal-tailed $100(1-\alpha)$\% credible inteval can be obtained via 
  \[ m' \pm c s'/\sqrt{k'} \]
  where $c = t_{v'}(1-\alpha)$ is the t-critical value from before. 
\end{frame}



\begin{frame}
\frametitle{Marginal posterior via simulation}

An alternative to deriving the closed form posterior for $\mu$ is to simulate from the distribution. \pause Recall that 
\[  \mu|\sigma^2,y \sim N(m',\sigma^2/k') \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v',(s')^2) \]
\pause
To obtain a simulation from the posterior distribution $p(\mu,\sigma^2|y)$, do the following
\begin{enumerate}[<+->]
\item Calculate $m', k', v',$ and $s'$. 
\item Simulate $\sigma^2 \sim \mbox{Inv-}\chi^2(v',(s')^2)$.
\item Using this value, simulate $\mu\sim  N(m',\sigma^2/k')$. 
\end{enumerate}
\pause
Not only does this provide a sample from the joint distribution for $\mu,\sigma$ but it also (therefore) provides a sample from the marginal distribution for $\mu$. \pause The integral was suggestive:
\[  p(\mu|y) = \int p(\mu|\sigma^2,y) p(\sigma^2|y) d\sigma^2 \]
\end{frame}



\begin{frame}
\frametitle{Predictive distribution via simulation}

Similarly, we can obtain the predictive distribution via simulation. \pause Recall that 
\[  p(\tilde{y}|y) = \int \int p(\tilde{y}|\mu,\sigma^2) p(\mu|\sigma^2,y) p(\sigma^2|y) d\mu d\sigma^2 \]
\pause
To obtain a simulation from the predictive distribution $p(\tilde{y}|y)$, do the following
\begin{enumerate}[<+->]
\item Calculate $m', k', v',$ and $s'$. 
\item Simulate $\sigma^2 \sim \mbox{Inv-}\chi^2(v',(s')^2)$.
\item Using this value, simulate $\mu\sim  N(m',\sigma^2/k')$.
\item Using $\mu$ and $\sigma^2$ from above, simulate $\tilde{y}\sim N(\mu,\sigma^2)$.
\end{enumerate}
\end{frame}


% 
% \subsection{Activity}
% \begin{frame}
% \frametitle{Prior elicitation for average class height}
%   Answer the following questions:
%   \begin{itemize}[<+->]
%   \item What do you think the average height in this STAT 544 is (without looking around the room)?
%   \item How many prior observations is this worth?
%   \item What is the standard deviation of heights in the class?
%   \item How many observations is this worth?
%   \end{itemize}
% \end{frame}
% 
% 
% \begin{frame}[fragile]
% <<height_activity>>=
% # Prior elicitation 
% prior = list(m  = 67,    # Mean height 5'7''
%              k  = 14,    
%              s2 = 3.5^2,
%              v  = 15)
% 
% tmp = read.csv("heights.csv")
% heights = tmp$height[tmp$class=="stat544spring2014"]
% 
% normal_post = function(y,prior) {
%   n  = length(y)
%   kp = prior$k+n
%   vp = prior$v+n
%   return(list(k = kp, 
%               v = prior$v+n, 
%               m = (prior$k*prior$m+sum(y))/kp,
%               s2 = (prior$v*prior$s2+(n-1)*var(y)+prior$k*n/kp*(mean(y)-prior$m)^2)/vp))
% }
% 
% post = normal_post(heights, prior)
% 
% # Credible interval
% cred_int = function(p, prob=0.95) { p$m+c(-1,1)*qt(1-(1-prob)/2,p$v)*sqrt(p$s2/p$k) }
% cred_int(post)
% @
% \end{frame}
% 
% \begin{frame}[fragile]
% \frametitle{Prior vs posterior}
% <<height_activity_plot, fig.width=8>>=
% dt2 = function(x, df, m, s, ...) dt((x-m)/s, df, ...)/s # Location-scale family
% curve(dt2(x, post$k, post$m, sqrt(post$s2/post$k)), 5.2*12, 6*12, col="red", lwd=2,
%       main="Mean height of students in STAT 544", xlab="Height (inches)", ylab="Density")
% curve(dt2(x, prior$k, prior$m, sqrt(prior$s2/prior$k)), col="blue", lwd=2, add=TRUE)
% legend("topleft", c("Prior","Posterior"), col=c("blue","red"), lwd=2)
% @
% \end{frame}
% 
% \subsection{Non-informative prior}
% \begin{frame}
% \frametitle{Non-informative prior}
%   A widely accepted non-informative prior for $\mu,\sigma^2$ is $p(\mu,\sigma^2) \propto 1/\sigma^2$ \pause which can be thought of as the prior as $k\to 0$ and $v\to 0$. \pause The posterior under this prior is 
%   \[ \begin{array}{rl}
%   \mu|\sigma^2,y &\sim N(m',\sigma^2/k') \\
%   \sigma^2|y &\sim \mbox{Inv-}\chi^2(v',(s')^2) 
%   \end{array} \]
%   \pause where 
%   \[ \begin{array}{rl}
%   k' &= n \\
%   m' &= \overline{y} \pause \\
%   v' &= \alert{n-1} \\
%   (s')^2 &= \alert{S^2},
%   \end{array} \]
%   \pause which is NOT the same as setting $v=0$. 
%   
%   \vspace{0.2in} \pause
%   
%   The marginal distribution for $\mu$ is $t_{n-1}(\overline{y}, s^2/n)$ \pause and an equal-tailed $100(1-\alpha)$\% credible interval is
%   \[ \overline{y} \pm c s/\sqrt{n} \qquad c = t_{n-1}(1-\alpha/2). \]
% \end{frame}
% 
% \subsection{Coverage rates}
% \begin{frame}
% \frametitle{Simulation study: confidence vs credible intervals}
%   Consider the following simulation study:
%   \begin{enumerate}[<+->]
%   \item Repeat the following many times:
%     \begin{itemize}
%     \item Sample 4 heights randomly (without replacement) from students in this class.
%     \item Calculate the credible and confidence intervals for mean class height.
%     \end{itemize}
%   \item Calculate the mean class height.
%   \item Record the percentage of times the credible and confidence intervals cover the truth.
%   \end{enumerate}
%   
%   \vspace{0.2in} \pause 
%   
%   Questions:
%   \begin{itemize}
%   \item What percentage of the confidence intervals are expected to cover the truth?
%   \item What percentage of the credible intervals are expected to cover the truth?
%   \end{itemize}
% \end{frame}
% 
% \begin{frame}[fragile]
% \frametitle{Coverage rate simulation study}
% <<coverage, eval=FALSE>>=
% conf_int = function(y, prob=0.95) { 
%   n = length(y)
%   mean(y) + c(-1,1)*qt(1-(1-prob)/2,n-1)*sqrt(var(y)/n)
% }
% set.seed(1)
% 
% sim = function(y,prior,size=4,prob=0.5) {
%   sample = sample(y,size)
%   cred = cred_int(normal_post(sample,prior),prob=prob)
%   conf = conf_int(sample,prob=prob)
%   data.frame(interval=c("credible","confidence"),
%              L = c(cred[1], conf[1]), U = c(cred[2], conf[2]))
% }
% d = rdply(100, sim(heights,prior))
% d$y = d$.n + (d$interval=="confidence")*0.1-0.05
% 
% p <- ggplot(d[1:40,], aes(x=L, y=y, xend=U, yend=y, colour=interval)) + geom_segment() + geom_vline(xintercept=mean(heights)) + labs(x=expression(mu), y="Simulation")
% print(p)
% @
% \end{frame}
% 
% 
% 
% \begin{frame}[fragile]
% <<coverage_plot, echo=FALSE>>=
% <<coverage>>
% @
% \end{frame}
% 
% \begin{frame}[fragile]
% \frametitle{Coverage rates}
% <<coverage_rate>>=
% m = mean(heights)
% ddply(d, .(interval), summarize, coverage=mean(I(L<m) & I(m<U)))
% @
% \end{frame}
% 
% 
% \subsection{JAGS}
% \begin{frame}[fragile]
% <<JAGS>>=
% library(rjags)
% model = "
% model {
%   for (i in 1:n) {
%     y[i] ~ dnorm(mu, tau) # tau is the precision
%   }
%   mu ~ dnorm(m,tau*k)
%   tau ~ dgamma(v/2, v*s2/2)
%   sigma <- 1/sqrt(tau)
% }"
% 
% dat = prior
% dat$n = length(heights)
% dat$y = heights
% 
% fit = jags.model(textConnection(model), dat, n.chains=3)
% s = coda.samples(fit,c("mu","sigma"), n.iter=1000)
% @
% \end{frame}
% 
% \begin{frame}[fragile]
% <<JAGS_plot>>=
% plot(s,trace=FALSE)
% @
% \end{frame}
% 
% \begin{frame}[fragile]
% <<JAGS_result>>=
% summary(s)
% @
% \end{frame}





\end{document}
