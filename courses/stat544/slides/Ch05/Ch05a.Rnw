\documentclass[handout]{beamer}

\usepackage{verbatim}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}



\title{Hierarchical models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
library(rstan)
library(GGally)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}


\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Motivating example
  \begin{itemize}
  \item Independent vs pooled estimates
  \end{itemize}
\item Hierarchical models
  \begin{itemize}
  \item General structure
  \item Posterior distribution
  \end{itemize}
\item Binomial hierarchial model
  \begin{itemize}
  \item Posterior distribution
  \item Prior distributions
  \end{itemize}
\item Stan for binomial hierarchical model
  \begin{itemize}
  \item proper prior
  \item improper prior
  \item integrating out $\theta$
  \item across seasons
  \end{itemize}
\end{itemize}

\end{frame}

\section{Modeling}
\frame{\frametitle{Andre Dawkin's three-point percentage}
  Suppose $Y_i$ are the number 3-pointers Andre Dawkin's makes in season $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \]
  where 
  \begin{itemize}[<+->]
  \item $n_i$ are the number of 3-pointers attempted and
  \item $\theta_i$ is the probability of making a 3-pointer in season$i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every season is the same, i.e. $\theta_i=\theta$. 
  \item The 3-point percentage every season is independent of other seasons. 
  \item The 3-point percentage every season should be similar to other seasons. 
  \end{itemize}
}


\frame{\frametitle{Andre Dawkin's three-point percentage}
  Suppose $Y_i$ are the number of 3-pointers Andre Dawkin's makes in \alert{game} $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \]
  where 
  \begin{itemize}[<+->]
  \item $n_i$ are the number of 3-pointers attempted in game $i$ and
  \item $\theta_i$ is the probability of making a 3-pointer in game $i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every game is the same, i.e. $\theta_i=\theta$. 
  \item The 3-point percentage every game is independent of other games. 
  \item The 3-point percentage every game should be similar to other games. 
  \end{itemize}
}

<<data, results='hide'>>=
d = structure(list(date = structure(c(16L, 9L, 10L, 11L, 12L, 13L, 
14L, 15L, 20L, 17L, 18L, 19L, 21L, 7L, 8L, 1L, 2L, 3L, 4L, 5L, 
6L, 22L, 23L, 24L), .Label = c("1/11/14", "1/13/14", "1/18/14", 
"1/22/14", "1/25/14", "1/27/14", "1/4/14", "1/7/14", "11/12/13", 
"11/15/13", "11/18/13", "11/19/13", "11/24/13", "11/27/13", "11/29/13", 
"11/8/13", "12/16/13", "12/19/13", "12/28/13", "12/3/13", "12/31/13", 
"2/1/14", "2/4/14", "2/8/14"), class = "factor"), opponent = structure(c(5L, 
13L, 9L, 21L, 6L, 22L, 1L, 2L, 15L, 11L, 20L, 7L, 8L, 17L, 12L, 
4L, 23L, 16L, 14L, 10L, 18L, 19L, 24L, 3L), .Label = c("alabama", 
"arizona", "boston college", "clemson", "davidson", "east carolina", 
"eastern michigan", "elon", "florida atlantic", "florida state", 
"gardner-webb", "georgia tech", "kansas", "miami", "michigan", 
"nc state", "notre dame", "pitt", "syracuse", "ucla", "unc asheville", 
"vermont", "virginia", "wake forest"), class = "factor"), made = c(0L, 
0L, 5L, 3L, 0L, 3L, 0L, 1L, 2L, 4L, 1L, 6L, 5L, 1L, 1L, 0L, 1L, 
3L, 2L, 3L, 6L, 4L, 4L, 0L), attempts = c(0L, 0L, 8L, 6L, 1L, 
9L, 2L, 1L, 2L, 8L, 5L, 10L, 7L, 4L, 5L, 4L, 1L, 7L, 6L, 6L, 
7L, 9L, 7L, 1L)), .Names = c("date", "opponent", "made", "attempts"
), class = "data.frame", row.names = c(NA, -24L))
@


\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
<<dawkins_data>>=
d = rbind(d, data.frame(date=NA, opponent='Total', made=sum(d$made), attempts=sum(d$attempts)))
d = mutate(d, 
           a = 0.5 + made,
           b = 0.5 + attempts-made,
           lcl = qbeta(0.025,a,b),
           ucl = qbeta(0.975,a,b),
           Estimate = ifelse(opponent=="Total", "Combined", "Individual"))
d$game = 1:nrow(d)
ggplot(d, 
       aes(x     = lcl, 
           xend  = ucl, 
           y     = game, 
           yend  = game, 
           color = Estimate))+
  geom_segment(lwd=2) + 
#  theme(legend.position="none") + 
  labs(x=expression(theta))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
{\tiny
<<results='asis'>>=
xtable(d)
d = subset(d, opponent!="Total")
@
}
\end{frame}


\section{Hierarchical models}
\frame{\frametitle{Hierarchical models}
  Consider the following model
	\[ \begin{array}{ll}
	y_i &\stackrel{ind}{\sim} p(y|\theta_i) \pause \\
	\theta_i &\stackrel{ind}{\sim} p(\theta|\phi) \pause \\
	\phi &\sim p(\phi)
	\end{array} \]
  \pause where 
  \begin{itemize}
  \item $y_i$ is observed\pause, 
	\item $\theta=(\theta_1,\ldots,\theta_n)$ and $\phi$ are parameters\pause, and 
	\item only $\phi$ has a prior that is set.
	\end{itemize}
	
	\vspace{0.2in} \pause 
	
	This is a hierarchical or multilevel model.
}

\subsection{Posteriors}
\frame{\frametitle{Posterior distribution for hierarchical models}
	The joint posterior distribution of interest in hierarchical models is 
{\small
	\[ \begin{array}{rl}
  p(\theta,\phi|y) \pause \propto p(y|\theta,\phi)p(\theta,\phi) \pause = p(y|\theta)p(\theta|\phi)p(\phi)\pause = \left[ \prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\phi)\right] p(\phi). 
  \end{array} \]
}
	\pause The joint posterior distribution can be decomposed via 
  \[ p(\theta,\phi|y) = p(\theta|\phi,y)p(\phi|y) \]
  where 
{\small
  \[ \begin{array}{rl}
  p(\theta|\phi,y) &\propto p(y|\theta)p(\theta|\phi) \pause = \prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\phi) \pause \propto \prod_{i=1}^n p(\theta_i|\phi,y_i) \pause \\
  p(\phi|y) &\propto p(y|\phi)p(\phi) \pause \\
  p(y|\phi) &= 
  \int p(y|\theta)p(\theta|\phi) d\theta \pause \\
  &= \int \cdots \int  \prod_{i=1}^n  \left[ p(y_i|\theta_i)p(\theta_i|\phi) \right] d\theta_1 \cdots d\theta_n \pause \\
  &= \prod_{i=1}^n \int p(y_i|\theta_i)p(\theta_i|\phi) d\theta_i \pause \\
  &= \prod_{i=1}^n p(y_i|\phi)  
  \end{array} \]
}
}

\section{Binomial hierarchical model}
\frame{\frametitle{Three-pointer example}
	Our statistical model 
	\[ \begin{array}{ll}
	Y_i &\stackrel{ind}{\sim} Bin(n_i,\theta_i) \pause \\
	\theta_i&\stackrel{ind}{\sim} Be(\alpha,\beta) \pause \\
	\alpha,\beta &\sim p(\alpha,\beta) 
	\end{array} \]
	
	\vspace{0.2in} \pause
	
	In this example,
	\begin{itemize}
	\item $\phi=(\alpha,\beta)$\pause\,
	\item $Be(\alpha,\beta)$ describes the variability in 3-point percentage across games\pause, and
	\item we are going to learn about this variability. 
	\end{itemize}
}

\begin{frame}
\frametitle{Decomposed posterior}
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \quad \theta_i\stackrel{ind}{\sim} Be(\alpha,\beta) \quad \alpha,\beta \sim p(\alpha,\beta) \]
  Conditional posterior for $\theta$:
\[ p(\theta|\alpha,\beta,y) = \prod_{i=1}^n p(\theta_i|\alpha,\beta,y_i) \pause = \prod_{i=1}^n Be(\theta_i|\alpha+y_i,\beta+n_i-y_i) \]
\pause 
Marginal posterior for $(\alpha,\beta)$:
{\small 
\[ \begin{array}{rl}
p(\alpha,\beta|y) &\propto p(y|\alpha,\beta)p(\alpha,\beta) \pause \\
p(y|\alpha,\beta) &= \prod_{i=1}^n p(y_i|\alpha,\beta) \pause = \prod_{i=1}^n \int p(y_i|\theta_i)p(\theta_i|\alpha,\beta) d\theta_i \pause \\
&= \prod_{i=1}^n \int Bin(y_i|n_i,\theta_i)Be(\theta_i|\alpha,\beta) d\theta_i \pause \\
&= \prod_{i=1}^n \int_0^1 {n_i\choose y_i} \theta_i^{y_i} (1-\theta_i)^{n_i-y_i} \frac{\theta_i^{\alpha-1}(1-\theta_i)^{\beta-1}}{B(\alpha,\beta)} d\theta_i \pause \\
&= \prod_{i=1}^n {n_i\choose y_i}\frac{1}{B(\alpha,\beta)} \int_0^1 \theta_i^{\alpha+y_i-1} (1-\theta_i)^{\beta+n_i-y_i-1}  d\theta_i \pause \\
&= \prod_{i=1}^n {n_i\choose y_i}\frac{B(\alpha+y_i,\beta+n_i-y_i)}{B(\alpha,\beta)} 
\end{array} \]
}
\pause
Thus $y_i|\alpha,\beta \stackrel{ind}{\sim} \mbox{Beta-binomial}(n_i,\alpha,\beta)$. 
\end{frame}



\subsection{Prior}
\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  Recall the interpretation: \pause
  \begin{itemize}[<+->]
  \item $\alpha$: prior successes
  \item $\beta$: prior failures
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  A more natural parameterization is 
  \begin{itemize}[<+->]
  \item prior expectation: $\mu = \frac{\alpha}{\alpha+\beta}$
  \item prior sample size: $\eta = \alpha + \beta$
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  Place priors on these parameters or transformed to the real line:
  \begin{itemize}[<+->]
  \item $\mbox{logit } \mu = \log(\mu/[1-\mu]) = \log(\alpha/\beta)$
  \item $\log \eta$
  \end{itemize}
}

\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  It seems reasonable to assume the mean ($\mu$) and size ($\eta$) are independent \emph{a priori}:
  \[ p(\mu,\eta) = p(\mu)p(\eta) \]
  
  \vspace{0.2in} \pause
  
  Let's assume a proper prior for $\mu$ and $\eta$ \pause perhaps
  \begin{itemize}
  \item $\mu \sim Be(1,1)$
  \item $\eta \sim LN(0,1)$
  \end{itemize}
  \pause where $LN(0,1)$ is a log-normal distribution, i.e. $\log(\eta) \sim N(0,1)$.
}

\begin{frame}[fragile]
\frametitle{Prior draws}
<<proper_prior, echo=TRUE>>=
n = 1e4
prior_draws = mutate(data.frame(mu  = runif(n, 0 , 1), eta = rlnorm(n, 0, 1)),
                      alpha = eta*   mu,
                      beta  = eta*(1-mu))
ddply(melt(prior_draws), .(variable), function(x) quantile(x$value, prob=c(.025,.5,.975)))

@
\end{frame}



\begin{frame}[fragile]
<<proper_prior_plot, out.width='0.8\\textwidth'>>=
ggpairs(prior_draws, lower=list(continuous='density'))
@
\end{frame}



\section{Stan}
\begin{frame}[fragile]
\frametitle{Stan}
<<stan, echo=TRUE>>=
model = "
data {
  int<lower=0> N;
  int<lower=0> n[N];
  int<lower=0> y[N];
}
parameters {
  real<lower=0,upper=1> mu;
  real<lower=0> eta;
  real<lower=0,upper=1> theta[N];
}
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;

  alpha <- eta*   mu ;
  beta  <- eta*(1-mu);
}
model {
  mu    ~ beta(1,1);
  eta   ~ lognormal(0,10);

  # implicit joint distributions
  theta ~ beta(alpha,beta); 
  y     ~ binomial(n,theta);
}
"
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan}
<<stan_run, dependson='stan', echo=TRUE, results='hide'>>=
dat = list(y=d$made, n=d$attempts, N=nrow(d))
m = stan_model(model_code=model)
r = sampling(m, dat, c("mu","eta","alpha","beta","theta"))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{stan}
<<stan_post, dependson="stan_run">>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{stan}
<<stan_plot, dependson="stan_run">>=
plot(r)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Comparing independent and hierarchical models}
<<quantiles, dependson="stan_run">>=
d$model = "independent"

tmp = data.frame(summary(r)$summary[,c(4,8)])
new_d = mutate(d,
               model = "hierarchical",
               lcl = tmp[5:28,1],
               ucl = tmp[5:28,2])

e = 0.2
ggplot(rbind(d, new_d), aes(x=lcl, 
                     xend=ucl, 
                     y=game+e*(model=="hierarchical"), 
                     yend=game+e*(model=="hierarchical"), 
                     color=model))+
  geom_segment(lwd=2, alpha=0.5) + 
  labs(x=expression(theta), y="game")
@
\end{frame}




\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  In Bayesian Data Analysis (3rd ed) page 110, several priors are discussed 
  
  \vspace{0.2in} \pause
  
	\begin{itemize}
	\item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto 1$ leads to an improper posterior.
  
  \vspace{0.2in} \pause
  
  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \sim Unif([-10^{10},10^{10}] \times [-10^{10},10^{10}])$ \pause while proper and seemingly vague is a very informative prior.
  
  \vspace{0.2in} \pause
	
  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto \alpha\beta(\alpha+\beta)^{-5/2}$ \pause which leads to a proper posterior \pause and is equivalent to $p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}$.
	\end{itemize}
}



\subsection{improper prior}
\begin{frame}[fragile]
\frametitle{Stan - improper prior}
<<stan2_run, echo=TRUE, results='hide'>>=
model2 = "
data {
  int<lower=0> N;
  int<lower=0> n[N];
  int<lower=0> y[N];
}
parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  real<lower=0,upper=1> theta[N];
}

model {
  # improper prior
  increment_log_prob(-5*log(alpha+beta)/2);

  # implicit joint distributions
  theta ~ beta(alpha,beta); 
  y     ~ binomial(n,theta);
}
"

m2 = stan_model(model_code=model2)
r2 = sampling(m2, dat, c("alpha","beta","theta"))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan - improper prior}
<<stan_post2, dependson="stan2_run">>=
r2
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - improper prior}
<<stan_plot2, dependson="stan2_run">>=
plot(r2)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Comparing all models}
<<quantiles2, dependson=c("stan_run","stan2_run")>>=
d$prior     = "independent"
new_d$prior = "proper"

tmp = data.frame(summary(r2)$summary[,c(4,8)])
new_d2 = mutate(new_d,
                prior = "improper",
                lcl = tmp[3:26,1],
                ucl = tmp[3:26,2])

e = 0.2
ggplot(rbind(d,new_d,new_d2), 
       aes(x     = lcl, 
           xend  = ucl, 
           y     = game+e*(prior=="improper")-e*(prior=="independent"), 
           yend  = game+e*(prior=="improper")-e*(prior=="independent"), 
           color = prior))+
  geom_segment(lwd=2, alpha=0.5) + 
  labs(x=expression(theta), y="game")
@
\end{frame}




\subsection{beta-binomial}
\begin{frame}
\frametitle{Marginal posterior for $\alpha,\beta$}

An alternative to jointly sampling $\theta,\alpha,\beta$ is to 
\begin{enumerate}
\item sample $\alpha,\beta\sim p(\alpha,\beta|y)$\pause, and then
\item sample $\theta_i \stackrel{ind}{\sim} p(\theta_i|\alpha,\beta,y_i) \stackrel{d}{=} Be(\alpha+y_i,\beta+n_i-y_i)$. 
\end{enumerate}

\vspace{0.2in} \pause

The maginal posterior for $\alpha,\beta$ is 
\[ p(\alpha,\beta|y) \propto p(y|\alpha,\beta)p(\alpha,\beta) = \left[ \prod_{i=1}^n \mbox{Beta-binomial}(y_i|n_i,\alpha,\beta) \right] p(\alpha,\beta) \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan3_run, echo=TRUE, results='hide'>>=
model3 = "
data {
  int<lower=0> N;
  int<lower=0> n[N];
  int<lower=0> y[N];
}
parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
}
model {
  increment_log_prob(-5*log(alpha+beta)/2);
  y     ~ beta_binomial(n,alpha,beta);
}
"

m3 = stan_model(model_code=model3)
r3 = sampling(m3, dat, c("alpha","beta"))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan3_post, dependson="stan3_run">>=
r3
@
\end{frame}


\begin{frame}[fragile]
\frametitle{}

Posterior samples for $\alpha$ and $\beta$ 

<<stan3_alpha_beta, dependson="stan3_run", echo=FALSE>>=
samples = extract(r3, c("alpha","beta"))
ggpairs(data.frame(log_alpha = log(as.numeric(samples$alpha)), log_beta  = log(as.numeric(samples$beta))),
        lower  = list(continuous='density'))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Posterior sample for $\theta_{22}$}
<<stan3_theta, dependson="stan3_run", fig.width=10>>=
game = 22
theta22 = rbeta(length(samples$alpha), 
              samples$alpha + d$made[game], 
              samples$beta  + d$attempts[game] - d$made[game])
hist(theta22, 100, 
     main=paste("Posterior for game against", d$opponent[game], "on", d$date[game]),
     xlab="3-point probability", 
     ylab="Posterior")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{$\theta$s are not independent in the posterior}
<<stan3_thetas, dependson="stan3_run", out.width='\\textwidth'>>=
game = 23
theta23 = rbeta(length(samples$alpha), 
              samples$alpha + d$made[game], 
              samples$beta  + d$attempts[game] - d$made[game])
ggpairs(data.frame(theta22=theta22,theta23=theta23), lower=list(continuous='density'))
@
\end{frame}



\subsection{3-point percentage across seasons}
\begin{frame}[fragile]
\frametitle{3-point percentage across seasons}

An alternative to modeling game-specific 3-point percentage is to model 3-point percentage in a season. \pause The model is exactly the same, but the data changes. \pause

<<season_data, results='asis'>>=
d = data.frame(season=1:4, y=c(36,64,67,64), n=c(95,150,171,152))
xtable(d, digits=0)
@

Due to the low number of seasons (observations), we will use the proper prior for $\alpha$ and $\beta$. 

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan_season, dependson='season_data', echo=TRUE, results='hide'>>=
model4 = "
data {
  int<lower=0> N;
  int<lower=0> n[N];
  int<lower=0> y[N];
  real eta_sd;
}
parameters {
  real<lower=0,upper=1> mu;
  real<lower=0> eta;
}  
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  alpha <- eta *    mu;
  beta  <- eta * (1-mu);
}
model {
  mu  ~ beta(20,30);
  eta ~ lognormal(0,eta_sd);
  y   ~ beta_binomial(n,alpha,beta);
}
generated quantities {
  real<lower=0,upper=1> theta[N];
  for (i in 1:N) theta[i] <- beta_rng(alpha+y[i], beta+n[i]-y[i]);
}
"

m4 = stan_model(model_code=model4)
r_seasons = sampling(m4, list(N=nrow(d), y=d$y, n=d$n, eta_sd=2), 
                     c("alpha","beta","mu","eta","theta"), seed=1)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

<<stan_season_summary, dependson="stan_season">>=
r_seasons
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

<<stan_season_posteriors, dependson="stan_season">>=
posterior = melt(extract(r_seasons, "theta")[[1]])
names(posterior)[2] = "season"
posterior$season = factor(posterior$season)
ggplot(posterior, aes(x=value, fill=season)) + geom_density(alpha=0.5)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

Probabilities that 3-point percentage is greater in season 4 than in the other seasons:

<<stan_season_comparison, echo=TRUE, dependson="stan_season">>=
theta = extract(r_seasons, "theta")[[1]] 
mean(theta[,4] > theta[,1])
mean(theta[,4] > theta[,2])
mean(theta[,4] > theta[,3])
@

\end{frame}



\subsection{Summary}
\frame{\frametitle{Summary - hierarchical models}
  Hierarchical models: \pause
	\begin{itemize}[<+->]
	\item Basic structure
	\[ y\sim p(y|\theta) \qquad \theta\sim p(\theta|\phi) \qquad \phi\sim p(\phi) \phantom{|\psi\qquad \psi\sim p(\psi)} \]
	\item \pause Extension (one more level in the hierarchy)
	\[ y\sim p(y|\theta) \qquad \theta\sim p(\theta|\phi) \qquad \phi\sim p(\phi|\psi) \qquad \psi\sim p(\psi) \]
	\item \pause When deriving posteriors, remember the conditional independence structure, \pause e.g.
	\[ p(\theta,\phi,\psi|y) \propto p(y|\theta) p(\theta|\phi) p(\phi|\psi) p(\psi) \]
	\end{itemize}
}



\end{document}
