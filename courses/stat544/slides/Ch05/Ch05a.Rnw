\documentclass[handout]{beamer}

\usepackage{verbatim}

\input{../frontmatter}
\input{../commands}

\title{Hierarchical models}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
# library("reshape2")
library("dplyr")
library("xtable")

library("ggplot2")
library("GGally")
library("RColorBrewer")

library("rstan")
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}


\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Motivating example
  \begin{itemize}
  \item Independent vs pooled estimates
  \end{itemize}
\item Hierarchical models
  \begin{itemize}
  \item General structure
  \item Posterior distribution
  \end{itemize}
\item Binomial hierarchial model
  \begin{itemize}
  \item Posterior distribution
  \item Prior distributions
  \end{itemize}
\item Stan for binomial hierarchical model
  \begin{itemize}
  \item informative prior
  \item default prior
  \item integrating out $\theta$
  \item across seasons
  \end{itemize}
\end{itemize}

\end{frame}

\section{Modeling}
\frame{\frametitle{Andre Dawkin's three-point percentage}
  Suppose $Y_i$ are the number 3-pointers Andre Dawkin's makes in season $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \]
  where 
  \begin{itemize}[<+->]
  \item $n_i$ are the number of 3-pointers attempted and
  \item $\theta_i$ is the probability of making a 3-pointer in season$i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every season is the same, i.e. $\theta_i=\theta$. 
  \item The 3-point percentage every season is independent of other seasons. 
  \item The 3-point percentage every season should be similar to other seasons. 
  \end{itemize}
}


\frame{\frametitle{Andre Dawkin's three-point percentage}
  Suppose $Y_i$ are the number of 3-pointers Andre Dawkin's makes in \alert{game} $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \]
  where 
  \begin{itemize}[<+->]
  \item $n_i$ are the number of 3-pointers attempted in \alert{game} $i$ and
  \item $\theta_i$ is the probability of making a 3-pointer in \alert{game} $i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every game is the same, i.e. $\theta_i=\theta$. 
  \item The 3-point percentage every game is independent of other games. 
  \item The 3-point percentage every game should be similar to other games. 
  \end{itemize}
}

<<dawkins_data, results='hide'>>=
d = structure(list(date = structure(c(16L, 9L, 10L, 11L, 12L, 13L, 
14L, 15L, 20L, 17L, 18L, 19L, 21L, 7L, 8L, 1L, 2L, 3L, 4L, 5L, 
6L, 22L, 23L, 24L), .Label = c("1/11/14", "1/13/14", "1/18/14", 
"1/22/14", "1/25/14", "1/27/14", "1/4/14", "1/7/14", "11/12/13", 
"11/15/13", "11/18/13", "11/19/13", "11/24/13", "11/27/13", "11/29/13", 
"11/8/13", "12/16/13", "12/19/13", "12/28/13", "12/3/13", "12/31/13", 
"2/1/14", "2/4/14", "2/8/14"), class = "factor"), opponent = structure(c(5L, 
13L, 9L, 21L, 6L, 22L, 1L, 2L, 15L, 11L, 20L, 7L, 8L, 17L, 12L, 
4L, 23L, 16L, 14L, 10L, 18L, 19L, 24L, 3L), .Label = c("alabama", 
"arizona", "boston college", "clemson", "davidson", "east carolina", 
"eastern michigan", "elon", "florida atlantic", "florida state", 
"gardner-webb", "georgia tech", "kansas", "miami", "michigan", 
"nc state", "notre dame", "pitt", "syracuse", "ucla", "unc asheville", 
"vermont", "virginia", "wake forest"), class = "factor"), made = c(0L, 
0L, 5L, 3L, 0L, 3L, 0L, 1L, 2L, 4L, 1L, 6L, 5L, 1L, 1L, 0L, 1L, 
3L, 2L, 3L, 6L, 4L, 4L, 0L), attempts = c(0L, 0L, 8L, 6L, 1L, 
9L, 2L, 1L, 2L, 8L, 5L, 10L, 7L, 4L, 5L, 4L, 1L, 7L, 6L, 6L, 
7L, 9L, 7L, 1L)), .Names = c("date", "opponent", "made", "attempts"
), class = "data.frame", row.names = c(NA, -24L)) 

d = rbind(d, 
          
          # Add a total row
          data.frame(date     = NA, 
                     opponent = 'Total', 
                     made     = sum(d$made), 
                     attempts = sum(d$attempts))) %>%
  
  # Add posterior parameters
  mutate(a        = 0.5 + made,
         b        = 0.5 + attempts-made,
         lcl      = qbeta(0.025,a,b),
         ucl      = qbeta(0.975,a,b),
         Estimate = ifelse(opponent=="Total", "Combined", "Individual"),
         game     = 1:n())
@


\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
<<dawkins_plot, out.width='0.7\\textwidth', dependson="dawkins_data">>=
ggplot(d, 
       aes(x     = lcl, 
           xend  = ucl, 
           y     = game, 
           yend  = game, 
           color = Estimate))+
  geom_segment(lwd=1) +
  scale_color_brewer(palette='Set1') + 
  labs(x=expression(theta), title='95% Credible Intervals') + 
  scale_y_reverse() +
  theme_bw()
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
{\tiny
<<results='asis'>>=
xtable(d)
d = d %>% filter(opponent != "Total")
@
}
\end{frame}


\section{Hierarchical models}
\frame{\frametitle{Hierarchical models}
  Consider the following model
	\[ \begin{array}{ll}
	y_i &\stackrel{ind}{\sim} p(y|\theta_i) \pause \\
	\theta_i &\stackrel{ind}{\sim} p(\theta|\phi) \pause \\
	\phi &\sim p(\phi)
	\end{array} \]
  \pause where 
  \begin{itemize}
  \item $y_i$ is observed\pause, 
	\item $\theta=(\theta_1,\ldots,\theta_n)$ and $\phi$ are parameters\pause, and 
	\item only $\phi$ has a prior that is set.
	\end{itemize}
	
	\vspace{0.2in} \pause 
	
	This is a hierarchical or multilevel model.
}

\subsection{Posteriors}
\frame{\frametitle{Posterior distribution for hierarchical models}
	The joint posterior distribution of interest in hierarchical models is 
{\small
	\[ \begin{array}{rl}
  p(\theta,\phi|y) \pause \propto p(y|\theta,\phi)p(\theta,\phi) \pause = 
  p(y|\theta)p(\theta|\phi)p(\phi)\pause = 
  \Big[ \prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\phi)\Big] p(\phi). 
  \end{array} \]
}
	\pause The joint posterior distribution can be decomposed via 
  \[ p(\theta,\phi|y) = p(\theta|\phi,y)p(\phi|y) \]
  where 
{\small
  \[ \begin{array}{rl}
  p(\theta|\phi,y) &\propto p(y|\theta)p(\theta|\phi) \pause = \prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\phi) \pause \propto \prod_{i=1}^n p(\theta_i|\phi,y_i) \pause \\
  p(\phi|y) &\propto p(y|\phi)p(\phi) \pause \\
  p(y|\phi) &= 
  \int p(y|\theta)p(\theta|\phi) d\theta \pause \\
  &= \int \cdots \int  \prod_{i=1}^n  \left[ p(y_i|\theta_i)p(\theta_i|\phi) \right] d\theta_1 \cdots d\theta_n \pause \\
  &= \prod_{i=1}^n \int p(y_i|\theta_i)p(\theta_i|\phi) d\theta_i \pause \\
  &= \prod_{i=1}^n p(y_i|\phi)  
  \end{array} \]
}
}

\section{Binomial hierarchical model}
\frame{\frametitle{Three-pointer example}
	Our statistical model 
	\[ \begin{array}{ll}
	Y_i &\stackrel{ind}{\sim} Bin(n_i,\theta_i) \pause \\
	\theta_i&\stackrel{ind}{\sim} Be(\alpha,\beta) \pause \\
	\alpha,\beta &\sim p(\alpha,\beta) 
	\end{array} \]
	
	\vspace{0.2in} \pause
	
	In this example,
	\begin{itemize}
	\item $\phi=(\alpha,\beta)$\pause\,
	\item $Be(\alpha,\beta)$ describes the variability in 3-point percentage across games\pause, and
	\item we are going to learn about this variability. 
	\end{itemize}
}

\begin{frame}
\frametitle{Decomposed posterior}
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \quad \theta_i\stackrel{ind}{\sim} Be(\alpha,\beta) \quad \alpha,\beta \sim p(\alpha,\beta) \]
  Conditional posterior for $\theta$:
\[ p(\theta|\alpha,\beta,y) = \prod_{i=1}^n p(\theta_i|\alpha,\beta,y_i) \pause = \prod_{i=1}^n Be(\theta_i|\alpha+y_i,\beta+n_i-y_i) \]
\pause 
Marginal posterior for $(\alpha,\beta)$:
{\small 
\[ \begin{array}{rl}
p(\alpha,\beta|y) &\propto p(y|\alpha,\beta)p(\alpha,\beta) \pause \\
p(y|\alpha,\beta) &= \prod_{i=1}^n p(y_i|\alpha,\beta) \pause = \prod_{i=1}^n \int p(y_i|\theta_i)p(\theta_i|\alpha,\beta) d\theta_i \pause \\
&= \prod_{i=1}^n \int Bin(y_i|n_i,\theta_i)Be(\theta_i|\alpha,\beta) d\theta_i \pause \\
&= \prod_{i=1}^n \int_0^1 {n_i\choose y_i} \theta_i^{y_i} (1-\theta_i)^{n_i-y_i} \frac{\theta_i^{\alpha-1}(1-\theta_i)^{\beta-1}}{B(\alpha,\beta)} d\theta_i \pause \\
&= \prod_{i=1}^n {n_i\choose y_i}\frac{1}{B(\alpha,\beta)} \int_0^1 \theta_i^{\alpha+y_i-1} (1-\theta_i)^{\beta+n_i-y_i-1}  d\theta_i \pause \\
&= \prod_{i=1}^n {n_i\choose y_i}\frac{B(\alpha+y_i,\beta+n_i-y_i)}{B(\alpha,\beta)} 
\end{array} \]
}
\pause
Thus $y_i|\alpha,\beta \stackrel{ind}{\sim} \mbox{Beta-binomial}(n_i,\alpha,\beta)$. 
\end{frame}



\subsection{Prior}
\begin{frame}
\frametitle{A prior distribution for $\alpha$ and $\beta$}
  Recall the interpretation: \pause
  \begin{itemize}[<+->]
  \item $\alpha$: prior successes
  \item $\beta$: prior failures
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  A more natural parameterization is 
  \begin{itemize}[<+->]
  \item prior expectation: $\mu = \frac{\alpha}{\alpha+\beta}$
  \item prior sample size: $\eta = \alpha + \beta$
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  Place priors on these parameters or transformed to the real line:
  \begin{itemize}[<+->]
  \item $\text{logit } \mu = \log(\mu/[1-\mu]) = \log(\alpha/\beta)$
  \item $\log \eta$
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{A prior distribution for $\alpha$ and $\beta$}
  It seems reasonable to assume the mean ($\mu$) and size ($\eta$) are independent \emph{a priori}:
  \[ p(\mu,\eta) = p(\mu)p(\eta) \]

  
  \vspace{0.1in} \pause
  
  Let's assume an informative prior for $\mu$ and $\eta$ \pause perhaps
  \begin{itemize}
  \item $\mu \sim Be(\Sexpr{a},\Sexpr{b})$
  \item $\eta \sim LN(\Sexpr{m},\Sexpr{C}^2)$
  \end{itemize}
  \pause where $LN(\Sexpr{m},\Sexpr{C})$ is a log-normal distribution, i.e. $\log(\eta) \sim N(\Sexpr{m},\Sexpr{C}^2)$.
  
    \vspace{0.1in} \pause
  
<<informative_prior, echo=TRUE>>=
a = 20
b = 30
m = 0
C = 3
@
  
\end{frame}

\begin{frame}[fragile]
\frametitle{Prior draws}
<<informative_prior_plot, dependson='informative_prior', echo=TRUE>>=
n = 1e4

prior_draws = data.frame(mu  = rbeta(n, a, b), 
                         eta = rlnorm(n, m, C)) %>%
  mutate(alpha = eta*   mu,
         beta  = eta*(1-mu))

prior_draws %>%
  tidyr::gather(parameter, value) %>%
  group_by(parameter) %>%
  summarize(lower95 = quantile(value, prob = 0.025),
            median  = quantile(value, prob = 0.5),
            upper95 = quantile(value, prob = 0.975))

cor(prior_draws$alpha, prior_draws$beta)
@
\end{frame}



% \begin{frame}[fragile]
% <<proper_prior_plot, dependson='proper_prior', out.width='0.8\\textwidth'>>=
% ggpairs(prior_draws, lower=list(continuous='density'))
% @
% \end{frame}



\section{Stan}
\begin{frame}[fragile]
\frametitle{}
<<stan, echo=TRUE>>=
model_informative_prior = "
data {
  int<lower=0> N;    // data
  int<lower=0> n[N];
  int<lower=0> y[N]; 
  real<lower=0> a;   // prior
  real<lower=0> b;
  real<lower=0> C;
  real m;
}
parameters {
  real<lower=0,upper=1> mu;
  real<lower=0> eta;
  real<lower=0,upper=1> theta[N];
}
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;

  alpha = eta*   mu ;
  beta  = eta*(1-mu);
}
model {
  mu    ~ beta(a,b);
  eta   ~ lognormal(m,C);

  // implicit joint distributions
  theta ~ beta(alpha,beta); 
  y     ~ binomial(n,theta);
}
"
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan}
<<stan_run, dependson=c('informative_prior','stan',"dawkins_data"), echo=TRUE, results='hide'>>=
dat = list(y = d$made, n = d$attempts, N = nrow(d),a = a, b = b, m = m, C = C)
m = stan_model(model_code = model_informative_prior)
r = sampling(m, dat, c("mu","eta","alpha","beta","theta"), 
             iter = 10000,
             control = list(adapt_delta = 0.9))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{stan}
<<stan_post, dependson="stan_run", echo=TRUE>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{stan}
<<stan_plot, dependson="stan_run", message=TRUE, echo=TRUE, out.width='0.5\\textwidth'>>=
plot(r, pars=c('eta','alpha','beta'))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{stan}
<<stan_plot2, dependson="stan_run", echo=TRUE, out.width='0.7\\textwidth'>>=
plot(r, pars=c('mu','theta'))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Comparing independent and hierarchical models}
<<quantiles, dependson="stan_run", out.width='0.7\\textwidth'>>=
d$model = "independent"

tmp = data.frame(summary(r)$summary[,c(4,8)])
new_d = mutate(d,
               model = "hierarchical",
               lcl = tmp[5:28,1],
               ucl = tmp[5:28,2])

e = 0.2
ggplot(rbind(d, new_d), aes(x=lcl, 
                     xend=ucl, 
                     y=game+e*(model=="hierarchical"), 
                     yend=game+e*(model=="hierarchical"), 
                     color=model))+
  geom_segment(lwd=1, alpha=0.5) + 
  labs(x=expression(theta), y="game", title='95% Credible Intervals') +
  scale_color_brewer(palette='Set1') + 
  scale_y_reverse() +
  theme_bw()
@
\end{frame}




\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  In Bayesian Data Analysis (3rd ed) page 110, several priors are discussed 
  
  \vspace{0.2in} \pause
  
	\begin{itemize}
	\item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto 1$ leads to an improper posterior.
  
  \vspace{0.2in} \pause
  
  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \sim Unif([-10^{10},10^{10}] \times [-10^{10},10^{10}])$ \pause while proper and seemingly vague is a very informative prior.
  
  \vspace{0.2in} \pause
	
  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto \alpha\beta(\alpha+\beta)^{-5/2}$ \pause which leads to a proper posterior \pause and is equivalent to $p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}$.
	\end{itemize}
}



\subsection{default prior}
\begin{frame}[fragile]
\frametitle{Stan - default prior}
<<stan2_run, echo=TRUE, results='hide'>>=
model_default_prior = "
data {
  int<lower=0> N;
  int<lower=0> n[N];
  int<lower=0> y[N];
}
parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  real<lower=0,upper=1> theta[N];
}

model {
  // default prior
  target += -5*log(alpha+beta)/2;

  // implicit joint distributions
  theta ~ beta(alpha,beta); 
  y     ~ binomial(n,theta);
}
"

m2 = stan_model(model_code=model_default_prior)
r2 = sampling(m2, dat, c("alpha","beta","theta"), iter=10000,
              control = list(adapt_delta = 0.9))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan - default prior}
<<stan_post2, dependson=c("stan2_run","stan_run"), echo=TRUE, out.width='0.7\\textwidth'>>=
r2
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - default prior}
<<stan2_plot, dependson="stan2_run", echo=TRUE, out.width='0.7\\textwidth'>>=
plot(r2, pars=c('alpha','beta'))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - default prior}
<<stan2_plot2, dependson="stan2_run", out.width='0.7\\textwidth'>>=
plot(r2, pars=c('theta'))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Comparing all models}
<<quantiles2, dependson=c("stan_run","stan2_run"), out.width='0.7\\textwidth'>>=
d$prior     = "independent"
new_d$prior = "informative"

tmp = data.frame(summary(r2)$summary[,c(4,8)])
new_d2 = mutate(new_d,
                prior = "default",
                lcl = tmp[3:26,1],
                ucl = tmp[3:26,2])

bind_d = rbind(d,new_d,new_d2)
bind_d$prior = factor(bind_d$prior, c('informative','default','independent'))

e = 0.2
ggplot(bind_d, 
       aes(x     = lcl, 
           xend  = ucl, 
           y     = game+e*(prior=="independent")-e*(prior=="informative"), 
           yend  = game+e*(prior=="independent")-e*(prior=="informative"), 
           color = prior))+
  geom_segment(lwd=1, alpha=0.5) + 
  labs(x=expression(theta), y="game", title='95% Credible Intervals') + 
  scale_color_brewer(palette='Set1') + 
  scale_y_reverse() +
  theme_bw()
@
\end{frame}




\subsection{beta-binomial}
\begin{frame}
\frametitle{Marginal posterior for $\alpha,\beta$}

An alternative to jointly sampling $\theta,\alpha,\beta$ is to 
\begin{enumerate}
\item sample $\alpha,\beta\sim p(\alpha,\beta|y)$\pause, and then
\item sample $\theta_i \stackrel{ind}{\sim} p(\theta_i|\alpha,\beta,y_i) \stackrel{d}{=} Be(\alpha+y_i,\beta+n_i-y_i)$. 
\end{enumerate}

\vspace{0.2in} \pause

The maginal posterior for $\alpha,\beta$ is 
\[ p(\alpha,\beta|y) \propto p(y|\alpha,\beta)p(\alpha,\beta) = \left[ \prod_{i=1}^n \mbox{Beta-binomial}(y_i|n_i,\alpha,\beta) \right] p(\alpha,\beta) \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan3_run, echo=TRUE, results='hide',dependson="stan_run">>=
# Marginalized (integrated) theta out of the model
model_marginalized = "
data {
  int<lower=0> N;
  int<lower=0> n[N];
  int<lower=0> y[N];
}
parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
}
model {
  target += -5*log(alpha+beta)/2;
  y     ~ beta_binomial(n,alpha,beta);
}
"

m3 = stan_model(model_code=model_marginalized)
r3 = sampling(m3, dat, c("alpha","beta"))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan3_post, dependson="stan3_run">>=
r3
@
\end{frame}


\begin{frame}[fragile]
\frametitle{}

Posterior samples for $\alpha$ and $\beta$ 

<<stan3_alpha_beta, dependson="stan3_run", echo=TRUE, out.width='0.7\\textwidth'>>=
samples = extract(r3, c("alpha","beta"))
ggpairs(data.frame(log_alpha = log(as.numeric(samples$alpha)), log_beta  = log(as.numeric(samples$beta))),
        lower  = list(continuous='density'))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Posterior sample for $\theta_{22}$}
<<stan3_theta, dependson="stan3_run", fig.width=10, echo=TRUE>>=
samples = extract(r3, c("alpha","beta"))
game = 22
theta22 = rbeta(length(samples$alpha), 
              samples$alpha + d$made[game], 
              samples$beta  + d$attempts[game] - d$made[game])
hist(theta22, 100, 
     main=paste("Posterior for game against", d$opponent[game], "on", d$date[game]),
     xlab="3-point probability", 
     ylab="Posterior")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{$\theta$s are not independent in the posterior}
<<stan3_thetas, dependson='stan3_run'>>=
game = 23
theta23 = rbeta(length(samples$alpha), 
              samples$alpha + d$made[game], 
              samples$beta  + d$attempts[game] - d$made[game])
ggpairs(data.frame(theta22=theta22,theta23=theta23), lower=list(continuous='density'))
@
\end{frame}



\subsection{3-point percentage across seasons}
\begin{frame}[fragile]
\frametitle{3-point percentage across seasons}

An alternative to modeling game-specific 3-point percentage is to model 3-point percentage in a season. \pause The model is exactly the same, but the data changes. \pause

<<season_data, results='asis'>>=
d = data.frame(season=1:4, y=c(36,64,67,64), n=c(95,150,171,152))
xtable(d, digits=0)
@

Due to the low number of seasons (observations), we will use an informative prior for $\alpha$ and $\beta$. 

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan_season, dependson='season_data', echo=TRUE, results='hide'>>=
model_seasons = "
data {
  int<lower=0> N; int<lower=0> n[N]; int<lower=0> y[N];
  real<lower=0> a; real<lower=0> b; real<lower=0> C; real m;
}
parameters {
  real<lower=0,upper=1> mu;
  real<lower=0> eta;
}  
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  alpha = eta *    mu;
  beta  = eta * (1-mu);
}
model {
  mu  ~ beta(a,b);
  eta ~ lognormal(m,C);
  y   ~ beta_binomial(n,alpha,beta);
}
generated quantities {
  real<lower=0,upper=1> theta[N];
  for (i in 1:N) theta[i] = beta_rng(alpha+y[i], beta+n[i]-y[i]);
}
"

dat  = list(N = nrow(d), y = d$y, n = d$n, a = 20, b = 30, m = 0, C = 2)
m4 = stan_model(model_code = model_seasons)
r_seasons = sampling(m4, 
                     dat, 
                     c("alpha","beta","mu","eta","theta"),
                     iter = 1000)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

<<stan_season_summary, dependson="stan_season">>=
r_seasons
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

<<stan_season_posteriors, dependson="stan_season">>=
posterior = extract(r_seasons, "theta") %>%
  as.data.frame() %>%
  tidyr::gather(parameter, value) %>%
  mutate(parameter = as.factor(parameter))

ggplot(posterior, 
       aes(x = value, 
           fill = parameter)) + 
  geom_density(alpha = 0.5) +
  theme_bw()
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

Probabilities that 3-point percentage is greater in season 4 than in the other seasons:

<<stan_season_comparison, echo=TRUE, dependson="stan_season">>=
theta = extract(r_seasons, "theta")[[1]] 
mean(theta[,4] > theta[,1])
mean(theta[,4] > theta[,2])
mean(theta[,4] > theta[,3])
@

\end{frame}



\subsection{Summary}
\begin{frame}
\frametitle{Summary - hierarchical models}
Two-level hierarchical model:
\[ y_i\ind p(y|\theta) \qquad \theta_i \ind p(\theta|\phi) \qquad \phi\sim p(\phi) \phantom{|\psi\qquad \psi\sim p(\psi)} \]

\vspace{0.2in} \pause

Conditional independencies:
\begin{itemize}
\item $y_i \independent y_j | \theta$ for $i\ne j$
\item $\theta_i \independent \theta_j | \phi$ for $i\ne j$
\item $y \independent \phi | \theta$
\item $y_i \independent y_j | \phi$ for $i\ne j$
\item $\theta_i \independent \theta_j | \phi, y$ for $i\ne j$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Summary - extension to more levels}
Three-level hierarchical model:
	\[ y\sim p(y|\theta) \qquad \theta\sim p(\theta|\phi) \qquad \phi\sim p(\phi|\psi) \qquad \psi\sim p(\psi) \]

\vspace{0.2in} \pause
  
  When deriving posteriors, remember the conditional independence structure, \pause e.g.
	\[ p(\theta,\phi,\psi|y) \propto p(y|\theta) p(\theta|\phi) p(\phi|\psi) p(\psi) \]
\end{frame}



\end{document}
