\documentclass[handout]{beamer}

\usepackage{verbatim,multicol}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Modeling assumptions}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center')
library(plyr)
library(ggplot2)
library(xtable)
@


\begin{document}

\begin{comment}
\begin{frame}
\maketitle
\end{frame}

\section{Normality assumptions}
\begin{frame}[fragile]
\frametitle{Normality assumptions}

In the paired t-test, we assume
\[ D_i \stackrel{iid}{\sim} N(\mu,\sigma^2). \]
In the two-sample t-test, we assume
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2). \]

\pause

<<visualize, echo=FALSE, out.width='\\textwidth'>>=
par(mfrow=c(2,2))
curve(dnorm(x,1), -4, 4, main="Paired t-test", xlab="Difference", ylab="Distribution", 
      lwd=2, axes=F, frame=T)
axis(1,0)
abline(v=0, col="gray", lty=2)
points(.8,0, pch=19)
segments(.8-1.3,0,.8+1.3,0, lwd=2)

curve(dnorm(x,-1), -4, 4, main="Two-sample t-test", xlab="", ylab="", lwd=2, axes=F, frame=T, col=2, lty=1)
curve(dnorm(x,1), add=TRUE, col=3, lwd=2, lty=2)
legend("bottomright", paste("Pop",1:2),col=2:3, lwd=2, lty=1:2)
@
\end{frame}



\begin{frame}
\frametitle{Normality assumptions}

In the paired t-test, we assume
\[ D_i \stackrel{iid}{\sim} N(\mu,\sigma^2). \]
In the two-sample t-test, we assume
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2). \]

\vspace{0.2in}

Key features of the normal distribution assumption:
\begin{itemize}[<+->]
\item Centered at the mean (expectation) $\mu$
\item Standard deviation describes the spread
\item Symmetric around $\mu$ (no skewness)
\item Non-heavy tails, i.e. outliers are rare (no kurtosis)
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Normality assumptions}
<<normal_distribution, echo=FALSE>>=
curve(dnorm, -3.1, 3.1, main='Probability density function', xlab='y', ylab='Probability density function, f(y)', lwd=2, axes=F, frame=TRUE)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3,expression(mu-3*sigma, mu-2*sigma, mu-sigma, mu,mu+sigma,mu+2*sigma,mu+3*sigma))
arrows(-1:-3, dnorm(1:3), 1:3, dnorm(1:3), code=3)
text(0, dnorm(1:3), round(1-2*pnorm(-1:-3),3), pos=3)
@

\end{frame}


\subsection{Kurtosis (heavy-tailedness)}
\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<kurtosis, echo=FALSE>>=
v = c(Inf,30,15,5)
sigma = v/(v-2); sigma[1] = 1
mykurtosis = paste("Kurtosis=", round(6/(v-4),2))
curve(dnorm, -3, 3, main="t distribution", ylab='Probability density function, f(y)', xlab='y', lwd=2, axes=F, frame=TRUE)
for (i in 2:4) curve(dt(x/sigma[i],v[i])/sigma[i], add=TRUE, col=i, lty=i, lwd=2)
abline(v=-3:3, lty=2, col='gray')
legend("topright", mykurtosis, lwd=2, lty=1:4, col=1:4)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<outliers, echo=FALSE>>=
v = 5
sigma = sqrt(v/(v-2))

curve(dnorm, -4, 4, main='Probability density function', xlab='y', ylab='Probability density function, f(y)', lwd=2, axes=F, frame=TRUE, col='gray')
curve(dt(x/sigma,v)/sigma, lwd=2, add=TRUE)
curve(dnorm, col="gray", add=TRUE)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3,expression(mu-3*sigma, mu-2*sigma, mu-sigma, mu,mu+sigma,mu+2*sigma,mu+3*sigma))
arrows(-1:-3, dt(1:3/sigma,v)/sigma, 1:3, dt(1:3/sigma,v)/sigma, code=3)
text(0, dt(3:1/sigma,v)/sigma, round(1-2*pt(-3:-1,v),3), pos=3)
legend("topright", c("Normal", "Scaled t_5"), lwd=2, col=c("gray","black"))
@
\end{frame}






\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<kurtosis_samples, echo=FALSE>>=
v = c(Inf,30,15,5)
sigma = v/(v-2); sigma[1] = 1
d = data.frame(v=v, sigma=sigma, kurtosis=mykurtosis)
samps = ddply(d, .(v,kurtosis), function(x) data.frame(samples = x$sigma*rt(100,x$v))) 
qplot(samples, data=samps, facets=~kurtosis, binwidth=8/30)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<kurtosis_samples_boxplot, echo=FALSE>>=
v = c(Inf,30,15,5)
sigma = v/(v-2); sigma[1] = 1
d = data.frame(v=v, sigma=sigma, kurtosis=mykurtosis)
samps = ddply(d, .(v,kurtosis), function(x) data.frame(samples = x$sigma*rt(100,x$v))) 
ggplot(samps, aes(factor(kurtosis), samples))+geom_boxplot()
@
\end{frame}




\subsection{Skewness}
\begin{frame}[fragile]
\frametitle{Skewness}
<<skewness, echo=FALSE>>=
skewness = function(mu,sigma) (exp(sigma^2)+2)*sqrt(exp(sigma^2)-1)
mydlnorm = function(x,sigma) dlnorm(x, -sigma^2/2, sigma) 
sigma = c(.5,1,1.5)
myskewness = paste("Skewness=", round(skewness(-sigma^2/2,sigma),2))
curve(mydlnorm(x,sigma[1]), 0, 4, main="Log-normal distribution", ylab='Probability density function, f(y)', xlab='y', lwd=2, axes=F, frame=TRUE, ylim=c(0,1.5))
curve(mydlnorm(x,sigma[2]), add=TRUE, col=2, lty=2, lwd=2)
curve(mydlnorm(x,sigma[3]), add=TRUE, col=3, lty=3, lwd=2)
legend("topright", myskewness, lwd=2, lty=1:3, col=1:3)
abline(v=1, lty=4, col='gray')
text(1,1,"Mean", pos=4, col="gray")
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Samples from skewed distributions}

<<skewness_samples, echo=FALSE>>=
d = data.frame(sigma=sigma, skewness=myskewness)
samps = ddply(d, .(sigma,skewness), function(x) data.frame(samples = rlnorm(100,-x$sigma^2/2, x$sigma))) 
qplot(samples, data=samps, facets=~sigma, binwidth=15/30)
@

\end{frame}




\subsection{Robustness}
\begin{frame}
\frametitle{Robustness}

\begin{definition}
A statistical procedure is \alert{robust to departures from a particular assumption} if it is valid even when the assumption is not met.
\end{definition}

\vspace{0.2in} \pause 

\begin{remark}
If a 95\% confidence interval is robust to departures from a particular assumption, the confidence interval should cover the true value about 95\% of the time.
\end{remark}
\end{frame}


\begin{frame}[fragile]
\frametitle{Robustness to skewness and kurtosis}

Percentage of 95\% confidence intervals that cover the true difference in means in an equal-sample two-sample t-test with non-normal populations (where the distributions are the same other than their means).

{\tiny
<<"robustness data", echo=FALSE, results='asis'>>=
options(width=170)
d = 
data.frame("sample size" = c(5,10,25,50,100),
           "strongly skewed" = c(95.5,95.5,95.3,95.1,94.8),
           "moderately skewed" = c(95.4,95.4,95.3,95.3,95.3),
           "mildly skewed" = c(95.2,95.2,95.1,95.1,95.0),
           "heavy-tailed" = c(98.3, 98.3, 98.2, 98.1, 98.0),
           "short-tailed" = c(94.5, 94.6, 94.9, 95.2, 95.6), 
           check.names=FALSE)
print(xtable(d, digits=c(NA,0,1,1,1,1,1), align='c|r|ccccc|'),
      include.row=FALSE)
@
}

\end{frame}


\begin{frame}[fragile]
\frametitle{Differences in variances}

<<different_variances, echo=FALSE>>=
par(mfrow=c(1,1))
sigma = c(1,2,4)
curve(dnorm, -5, 5, main="Normal distribution", xlab="", ylab="",
      axes=F, frame=T, type="n")
for (i in 1:3) curve(dnorm(x,0,sigma[i]), add=TRUE, lwd=2, col=i, lty=i)
legend("topright", paste("SD=",sigma), lwd=2, col=1:3, lty=1:3)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Differences in variances}

<<different_variances_samples, echo=FALSE>>=
d = adply(sigma,1,function(x) data.frame(sigma=x,y=rnorm(100,0,x)))
ggplot(d, aes(factor(sigma), y))+geom_boxplot()
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Robustness to differences in variances}

Percentage of 95\% confidence intervals that cover the true difference in means in an equal-sample two-sample t-test ($r=\sigma_1/\sigma_2$). 

{\small
<<"robustness data2", echo=FALSE, results='asis'>>=
options(width=170)
d = 
data.frame(n1=c(10,10,10,100,100,100), 
           n2=c(10,20,40,100,200,400), 
           "r=1/4" = c(95.2,83,71,94.8,86.5,71.6),
           "r=1/2" = c(94.2,89.3,82.6,96.2,88.3,81.5),
           "r=1"   = c(94.7,94.4,95.2,95.4,94.8,95.0),
           "r=2"   = c(95.2,98.7,99.5,95.3,98.8,99.5),
           "r=4"   = c(94.5,99.1,99.9,95.1,99.4,99.9),
           check.names=FALSE)
print(xtable(d, digits=c(NA,0,0,1,1,1,1,1), align='c|rr|ccccc|'),
      include.row=FALSE)
@
}

\end{frame}




\begin{frame}
\frametitle{Outliers}
\begin{definition}
A statistical procedure is \alert{resistant} if it does not change very much when a small part of the data changes, perhaps drastically.
\end{definition}

\vspace{0.2in} \pause 

Identify outliers:
\begin{enumerate}[<+->]
\item If recording errors, fix.
\item If outlier comes from a different population, remove and report.
\item If results are the same with and without outliers, report with outliers.
\item If results are different, use resistant analysis or report both analyses. 
\end{enumerate}


\end{frame}




\section{Independence}

\begin{frame}
\frametitle{Common ways for independence to be violated}



\begin{itemize}[<+->]
\item Cluster effect
  \begin{itemize}
  \item e.g. pigs in a pen
  \end{itemize}
\item Correlation effect
  \begin{itemize}
  \item e.g. measurements in time with drifting scale
  \end{itemize}
\item Spatial effect
  \begin{itemize}
  \item e.g. corn yield plots (drainage)
  \end{itemize}
\end{itemize}

\end{frame}

\end{comment}



\section{Transformations of the data}
\begin{frame}
\frametitle{Common transformations for data}

{\tiny From: \url{http://en.wikipedia.org/wiki/Data_transformation_\%28statistics\%29}}

\begin{definition}
In statistics, \alert{data transformation} refers to the application of a deterministic mathematical function to each point in a data set \pause â€” that is, each data point $z_i$ is replaced with the transformed value $y_i = f(z_i)$, where $f$ is a function. 
\end{definition}

\vspace{0.2in} \pause

The most common transformations to 

\begin{itemize}[<+->]
\item If $z\in (0,1)$, then $f(z) = sin^{-1}(\sqrt{z})$.
\item If $z$ is a count, then $f(z) = sqrt{z}$. 
\item If $z$ is positive and right-skewed, then $f(z) = log(z)$, the \emph{natural logarithm} of $z$. 
\end{itemize}

\pause

{\scriptsize
\begin{remark}
Since $log(0)=-\infty$, the logarithm cannot be used directly when some $z_i$ are zero. In these cases, use $\log(z+c)$ where $c$ is something small relative to your data, e.g. the minimum non-zero value divided by 2. 
\end{remark}
}

\end{frame}



\subsection{Log transformation}
\begin{frame}
\frametitle{Log transformation}

If $y=log(z)$, then $z=e^y$ and so if 
\[ Y_{ij} \sim N(\mu_j, \sigma^2) \mbox{ then } Z_{ij}=e^{Y_{ij}} \sim LN(\mu_j,\sigma^2). \]
Let $\delta = \mu_1-\mu_2 = E[Y_{i1}] - E[(Y_{i2}]$ 


\end{frame}


\subsection{Two-sample t-test using logarithms}
\frame[containsverbatim]{\frametitle{SAS code for t-test using logartihms}
{\small
\begin{verbatim}
DATA mpg;
  INFILE 'mpg.csv' DELIMITER=',' firstobs=2;
	INPUT mpg country $;
	lmpg = log(mpg);

PROC TTEST DATA=mpg;
	CLASS country;
	VAR lmpg;
	RUN;
\end{verbatim}
}
}

\frame[containsverbatim]{\frametitle{SAS code for t-test on logarithms}
{\tiny
\begin{verbatim}
                                       The TTEST Procedure

                                         Variable:  lmpg

         country          N        Mean     Std Dev     Std Err     Minimum     Maximum

         Japan           79      3.3963      0.2088      0.0235      2.8904      3.8501
         US             249      2.9552      0.3073      0.0195      2.1972      3.6636
         Diff (1-2)              0.4411      0.2868      0.0370

  country       Method               Mean       95% CL Mean        Std Dev      95% CL Std Dev

  Japan                            3.3963      3.3495   3.4430      0.2088      0.1805   0.2476
  US                               2.9552      2.9168   2.9935      0.3073      0.2824   0.3369
  Diff (1-2)    Pooled             0.4411      0.3682   0.5139      0.2868      0.2664   0.3106
  Diff (1-2)    Satterthwaite      0.4411      0.3809   0.5013

                   Method           Variances        DF    t Value    Pr > |t|

                   Pooled           Equal           326      11.91      <.0001
                   Satterthwaite    Unequal      193.33      14.46      <.0001

                                      Equality of Variances

                        Method      Num DF    Den DF    F Value    Pr > F

                        Folded F       248        78       2.17    0.0001
\end{verbatim}
}
}



\frame{\frametitle{Conclusion}
	Take the 

	\pause Japanese median miles per gallon is 1.55 [95\% CI (1.46,1.65)] times as large as the US median miles per gallon.
	
	\vspace{0.4in} \pause
	
	\[ e^{0.4411} = 1.554416 \quad e^{0.3809}=1.463601 \quad e^{0.5013} = 1.650866 \]
}



\end{document}
