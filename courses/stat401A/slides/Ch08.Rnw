\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Regression diagnostics}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center', message=FALSE)
library(plyr)
library(ggplot2)
library(xtable)
library(Sleuth3)
library(abd)
@


\begin{document}



\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{All models are wrong!}

George Box (Empirical Model-Building and Response Surfaces, 1987):
\begin{quote}
All models are wrong, but some are useful. 
\end{quote}

\vspace{0.2in} \pause

{\tiny \url{http://stats.stackexchange.com/questions/57407/what-is-the-meaning-of-all-models-are-wrong-but-some-are-useful}}

{\small
\begin{quotation}
``All models are wrong" that is, every model is wrong because it is a simplification of reality. Some models, especially in the "hard" sciences, are only a little wrong. They ignore things like friction or the gravitational effect of tiny bodies. Other models are a lot wrong - they ignore bigger things. \pause \\

``But some are useful" - simplifications of reality can be quite useful. They can help us explain, predict and understand the universe and all its various components. \pause \\

This isn't just true in statistics! Maps are a type of model; they are wrong. But good maps are very useful. 
\end{quotation}
}


\end{frame}


\section{Regression diagnostics}
\frame{\frametitle{Regression}
  The simpler linear regression model is 
  \[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
	\pause this can be rewritten as 
	\[ Y_i = \beta_0 + \beta_1 X_i + e_i \quad e_i \stackrel{ind}{\sim} N(0,\sigma^2) \]
	\pause where we estimate the errors via the residuals
	\[ r_i = \hat{e}_i = Y_i - (\hat{\beta}_0+\hat{\beta}_1 X_i). \]
	
	\vspace{0.2in} \pause
	
	Key assumptions are:
	\begin{itemize}[<+->]
  \item Normality of the errors
	\item Constant variance of the errors
	\item Independence of the errors
  \item Linearity between mean response and explanatory variable
	\end{itemize}
}



\subsection{Normality}
\begin{frame}[fragile]
\frametitle{Histograms with best fitting bell curves}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
x = rnorm(100)
hist(x, freq=F, main="Normal data", ylim=c(0,dnorm(0)))
curve(dnorm, add=TRUE, lwd=2)
@

\end{frame}









\begin{frame}
\frametitle{Normal QQ-plot}

\begin{definition}
The quantile-quantile or qq-plot is an exploratory graphical device used to check the validity of a distributional assumption for a data set.
\end{definition}

\vspace{0.2in} \pause

A normal qq-plot graphs the theoretical quantiles from a normal distribution versus the observed quantiles. \pause With a line that indicates perfect normality.

\vspace{0.2in} \pause

\begin{remark}
The bottom line is that, if the distribution assumption is satisfied, the points should fall roughly along the line. \pause Systematic variation from this line indicates skewness, 
\end{remark}

\end{frame}



\begin{frame}[fragile]
\frametitle{Normal}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
n = 1000
x = rnorm(n)
par(mfrow=c(1,3))
for (i in c(10,100,1000)) { qqnorm(x[1:i], main=paste("n=",i)); qqline(x[1:i]) }
@
	
\end{frame}





\begin{frame}[fragile]
\frametitle{Normal (n=10)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
par(mfrow=c(2,5), mar=rep(0,4)+.5)
for (i in 1:10) {
  qqnorm(rnorm(10), main='', xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(x)
}
@
	
\end{frame}




\begin{frame}[fragile]
\frametitle{Normal(n=100)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
par(mfrow=c(2,5), mar=rep(0,4)+.5)
for (i in 1:10) {
  qqnorm(rnorm(100), main='', xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(x)
}
@
  
\end{frame}



\begin{frame}[fragile]
\frametitle{Normal (n=1000)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
par(mfrow=c(2,5), mar=rep(0,4)+.5)
for (i in 1:10) {
  qqnorm(rnorm(1000), main='', xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(x)
}
@
  
\end{frame}




\begin{frame}
\frametitle{Not normal (n=10)}
<<echo=FALSE>>=
set.seed(20141018)
n=10
x = rnorm(n)
par(mfrow=c(2,2), mar=c(0,0,4,0)+.5)
qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
@
	
\end{frame}



\begin{frame}
\frametitle{Not normal (n=100)}
<<echo=FALSE>>=
n=100
x = rnorm(n)
par(mfrow=c(2,2), mar=c(0,0,4,0)+.5)
qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
@
  
\end{frame}



\begin{frame}
\frametitle{Not normal (n=1000)}
<<echo=FALSE>>=
n=1000
x = rnorm(n)
par(mfrow=c(2,2), mar=c(0,0,4,0)+.5)
qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
@
  
\end{frame}






\subsection{Constant variance}
\begin{frame}
\frametitle{Constant variance}

Recall the model
\[ Y_i = \beta_0+\beta_1 X_i + e_i \quad e_i \stackrel{iid}{\sim} N(0,\sigma^2) \]
so the variance for the $e_i$ is constant. 

\vspace{0.2in} \pause

To assess this assumption, we look at plots of residuals vs anything and look for patterns that show different ``spreads''\pause, e.g.
\begin{itemize}
\item funnels
\item football shapes
\end{itemize}

\vspace{0.2in} \pause

The most common way this assumption is violated is by having increasing variance with increasing mean\pause, thus we often look at a residuals vs predicted (fitted) mean plot.

\end{frame}



\begin{frame}[fragile]
\frametitle{Constant variance}

<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
for (n in c(10,100,1000)) { 
  plot(runif(n), rnorm(n), axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Constant variance}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
plot(runif(n), rnorm(n), axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
xx = (1:4)/5
yy = 2.5
arrows(xx, -yy, xx, yy, col="red", lwd=2, code = 3)
abline(h=0, col="blue")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Extreme non-constant variance (funnel)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
ub = 1
x = runif(n,0, ub)
e = rnorm(n,0,x)
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Extreme non-constant variance (funnel)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
xx = ub*(1:4)/5
yy = ub*(.5+(1:4))/2
arrows(xx, -yy, xx, yy, col="red", lwd=2, code = 3)
abline(h=0, col="blue")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=10, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
n = 10
lb = 1
ub = 4
for (i in 1:3) {
  x = runif(n, lb, ub)
  e = rnorm(n,0,x)
  plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=100, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
n = 100
for (i in 1:3) {
  x = runif(n, lb, ub)
  e = rnorm(n,0,x)
  plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=1000, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
n = 1000
for (i in 1:3) {
  x = runif(n, lb, ub)
  e = rnorm(n,0,x)
  plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Extreme non-constant variance (football)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
ub = 1
x = runif(n, -1, 1)
e = rnorm(n,0, 1-abs(x))
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
@

\end{frame}





\subsection{Independence}
\frame{\frametitle{Independence}
	Lack of independence includes
	\begin{itemize}
	\item Cluster effect
	\item Serial correlation
	\item Spatial association
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Make plots of residuals vs relevant explanatory variables and look for patterns, \pause e.g.
	\begin{itemize}[<+->]
	\item Residuals vs groups 
	\item Residuals vs time (or observation number)
	\item Residuals vs spatial variable 
	\end{itemize}
}



\subsection{Summary}
\frame{\frametitle{Summary}
	Often the best strategy is graphical exploration of the data, \pause here are some relevant graphs: \pause
	\begin{itemize}
	\item transformed response vs transformed explanatory
	\item transformed response vs transformed explanatory
	\item qqplot of residuals
	\item residual vs fitted value
	\item residual vs explanatory
	\item residual vs observation number
	\item residual vs any other variable
	\end{itemize}
}





\subsection{Linearity}
\begin{frame}[fragile]
\frametitle{Linearity}
  Assess using scatterplots of (transformed) response vs (transformed) explanatory variable:
	
<<echo=FALSE>>=
n = 100
set.seed(1) 
x = runif(n,0,10)
e = rnorm(n)
y = x+e

myplot = function(x,y,main="") {
  plot(x,y,axes=FALSE, frame=TRUE, main=main)
}

par(mfrow=c(2,4), mar=c(0,0,4,0)+.5)
myplot(x, y)
myplot(x, exp(y))
myplot(exp(x), y)
myplot(exp(x), exp(y))
myplot(x, y)
myplot(x, y, "log(y)")
myplot(x, y, "log(x)")
myplot(x, y, "log(x), log(y)")
@
\end{frame}



\subsection{Lack-of-fit F-test}
\frame{\frametitle{Testing Composite hypotheses}
  Comparing two models
	\begin{itemize}
	\item $H_0:$ \alert{(reduced)}
	\item $H_1:$ \alert{(full)}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	\small
	Do the following
	\begin{enumerate}[1.]
	\item Calculate extra sum of squares.
	\item Calculate extra degrees of freedom
	\item Calculate 
	\[ \mbox{F-statistic} = \frac{\mbox{Extra sum of squares / Extra degrees of freedom}}{\hat{\sigma}^2_{full}} \]
	\item Compare this to an F-distribution with 
		\begin{itemize}
		\item numerator degrees of freedom = extra degrees of freedom
		\item denominator degrees of freedom = degrees of freedom in estimating $\hat{\sigma}_{full}^2$
		\end{itemize}
	\end{enumerate}
}


\frame{\frametitle{Lack-of-fit F-test}
  Let $Y_{ij}$ be the $i^{th}$ observation from the $j^{th}$ group where the group is defined by those observations having the same explanatory variable value ($X_j$). 
  
  \vspace{0.1in} \pause

	Two models:
	
	\begin{tabular}{lll}
	ANOVA: & $Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2)$ & \pause \uncover<3->{\alert{(full)}} \\
	Regression: & $Y_{ij} \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_j, \sigma^2)$ & \uncover<3->{\alert{(reduced)}}
	\end{tabular}
	
	\vspace{0.2in} \pause \pause
	
	\begin{itemize}[<+->]
	\item Regression model is reduced:
	\begin{itemize}
	\item ANOVA has $J$ parameters for the mean
	\item Regression has 2 parameters for the mean
	\item Set $\mu_{j} = \beta_0+\beta_1 X_j$.
	\end{itemize}
	\item Small pvalues indicate a lack-of-fit, i.e. the reduced model is not adequate.
	\item Lack-of-fit F-test requires multiple observations at a few $X_j$ values!
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Telomere length}
<<echo=FALSE>>=
ggplot(Telomeres, aes(factor(years), telomere.length))+
  geom_boxplot()+
  geom_jitter()+
  labs(x="Years", y="Telomere length", 
       title="Telomere length vs years since diagnosis")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Telomere length}
<<echo=FALSE>>=
ggplot(Telomeres, aes(years, telomere.length))+
  geom_jitter()+
  geom_smooth(method=lm, se=FALSE)+
  labs(x="Years", y="Telomere length", 
       title="Telomere length vs years since diagnosis")
@
\end{frame}

\frame[containsverbatim]{\frametitle{SAS code}
\tiny
\begin{verbatim}
DATA t;
  INFILE 'telomeres.csv' DSD FIRSTOBS=2;
  INPUT years length;

PROC REG DATA=t;
  MODEL length = years / CLB LACKFIT;
  RUN;

                                        The REG Procedure
                                          Model: MODEL1
                                   Dependent Variable: length 

                             Number of Observations Read          39
                             Number of Observations Used          39


                                      Analysis of Variance
 
                                             Sum of           Mean
         Source                   DF        Squares         Square    F Value    Pr > F

         Model                     1        0.22777        0.22777       8.42    0.0062
         Error                    37        1.00033        0.02704                     
           Lack of Fit             9        0.18223        0.02025       0.69    0.7093
           Pure Error             28        0.81810        0.02922                     
         Corrected Total          38        1.22810           
\end{verbatim}

\pause

Indicates no evidence for a lack of fit, i.e. regression seems adequate.
}



\begin{frame}[fragile]
<<>>=
# Use as.factor to turn a continuous variable into a categorical variable
m_anova = lm(telomere.length ~ as.factor(years), Telomeres) 
m_reg   = lm(telomere.length ~           years , Telomeres)
anova(m_reg, m_anova)
@

\vspace{0.2in} \pause

No evidence of a lack of fit. 

\end{frame}


\frame{\frametitle{Lack-of-fit F-test summary}
	\begin{itemize}[<+->]
	\item Lack-of-fit F-test tests the assumption of linearity
	\item Needs multiple observations at various explanatory variable values
	\item Small pvalue indicates a lack-of-fit, i.e. means are not linear
		\begin{itemize}
		\item Transform response, e.g. log
		\item Transform explanatory variable
		\item Add other explanatory variables
		\end{itemize}
	\end{itemize}
}



\subsection{Interpretations using logs}
\begin{frame}
\frametitle{Interpretations using logs}

The most common transformation of either the response or explanatory is to take logarithms \pause because
\begin{itemize}[<+->]
\item linearity will often then be approximately true,
\item the variance will likely be approximately constant, and
\item there is a (relatively) convenient interpretation.
\end{itemize}

\vspace{0.2in} \pause

We will talk about interpretation of $\beta_0$ and $\beta_1$ when
\begin{itemize}[<+->]
\item only the response is logged, 
\item only the explanatory variable is logged, and
\item when both are logged.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Neither response nor explanatory variable are logged}

If 
\[ E[Y|X] = \beta_0 + \beta_1 X \]
\pause, then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected response when $X$ is zero and
\item $\beta_1$ is the expected change in the response for a one unit increase in the explanatory variable.
\end{itemize}

\vspace{0.2in} \pause

For the following discussion,
\begin{itemize}[<+->]
\item $Y$ is always going to be the original response and
\item $X$ is always going to be the original explanatory variable.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Example}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[Y|X] = \beta_0 + \beta_1 X \]

\begin{itemize}[<+->]
\item $\beta_0$ is the expected corn yield per acre when fertilizer level is zero and 
\item $\beta_1$ is the expected change in corn yield per acre when fertilizer is increase by 1 lbs/acre. 
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Response is logged}

If 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 X \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 X} \]
\pause, then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected $\log(Y)$ when $X$ is zero and
\item $\beta_1$ is the expected change in $\log(Y)$ for a one unit increase in the explanatory variable. 
\end{itemize}

\pause 
Alternatively,

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median of $Y$ when $X$ is zero and 
\item $e^{\beta_1}$ is the multiplicative effect on the median of $Y$ for a one unit increase in the explanatory variable.
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Response is logged}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 X \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 X} \]

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median corn yield per acre when fertilizer level is zero and 
\item $e^{\beta_1}$ is the multiplicative effect in median corn yield per acre when fertilizer is increase by 1 lbs/acre. 
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Explanatory variable is logged}

If 
\[ E[Y|X] = \beta_0 + \beta_1 \log(X)  \]
\pause, then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected response when $\log(X)$ is zero and
\item $\beta_1$ is the expected change in the response for a one unit increase in $\log(X)$. 
\end{itemize}

\pause 
Alternatively,

\begin{itemize}[<+->]
\item $\beta_0$ is the expected response when $X$ is 1 and 
\item $\beta_1 \log(d)$ is the expected change in the response when $X$ increase multiplicatively by $d$, e.g.
  \begin{itemize}
  \item $\beta_1 \log(2)$ is the expected change in the response for each doubling of $X$ or
  \item $\beta_1 \log(10)$ is the expected change in the response for each ten-fold increase in $X$.
  \end{itemize}
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Explanatory variable is logged}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[Y|X] = \beta_0 + \beta_1 \log(X)  \]

\begin{itemize}[<+->]
\item $\beta_0$ is the median corn yield per acre when fertilizer level is 1 lb/acre and 
\item $\beta_1 \log(2)$ is the expected change in corn yield when fertilizer level is doubled. 
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Both response and explanatory variables are logged}

If 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 \log(X) \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 \log(X)} = e^{\beta_0} X^{\beta_1} \]
\pause, then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected $\log(Y)$ when $\log(X)$ is zero and
\item $\beta_1$ is the expected change in $\log(Y)$ for a one unit increase in $\log(X)$. 
\end{itemize}

\pause 
Alternatively,

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median of $Y$ when $X$ is 1 and 
\item $d^{\beta_1}$ is the multiplicative change in the median of the response when $X$ increase multiplicatively by $d$, e.g.
  \begin{itemize}
  \item $2^{\beta_1}$ is the multiplicative effect on the median of the response for each doubling of $X$ or
  \item $10^{\beta_1}$ is the multiplicative effect on the median of the response for each ten-fold increase in $X$.
  \end{itemize}
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Both response and explanatory variables are logged}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 \log(X) \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 \log(X)} = e^{\beta_0} X^{\beta_1} \]

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median corn yield per acre when fertilizer level is 1 lb/acre and 
\item $2^{\beta_1}$ is the multiplicative effect on median corn yield per acre when fertilizer level doubles.
\end{itemize}

\end{frame}


\end{document}
