\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Regression diagnostics}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center', message=FALSE)
library(plyr)
library(ggplot2)
library(xtable)
library(Sleuth3)
library(abd)
@


\begin{document}



\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{All models are wrong!}

George Box (Empirical Model-Building and Response Surfaces, 1987):
\begin{quote}
All models are wrong, but some are useful. 
\end{quote}

\vspace{0.2in} \pause

{\tiny \url{http://stats.stackexchange.com/questions/57407/what-is-the-meaning-of-all-models-are-wrong-but-some-are-useful}}

{\small
\begin{quotation}
``All models are wrong" that is, every model is wrong because it is a simplification of reality. Some models, especially in the "hard" sciences, are only a little wrong. They ignore things like friction or the gravitational effect of tiny bodies. Other models are a lot wrong - they ignore bigger things. \pause \\

``But some are useful" - simplifications of reality can be quite useful. They can help us explain, predict and understand the universe and all its various components. \pause \\

This isn't just true in statistics! Maps are a type of model; they are wrong. But good maps are very useful. 
\end{quotation}
}


\end{frame}


\section{Regression diagnostics}
\frame{\frametitle{Regression}
  The simpler linear regression model is 
  \[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
	\pause this can be rewritten as 
	\[ Y_i = \beta_0 + \beta_1 X_i + e_i \qquad e_i \stackrel{ind}{\sim} N(0,\sigma^2) \]
	\pause where we estimate the errors via the residuals
	\[ r_i = \hat{e}_i = Y_i - (\hat{\beta}_0+\hat{\beta}_1 X_i). \]
	
	\vspace{0.2in} \pause
	
	Key assumptions are:
	\begin{itemize}[<+->]
  \item Normality of the errors
	\item Constant variance of the errors
	\item Independence between observations
  \item Linearity between mean response and explanatory variable
	\end{itemize}
}



\subsection{Normality}
\begin{frame}[fragile]
\frametitle{Histograms with best fitting bell curves}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
x = rnorm(100)
hist(x, freq=F, main="Normal data", ylim=c(0,dnorm(0)))
curve(dnorm, add=TRUE, lwd=2)
@

\end{frame}









\begin{frame}
\frametitle{Normal QQ-plot}

\begin{definition}
The quantile-quantile or qq-plot is an exploratory graphical device used to check the validity of a distributional assumption for a data set.
\end{definition}

\vspace{0.2in} \pause

A normal qq-plot graphs the theoretical quantiles from a normal distribution versus the observed quantiles. 

\vspace{0.2in} \pause

\begin{remark}
The bottom line is that, if the distribution assumption is satisfied, the points should fall roughly along the y=x line. 
\end{remark}

\end{frame}



\begin{frame}[fragile]
\frametitle{Normal}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
n = 1000
x = rnorm(n)
par(mfrow=c(1,3))
for (i in c(10,100,1000)) { qqnorm(x[1:i], main=paste("n=",i)); qqline(x[1:i]) }
@
	SAS swaps the x and y axes
\end{frame}





\begin{frame}[fragile]
\frametitle{Normal (n=10)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
par(mfrow=c(2,5), mar=rep(0,4)+.5)
for (i in 1:10) {
  qqnorm(rnorm(10), main='', xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(x)
}
@
	SAS swaps the x and y axes
\end{frame}




\begin{frame}[fragile]
\frametitle{Normal(n=100)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
par(mfrow=c(2,5), mar=rep(0,4)+.5)
for (i in 1:10) {
  qqnorm(rnorm(100), main='', xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(x)
}
@
  SAS swaps the x and y axes
\end{frame}



\begin{frame}[fragile]
\frametitle{Normal (n=1000)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
par(mfrow=c(2,5), mar=rep(0,4)+.5)
for (i in 1:10) {
  qqnorm(rnorm(1000), main='', xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(x)
}
@
  SAS swaps the x and y axes
\end{frame}




\begin{frame}
\frametitle{Not normal (n=10)}
<<echo=FALSE>>=
set.seed(20141018)
n=10
x = rnorm(n)
par(mfrow=c(2,2), mar=c(0,0,4,0)+.5)
qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
@
	SAS swaps the x and y axes
\end{frame}



\begin{frame}
\frametitle{Not normal (n=100)}
<<echo=FALSE>>=
n=100
x = rnorm(n)
par(mfrow=c(2,2), mar=c(0,0,4,0)+.5)
qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
@
  SAS swaps the x and y axes
\end{frame}



\begin{frame}
\frametitle{Not normal (n=1000)}
<<echo=FALSE>>=
n=1000
x = rnorm(n)
par(mfrow=c(2,2), mar=c(0,0,4,0)+.5)
qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
@
  SAS swaps the x and y axes
\end{frame}






\subsection{Constant variance}
\begin{frame}
\frametitle{Constant variance}

Recall the model
\[ Y_i = \beta_0+\beta_x X_i + e_i \qquad e_i \stackrel{iid}{\sim} N(0,\sigma^2) \]
so the variance for the $e_i$ is constant. 

\vspace{0.2in} \pause

To assess this assumption, we look at plots of residuals vs anything and look for patterns that show different ``spreads''\pause, e.g.
\begin{itemize}
\item funnels
\item football shapes
\end{itemize}

\vspace{0.2in} \pause

The most common way this assumption is violated is by having increasing variance with increasing mean\pause, thus we often look at a residuals vs predicted (fitted) mean plot.

\end{frame}



\begin{frame}[fragile]
\frametitle{Constant variance}

<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
for (n in c(10,100,1000)) { 
  plot(runif(n), rnorm(n), axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Constant variance}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
plot(runif(n), rnorm(n), axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
xx = (1:4)/5
yy = 2.5
arrows(xx, -yy, xx, yy, col="red", lwd=2, code = 3)
abline(h=0, col="blue")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Extreme Non-Constant variance (funnel)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
ub = 1
x = runif(n,0, ub)
e = rnorm(n,0,x)
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Extreme non-Constant variance (funnel)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
xx = ub*(1:4)/5
yy = ub*(.5+(1:4))/2
arrows(xx, -yy, xx, yy, col="red", lwd=2, code = 3)
abline(h=0, col="blue")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=10, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
n = 10
lb = 1
ub = 4
for (i in 1:3) {
  x = runif(n, lb, ub)
  e = rnorm(n,0,x)
  plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=100, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
n = 100
for (i in 1:3) {
  x = runif(n, lb, ub)
  e = rnorm(n,0,x)
  plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=1000, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
n = 1000
for (i in 1:3) {
  x = runif(n, lb, ub)
  e = rnorm(n,0,x)
  plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Extreme non-Constant variance (football)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
ub = 1
x = runif(n, -1, 1)
e = rnorm(n,0, 1-abs(x))
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
@

\end{frame}





\subsection{Independence}
\frame{\frametitle{Independence}
	Lack of independence includes
	\begin{itemize}
	\item Cluster effect
	\item Serial correlation
	\item Spatial association
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Make plots of residuals vs relevant explanatory variables and look for patterns, \pause e.g.
	\begin{itemize}[<+->]
	\item Residuals vs groups (prefer blocking)
	\item Residuals vs time (or observation number)
	\item Residuals vs spatial variable 
	\end{itemize}
}



\subsection{Summary}
\frame{\frametitle{Summary}
	Often the best strategy is graphical exploration of the data, \pause here are some relevant graphs: \pause
	\begin{itemize}
	\item transformed response vs transformed explanatory
	\item transformed response vs transformed explanatory
	\item qqplot of residuals
	\item residual vs fitted value
	\item residual vs explanatory
	\item residual vs observation number
	\item residual vs any other variable
	\end{itemize}
}





\subsection{Linearity}
\begin{frame}[fragile]
\frametitle{Linearity}
  Assess using scatterplots of (transformed) response vs (transformed) explanatory variable:
	
<<echo=FALSE>>=
n = 100
set.seed(1) 
x = runif(n,0,10)
e = rnorm(n)
y = x+e

myplot = function(x,y,main="") {
  plot(x,y,axes=FALSE, frame=TRUE, main=main)
}

par(mfrow=c(2,4), mar=c(0,0,4,0)+.5)
myplot(x, y)
myplot(x, exp(y))
myplot(exp(x), y)
myplot(exp(x), exp(y))
myplot(x, y)
myplot(x, y, "log(y)")
myplot(x, y, "log(x)")
myplot(x, y, "log(x), log(y)")
@
\end{frame}



\subsection{Lack-of-fit F-test}
\frame{\frametitle{Testing Composite hypotheses}
  Comparing two models
	\begin{itemize}
	\item $H_0:$ \alert{(reduced)}
	\item $H_1:$ \alert{(full)}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	\small
	Do the following
	\begin{enumerate}[1.]
	\item Calculate extra sum of squares.
	\item Calculate extra degrees of freedom
	\item Calculate 
	\[ \mbox{F-statistic} = \frac{\mbox{Extra sum of squares / Extra degrees of freedom}}{\hat{\sigma}^2_{full}} \]
	\item Compare this to an F-distribution with 
		\begin{itemize}
		\item numerator degrees of freedom = extra degrees of freedom
		\item denominator degrees of freedom = degrees of freedom in estimating $\hat{\sigma}_{full}^2$
		\end{itemize}
	\end{enumerate}
}


\frame{\frametitle{Lack-of-fit F-test}
  Let $Y_{ij}$ be the $i^{th}$ observation from the $j^{th}$ group where the group is defined by those observations having the same explanatory variable value ($X_j$). 
  
  \vspace{0.1in} \pause

	Two models:
	
	\begin{tabular}{lll}
	ANOVA: & $Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2)$ & \pause \uncover<3->{\alert{(full)}} \\
	Regression: & $Y_{ij} \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_j, \sigma^2)$ & \uncover<3->{\alert{(reduced)}}
	\end{tabular}
	
	\vspace{0.2in} \pause \pause
	
	\begin{itemize}[<+->]
	\item Regression model is reduced:
	\begin{itemize}
	\item ANOVA has $J$ parameters for the mean
	\item Regression has 2 parameters for the mean
	\item Set $\mu_{j} = \beta_0+\beta_1 X_j$.
	\end{itemize}
	\item Small pvalues indicate a lack-of-fit, i.e. the reduced model is not adequate.
	\item Lack-of-fit F-test requires multiple observations at a few $X_j$ values!
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Telomere length}
<<echo=FALSE>>=
ggplot(Telomeres, aes(factor(years), telomere.length))+
  geom_boxplot()+
  geom_jitter()+
  labs(x="Years", y="Telomere length", 
       title="Telomere length vs years since diagnosis")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Telomere length}
<<echo=FALSE>>=
ggplot(Telomeres, aes(years, telomere.length))+
  geom_jitter()+
  geom_smooth(method=lm, se=FALSE)+
  labs(x="Years", y="Telomere length", 
       title="Telomere length vs years since diagnosis")
@
\end{frame}

\frame[containsverbatim]{\frametitle{SAS code}
\tiny
\begin{verbatim}
DATA t;
  INFILE 'telomeres.csv' DSD FIRSTOBS=2;
  INPUT years length;

PROC REG DATA=t;
  MODEL length = years / CLB LACKFIT;
  RUN;

                                        The REG Procedure
                                          Model: MODEL1
                                   Dependent Variable: length 

                             Number of Observations Read          39
                             Number of Observations Used          39


                                      Analysis of Variance
 
                                             Sum of           Mean
         Source                   DF        Squares         Square    F Value    Pr > F

         Model                     1        0.22777        0.22777       8.42    0.0062
         Error                    37        1.00033        0.02704                     
           Lack of Fit             9        0.18223        0.02025       0.69    0.7093
           Pure Error             28        0.81810        0.02922                     
         Corrected Total          38        1.22810           
\end{verbatim}

\pause

Indicates no evidence for a lack of fit, i.e. regression seems adequate.
}



\begin{frame}[fragile]
<<>>=
# Use as.factor to turn a continuous variable into a categorical variable
m_anova = lm(telomere.length ~ as.factor(years), Telomeres) 
m_reg   = lm(telomere.length ~           years , Telomeres)
anova(m_reg, m_anova)
@

\vspace{0.2in} \pause

No evidence of a lack of fit. 

\end{frame}


\frame{\frametitle{Lack-of-fit F-test summary}
	\begin{itemize}[<+->]
	\item Lack-of-fit F-test tests the assumption of linearity
	\item Needs multiple observations at various explanatory variable values
	\item Small pvalue indicates a lack-of-fit, i.e. means are not linear
		\begin{itemize}
		\item Transform response, e.g. log
		\item Transform explanatory variable
		\item Add other explanatory variables
		\end{itemize}
	\end{itemize}
}






\end{document}
