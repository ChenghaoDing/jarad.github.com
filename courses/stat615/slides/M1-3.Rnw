\documentclass{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian regression}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(arm)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Bayesian regression}
\subsection{Default Bayesian inference}
\frame{\frametitle{Default Bayesian regression}
	Assume the standard noninformative prior
	\[ p(\beta,\sigma^2) \propto 1/\sigma^2 \]
	\pause then the posterior is 
	\[ \begin{array}{rl}
	p(\beta,\sigma^2|y) &= p(\beta|\sigma^2,y) p(\sigma^2|y) \pause \\
	\beta|\sigma^2,y &\sim N(\hat{\beta}_{MLE}, \sigma^2 V_\beta) \pause \\
	\sigma^2|y &\sim IG\left(\frac{n-k}{2}, \frac{[n-k]s^2}{2}\right) \pause \\
	\beta|y &\sim t_{n-k}(\hat{\beta}_{MLE}, s^2V_{\beta}) \pause \\
	\\
  V_\beta &= (X^\top X)^{-1} \pause \\
	\hat{\beta}_{MLE} &= V_\beta X^\top y \pause \\
	s^2 &= \frac{1}{n-k}(y-X\hat{\beta}_{MLE})^\top(y-X\hat{\beta}_{MLE})
	\end{array} \]
	\pause The posterior is proper if $n>k$ and rank$(X)=k$. 
}


\subsection{Cricket chirps}
\begin{frame}[fragile]
\frametitle{Cricket chirps}
	As an example, consider the relationship between the number of cricket chirps (in 15 seconds) and temperature (in Fahrenheit). From example in {\tt LearnBayes::blinreg}.
<<chirps_data, fig.width=8>>=
chirps=c(20,16.0,19.8,18.4,17.1,15.5,14.7,17.1,15.4,16.2,15,17.2,16,17,14.1)
temp=c(88.6,71.6,93.3,84.3,80.6,75.2,69.7,82,69.4,83.3,78.6,82.6,80.6,83.5,76.3)
qplot(temp,chirps)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Default Bayesian regression}

<<lm, echo=TRUE>>=
summary(m <- lm(chirps~temp))
confint(m) # Credible intervals
@
\end{frame}


\subsection{Subjective Bayesian inference}
\frame{\frametitle{Fully conjugate subjective Bayesian inference}
	If we assume the following normal-inverse-gamma prior,
	\[ \beta|\sigma^2 \sim N(b_0, \sigma^2 B_0) \qquad \sigma^2 \sim IG(a,b) \]
	\pause then the posterior is 
	\[ \beta|\sigma^2,y \sim N(b_n, \sigma^2 B_n) \qquad \sigma^2|y \sim IG(a',b') \]
	\pause with
	\[ \begin{array}{rl}
	B_n^{-1} &= \frac{1}{g}B_0^{-1} + \frac{1}{\sigma^2}X^\top X \\
	b_n &= B_n^{-1}\left[ \frac{1}{g}B_0^{-1}b_0 + \frac{1}{\sigma^2}X^\top y \right] \\
	a' &= a + \frac{n}{2} \\
	b' &= b + \frac{1}{2}(y-X b_0)^\top (XB_0X^\top+\I)^{-1}(y-X b_0) 
	\end{array} \]
}



\begin{frame}
\frametitle{Information about chirps per 15 seconds}

Let
\begin{itemize}
\item $Y_i$ is the average number of chirps per 15 seconds and 
\item $X_i$ is the temperature in Fahrenheit.
\end{itemize}

\vspace{0.2in} \pause

And we assume 
\[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
then 
\begin{itemize}
\item $\beta_0$ is the expected number of chirps at 0 degrees Fahrenheit
\item $\beta_1$ is the expected increase in number of chirps (per 15 seconds) for each degree increase in Fahrenheit.
\end{itemize}
\vspace{0.2in} \pause
Based on prior experience, the prior $\beta_1\sim N(0,1)$ might be reasonable. 

\end{frame}


\begin{frame}[fragile]
\frametitle{Subjective Bayesian regression}
<<subjective, echo=TRUE, warning=FALSE>>=
m = arm::bayesglm(chirps~temp, 
                  prior.mean.for.intercept  = 0,   # E[\beta_0]
                  prior.scale.for.intercept = 10,  # SD[\beta_0]
                  prior.df.for.intercept    = Inf, # normal prior for \beta_0
                  prior.mean  = 0,                 # E[\beta_1]
                  prior.scale = 1,                 # SD[\beta_1]
                  prior.df    = Inf,               # normal prior
                  scaled = TRUE)                   # scale explanatory variables
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Subjective Bayesian regression}
<<subjective2, echo=TRUE, warning=FALSE>>=
summary(m)
@
\end{frame}






\begin{frame}[fragile]
\frametitle{Subjective vs Default}
<<t-distribution, message=FALSE, echo=TRUE>>=
# default analysis
tmp = lm(chirps~temp)
tmp$coefficients
confint(tmp)

# Subjective analysis
m$coefficients
confint(m)
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Subjective vs Default}
<<t-distribution_fit, message=FALSE, fig.width=8>>=
qplot(temp, chirps) +
  geom_smooth(method='lm',formula=y~x, se=FALSE, color='red', linetype=2, size=2) + 
  geom_abline(intercept=m$coefficients[1], slope=m$coefficients[2], color='blue', linetype=3, size=2)
@
\end{frame}





\begin{frame}[fragile]
\frametitle{Shrinkage (as $V[\beta_1]$ gets smaller)}
<<shrinkage, fig.width=8>>=
d = ddply(data.frame(V=10^seq(-2,2,by=0.2)), .(V), function(x) {
  m = bayesglm(chirps~temp, prior.mean=0, prior.scale=x$V, prior.df=Inf)
  data.frame(beta0=m$coefficients[1], beta1=m$coefficients[2])
})
tmp = melt(d, id="V", value.name="estimate")
ggplot(tmp, aes(V, estimate)) +
  geom_line() +
  scale_x_log10() + 
  facet_wrap(~variable, scales='free')
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Shrinkage (as $V[\beta_1]$ gets smaller)}
<<shrinkage2>>=
ggplot(data.frame(chirps=chirps, temperature=temp), aes(temp, chirps)) + 
  geom_point() + 
  geom_abline(data=d, aes(intercept=beta0, slope=beta1, color=V)) + 
  scale_colour_gradient(trans = "log10")
@
\end{frame}




\section{Zellner's g-prior}
\begin{frame}
\frametitle{Zellner's g-prior}

Let 
\[ y = X\beta + \epsilon, \quad \epsilon \sim N(\sigma^2 \I). \]
If we choose the conjugate prior $\beta \sim N(b_0, \sigma^2 B_0)$, we still need to choose $b_0$ and $B_0$. \pause It seems natural to set $b_0=0$ which will shrink the estimates for $\beta$ toward zero, i.e. toward no effect. \pause But how should we choose $B_0$?

\vspace{0.2in} \pause 

One option is \alert{Zellner's $g$-prior} where $B_0 = g[X^\top X]^{-1}$ \pause where $g$ is either set or learned.

\end{frame}



\begin{frame}
\frametitle{Zellner's g-prior posterior}

Suppose 
\[ y \sim N(X\beta,\sigma^2\I) \]
where $X$ is $n\times p$ \pause 
and you use Zellner's g-prior
\[ \beta \sim N(b_0, g\sigma^2 (X'X)^{-1}). \]

\vspace{0.2in} \pause

The posterior is then 
\[ 
\beta|\sigma^2,y \sim N\left(\frac{1}{1+g}b_0 + \frac{g}{1+g}\hat{\beta}_{MLE}, \frac{\sigma^2 g}{g+1}(X'X)^{-1} \right) 
 \]

\end{frame}


\subsection{Marginal likelihood}
\begin{frame}
\frametitle{Marginal likelihood}

The marginal likelihood under Zellner's $g$-prior is 
\[ \begin{array}{rl}
p(y|g) = \frac{\mG\left(\frac{n-1}{2}\right)}{\pi^{\frac{n+1}{2}}n^{1/2}} ||y-\overline{y}||^{-(n-1)} \frac{(1+g)^{\frac{n-p-1}{2}}}{(1+g[1-R^2])^{\frac{n-1}{2}}}
\end{array} \]
where $R^2$ is the coefficient of determination.

\vspace{0.2in} \pause

And thus the Bayes Factor for comparing model 0 to model 1 is 
\[ BF(M_0:M_1) = \frac{(1+g[1-R_1^2])^{\frac{n-1}{2}}}{(1+g[1-R_0^2])^{\frac{n-1}{2}}} \]
\pause
where we used a common $g$ for both models and 
\begin{itemize}
\item $R_0^2$ is the coefficient of determination for model 0
\item $R_1^2$ is the coefficient of determination for model 1
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Why the marginal likelihood?}

By Bayes' rule, we have 
\[ p(\theta|y,M) = p(y|\theta,M)p(\theta|M)/p(y|M) \]
\pause
Rearranging yields
\[ 
p(y|M) = p(y|\theta,M)p(\theta|M)/p(\theta|y,M)
\]
\pause
Taking logarithms yields
\[ 
\log p(y|M) = \log(y|\theta,M) + \log p(\theta|M) - \log p(\theta|y,M) 
\]
\pause
where $\log(y|\theta,M)$ is the likelihood and $\log p(\theta|M) - \log p(\theta|y,M)$ is the penalty. 

\end{frame}




\begin{frame}
\frametitle{Setting $g$}

In Zellner's g-prior, 
\[ \beta \sim N(b_0, g\sigma^2 (X'X)^{-1}), \]
we need to determine how to set g.

\vspace{0.2in} \pause

Here are some thoughts:
\begin{itemize}[<+->]
\item $g\to 0$ makes posterior equal to the prior, \pause
\item $g=1$ puts equal weight to prior and likelihood, \pause
\item $g=n$ means prior has the equivalent weight of 1 observation, \pause
\item $g\to \infty$ recovers a uniform prior,
\item empirical Bayes estimate of $g$, $\hat{g}_{EB} =\mbox{argmax}_g p(y|g)$, or 
\item put a prior on $g$ and perform a fully Bayesian analysis.
\end{itemize}
\end{frame}



\subsection{Model selection}
\begin{frame}
\frametitle{Model selection}

If $\beta$ is a vector of length $p$, let $\gamma$ be a vector with binary elements that indicate whether that component of $\beta$ is non-zero, i.e. that explanatory variable is included. \pause For example, 
\[ \gamma = (1,0,1,1,0,0,0,1) \]
indicates that $\beta$ is of length 8 and that the first, third, fourth, and eighth elements are non-zero. \pause
Then we have $X_\gamma$ which indicates the design matrix that only has columns corresponding to those columns in $\gamma$ that are non-zero \pause and $\beta_\gamma$ is the subset of $\beta$ including elements of $\beta$ where $\gamma$ is 1. \pause

\vspace{0.2in} \pause

Now, we have $2^p$ models $M_\gamma$ of the form
\[ y = X_\gamma \beta_\gamma + \epsilon \]
where $\epsilon \sim N(0,\sigma^2\I)$. \pause Two special cases are 
\[ \begin{array}{rl}
\gamma_{null} &= (0,\ldots,0) \\
\gamma_{full} &= (1,\ldots,1)
\end{array} \]
\end{frame}



\begin{frame}
\frametitle{Model selection (cont.)}

If we want to compare $M_\gamma$ to $M_{\gamma'}$, we can use the Bayes Factor 
\[ 
BF(M_\gamma:M_{\gamma'}) = \frac{p(y|M_\gamma)}{p(y|M_{\gamma'})} = \frac{(1+g[1-R_{\gamma'}^2])^{\frac{n-1}{2}}}{(1+g[1-R_\gamma^2])^{\frac{n-1}{2}}}
\] 
For some base model, $M_b$, we can calculate 
\[ 
BF(M_\gamma:M_b) = \frac{p(y|M_\gamma)}{p(y|M_b)}
\]
Then, for any two models, 
\[
BF(M_\gamma:M_{\gamma'}) = \frac{BF(M_\gamma:M_b) }{BF(M_{\gamma'}:M_b) }
\]
If the base model is the null model, then the common parameters amongst the models are $\sigma^2$ and possibly an intercept $\alpha$. \pause We can place an improper prior on these parameters, typically $p(\alpha,\sigma^2)\propto 1/\sigma^2$. 
\end{frame}



\begin{frame}[fragile]
\frametitle{Zellner's g-prior in R}
<<echo=TRUE, warning=FALSE, message=FALSE>>=
library(BMS)
<<echo=TRUE, warning=FALSE, message=FALSE>>=
m0 = zlm(chirps~1   , g='UIP') # g=n
m1 = zlm(chirps~temp, g='UIP') # g=n
(bf = exp(m1$marg.lik-m0$marg.lik))
@
\end{frame}


\begin{frame}
\frametitle{Limiting Bayes Factors}

If the base model is the null model, then 
\[ 
BF(M_\gamma:M_{null}) = \frac{(1+g)^{(n-p_\gamma-1)/2}}{(1+g[1-R_\gamma^2])^{(n-1)/2}}
\]
where $p_\gamma$ is the number of non-zero elements in $\gamma$, i.e. the number of explanatory variables included in the model.

\vspace{0.2in} \pause 

\begin{itemize}
\item If $g\to \infty$, then $BF(M_\gamma:M_{null}) \to 0$. \pause (Lindley's Paradox) \pause
\item If $n\to\infty$, then $BF(M_\gamma:M_{null})\to \infty$. \pause
\item If $R_\gamma^2$, then $BF(M_\gamma:M_{null}) \to (1+g)^{(n-p_\gamma-1)/2}$. \pause (information paradox)
\end{itemize}

\pause

If $M_\lambda$ is the true model, we would like 
\[ 
BF(M_\lambda:M_\gamma) \stackrel{a.s.}{\longrightarrow} \infty
\]
for any other model $M_\gamma$. \pause This is called \alert{model selection consistency}.

\end{frame}



\subsection{Empirical Bayes}
\begin{frame}
\frametitle{Empirical Bayes}

The empirical Bayes approach chooses $g$ such that it maximizes $p(y|M_\gamma,g)$. \pause It turns out that $g_\gamma^{EB} = \max(F_\gamma-1,0)$, where 
\[ 
F_\gamma = \frac{R_\gamma^2/p_\gamma}{(1-R_\gamma^2)/(n-p_\gamma-1)}.
\]
Plugging this back into the expression for the Bayes Factor, we find that 
\[ 
BF^{EB}(M_\gamma:M_{null}) \to \infty
\]
as $R_\gamma \to 1$ and thus the Empirical Bayes approach does not suffer from either paradox. \pause Unfortunately, this empirical Bayes approach is not consistent. 
\end{frame}



\subsection{Fully Bayesian}
\begin{frame}
\frametitle{Fully Bayesian}

Alternatively, we can perform a fully Bayes analysis by putting a prior on $g$. \pause The \alert{Zellner-Siow} prior is 
\[ g \sim IG\left( \frac{1}{2}, \frac{n}{2} \right) \]

\vspace{0.2in} \pause

For this prior, we have $BF^{EB}(M_\gamma:M_{null}) \to \infty$ as $R_\gamma^2 \to 1$ and thus do not suffer from any paradoxes \pause and we have model selection consistency, i.e. $BF(M_\lambda:M_\gamma) \stackrel{a.s.}{\longrightarrow} \infty$ for true model $M_\lambda$ compared to any other model $M_\gamma$. 

\vspace{0.2in} \pause

There are other priors for $g$ that have these properties.

\end{frame}



\end{document}
