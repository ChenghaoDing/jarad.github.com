\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,xmpmulti}

\graphicspath{{figs/}}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian nonparametrics}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               cache=TRUE,
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE>>=
library(ggplot2)
library(grid)
library(gridExtra)
library(plyr)
library(dplyr)
library(reshape2)
library(rjags)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Finite mixtures}


\begin{frame}
\frametitle{Complicated distributions}
<<rnaseq, message=FALSE>>=
library(edgeR)
d = read.table("rna_seq.txt", header=TRUE)

# Keep only total columns
d = d[,c(1, grep("total", names(d)))]


#regmatches(names(d), regexpr('_[B,M]{1,2}\\.', names(d)))


# Rename columns
names(d)[-1] = paste(c("B73","Mo17","B73xMo17","Mo17xB73"), 
                     rep(1:4, each=4), 
                     sep="_")

# Order columns
B_cols = c(2,6,10,14)
d = d[,c(1, B_cols, B_cols+1, B_cols+2, B_cols+3)]

variety = factor(gsub("_[0-9]{1,2}", "", names(d)[-1]), 
                 levels = c("B73","Mo17","B73xMo17","Mo17xB73"))

# phi, alpha, delta parameterization
design = cbind(1,
               ifelse(variety=='Mo17', -1, 1),
               ifelse(variety=='B73',  -1, 1),
               ifelse(variety=='B73xMo17', 1, ifelse(variety=='Mo17xB73',-1,0)))

# GLM fit using edgeR
fit = d[,-1] %>% 
  DGEList() %>%
  calcNormFactors %>%
  estimateCommonDisp %>%
  estimateGLMTagwiseDisp(design) %>%
  glmFit(design)

# Calculate gene-specific estimates for phi, alpha, delta, and psi
hat = data.frame(gene = 1:length(fit$dispersion),
                 phi   = fit$coefficients[,1] + mean(fit$offset[1,]),
                 alpha = fit$coefficients[,2],
                 delta = fit$coefficients[,3],
                 gamma = fit$coefficients[,4],
                 psi   = log(fit$dispersion))

hat$gene = d$GeneID
@

<<rnaseq_plot, dependson='rnaseq', message=FALSE, fig.keep='last', fig.height=10, fig.width=12>>=
parms = names(hat)[-1]
plots = list()

n = length(parms)
for (i in 1:n) {
  for (j in 1:n) {
    p = (i-1)*n+j # plot number
    if (i>j) 
      plots[[p]] = grid.rect(gp=gpar(col="white"))
    if (i==j) {
      plots[[p]] = ggplot(hat, aes_string(parms[i])) + 
        geom_histogram(fill='gray') +
        theme_bw() + 
        labs(x=parse(text=parms[i]))
    }
    if (i<j) {   
      plots[[p]] = ggplot(hat, aes_string(parms[j], parms[i])) + 
        stat_binhex(bin=100) +
        theme_bw() + 
        scale_fill_gradientn(trans='log',breaks=c(1,10,100,1000), guide=FALSE, colours=c("gray","black")) + 
        labs(x=parse(text=parms[j]), y=parse(text=parms[i]))
    }
  }
}

do.call("grid.arrange", c(plots, ncol=5))
@
\end{frame}


\begin{frame}
\frametitle{Finite mixtures}

Let's focus on modeling the univariate distribution for $\phi$

<<phi, dependson='rnaseq'>>=
ggplot(hat, aes(phi)) + 
  geom_histogram(aes(y=..density..))
@
\end{frame}







\subsection{Finite mixture}
\begin{frame}
\frametitle{Finite mixture}

A model for the marginal distribution for $\phi$ is

\[ 
Y_i \stackrel{ind}{\sim} \sum_{h=1}^H \pi_h N(\mu_h, \sigma_h^2)
\]
where $\sum_{h=1}^H \pi_h = 1$. 

\vspace{0.2in} \pause

Alternatively, we can introduce a latent variable $\zeta_i=h$ if observation $i$ came from group $h$. \pause
Then 
\[ \begin{array}{rl}
Y_i|\zeta_i=z &\stackrel{ind}{\sim} N(\mu_{z},\sigma_{z}^2) \\ 
\zeta_i &\stackrel{ind}{\sim} Cat(H,\pi)
\end{array} \]
where $Cat(H,\pi)$ is a categorical random variable with $P(\zeta=h) = \pi_h$ for $h=1,\ldots,H$ and $\pi=(\pi_1,\ldots,\pi_H)$. 

\end{frame}



\begin{frame}
\frametitle{Prior distributions}

The parameters of the model are unidentified due to \alert{label-switching}, \pause i.e. 
\[ Y_i \stackrel{ind}{\sim} \sum_{h=1}^H \pi_h N(\mu_h, \sigma_h^2) \stackrel{d}{=} \sum_{h'=1}^H \pi_{h'} N(\mu_{h'}, \sigma_{h'}^2) \]
for some permutation $h'$. 

\vspace{0.2in} \pause

One way to resolve this issue is to enforce identifiability in the prior. \pause For example, in one-dimension, we can order the component means: $\mu_1<\mu_2<\cdots<\mu_H$. 

\vspace{0.2in} \pause

To ensure the posterior is proper
\begin{itemize}
\item Maintain proper prior for $\pi$
\item Ensure proper prior for ratios of variances 

(perhaps by ensuring prior is proper for variances themselves)
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Two conditionally conjugate prior options}

Option 1:
\[ 
Dir(\pi;a) \I(\mu_1<\cdots<\mu_H) \prod_{h=1}^H N(\mu_h;m_h,v_h^2\phantom{\sigma_h^2}) IG(\sigma_h^2; c_h, d_h) 
\]

\vspace{0.2in} \pause

Option 2:
\[
Dir(\pi;a) \I(\mu_1<\cdots<\mu_H)  \prod_{h=0}^1 N(\mu_h;m_h,v_h^2\sigma_h^2) IG(\sigma_h^2; c_h, d_h)
\]

\end{frame}


% \begin{frame}
% \frametitle{MCMC - Option 1}
% The steps of a Gibbs sampler are 
% \begin{enumerate}
% \item Sample $\zeta_i \stackrel{ind}{\sim} p(\zeta_i|\ldots)$ where 
% \[ 
% P(\zeta_i=1|\ldots) = \frac{\pi_1 N(y_i; \mu_1, \sigma_1^2)}{\pi_0 N(y_i; \mu_0, \sigma_0^2)+\pi_1 N(y_i; \mu_1, \sigma_1^2)}.
% \]
% \item For $h=0,1$, sample $\mu_h\sim p(\mu_h|\ldots)$ where
% \[
% p(\mu_h|\ldots) \propto \left[ \prod_{i:\zeta_i=h} N(y_i;\mu_h,\sigma_h^2) \right] N(\mu_h;m_h,v_h^2\phantom{\sigma_h^2}) 
% \]
% \end{enumerate}
% \end{frame}


\subsection{JAGS}
\begin{frame}[fragile]
<<jags, echo=TRUE>>=
library(rjags)
jags_model = "
model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (i in 1:H) {
    mu[i] ~ dnorm(0,1e-5)
    tau[i] ~ dgamma(1,1)
    sigma[i] <- 1/sqrt(tau[i])
  }

  pi ~ ddirich(a)
}"
@

<<jags_data, dependson='rnaseq', echo=TRUE>>=
tmp = hat[sample(nrow(hat), 1000),]
dat = list(n=nrow(tmp), H=3, y=tmp$phi, a=rep(1,3))
@

<<jags_run, dependson=c('jags','jags_data'), echo=TRUE, results='hide'>>=
jm = jags.model(textConnection(jags_model), data = dat, n.chains = 3)
r = coda.samples(jm, c('mu','sigma','pi'), 1e3)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics}
<<jags_convergence, dependson='jags_run', echo=TRUE>>=
gelman.diag(r)
gelman.diag(r, multivariate=FALSE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics (2)}
<<jags_convergence2, dependson='jags_run', echo=TRUE>>=
plot(r, density=FALSE)
@
\end{frame}




\subsection{JAGS}
\begin{frame}[fragile]
<<jags2, echo=TRUE>>=
library(rjags)
jags_model = "
model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (i in 1:H) {
    mu0[i] ~ dnorm(0,1e-5)
    tau[i] ~ dgamma(1,1)
    sigma[i] <- 1/sqrt(tau[i])
  }

  mu[1:H] <- sort(mu0)
  pi ~ ddirich(a)
}"
@

<<jags2_run, dependson=c('jags_data', 'jags2'), echo=TRUE, results='hide'>>=
jm = jags.model(textConnection(jags_model), data = dat, n.chains = 3)
r = coda.samples(jm, c('mu','sigma','pi'), 1e3)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics}
<<jags2_convergence, dependson='jags2_run', echo=TRUE>>=
gelman.diag(r)
gelman.diag(r, multivariate=FALSE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics (2)}
<<jags2_convergence2, dependson='jags2_run', echo=TRUE>>=
plot(r, density=FALSE)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Posterior on data density}
<<posterior_density, dependson='jags2_run'>>=
draws = ddply(data.frame(parameter=varnames(r)), .(parameter), function(x) {
  data.frame(iteration = start(r):end(r),
             chain = rep(1:3, each=end(r)-start(r)+1),
             value = unlist(r[,x$parameter])) 
}) %>%
  dcast(iteration+chain~parameter)

d = ddply(draws, .(iteration, chain), function(x) {
  xx = seq(-3, 10, by=0.1)
  data.frame(xx = xx,
             yy = 
               x$`pi[1]`*dnorm(xx, x$`mu[1]`, x$`sigma[1]`) + 
               x$`pi[2]`*dnorm(xx, x$`mu[2]`, x$`sigma[2]`) + 
               x$`pi[3]`*dnorm(xx, x$`mu[3]`, x$`sigma[3]`))
})

sm = ddply(d, .(xx), summarize, 
           mean=mean(yy), sd=sd(yy),
           lb = quantile(yy, .025),
           ub = quantile(yy, .975))
@

<<posterior_density_plot, dependson=c('posterior_density','jags_data'), results='hide'>>=
ggplot(data.frame(x=c(-3,10)), aes(x)) + 
  geom_histogram(data=tmp, aes(x=phi, y=..density..), binwidth=0.2, alpha=0.5)+
  geom_ribbon(data=sm, aes(x=xx,y=mean, ymin=lb, ymax=ub), fill='blue', alpha=0.5) 
#  geom_line() +
  labs(x='x', y='density') 
@
\end{frame}



\subsection{Clustering}
\begin{frame}[fragile]
\frametitle{Group membership}

Group membership can be obtained using the $\zeta_i$, e.g. 
\[
P(\mbox{gene $i$ in cluster $h$}) = P(\zeta_i=h|y) \approx \sum_{m=1}^M \I\left(\zeta_i^{(m)} = h\right).
\]

<<clustering, dependson=c('jags2_run'), results='hide'>>=
jm = jags.model(textConnection(jags_model), data = dat, n.chains = 3)
r = coda.samples(jm, c('zeta'), 1e3)
@

<<clustering_results, dependson='clustering'>>=
draws = ddply(data.frame(parameter=varnames(r)), .(parameter), function(x) {
  data.frame(iteration = start(r):end(r),
             chain = rep(1:3, each=end(r)-start(r)+1),
             value = unlist(r[,x$parameter])) 
}) 

sm = ddply(draws, .(parameter), function(x) {
  data.frame(p1 = mean(x$value==1), 
             p2 = mean(x$value==2),
             p3 = mean(x$value==3))
})
head(sm)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Clustering}

Genes can then be clustered by assigning them to a group based on their posterior probabilities of group membership, i.e. for gene $i$, we assign the group according to 
\[
\mbox{argmax}_h P(\zeta_i=h|y).
\]

<<clustering_final>>=
cluster = ddply(sm, .(parameter), function(x) data.frame(group=which.max(x[,-1])))
library(gtools)
ordr = mixedorder(as.character(cluster$parameter))
# ggplot(cbind(cluster[ordr,], tmp), aes(x=phi,y=..density..)) +
#   geom_histogram(binwidth=0.2, alpha=0.5) +
#   facet_wrap(~group)
@

\vspace{0.2in} \pause

Unfortunately clustering is extremely sensitive to the parametric model chosen, e.g. normal in this example, and the cluster could change dramatically with a different choice, e.g. $t$. 
\end{frame}


\subsection{Choosing $H$}
\begin{frame}
\frametitle{Choosing $H$}
When using finite mixture modeling one of the key choices is to choose $H$ the number of clusters. 

\begin{itemize}[<+->]
\item A Bayesian approach would place a prior on $H$, e.g. a Poisson or truncated Poisson, and then use reversible jump MCMC to estimate it.
\item A more pragmatic approach is to start with a small $H$ and then determine whether there is some feature of the data that is not being adequately addressed\pause, e.g. via posterior predictive pvalues. 
\item An approach that is somewhere in between is to find an MLE (or MAP) via 
\[ 
\hat{H} = \mbox{argmax}_H p(y|H) = \int p(y|\pi,\mu,\sigma^2,H) p(\pi,\mu,\sigma^2|H) d\pi d\mu d\sigma^2
\]
and then condition on $\hat{H}$ in the analysis. \pause Typically this MLE (or MAP) is found via the EM algorithm.
\end{itemize}

\end{frame}


\end{document}

