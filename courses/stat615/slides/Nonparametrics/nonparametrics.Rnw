\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,xmpmulti}

\graphicspath{{figs/}}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian nonparametrics}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               cache=TRUE,
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE>>=
library(ggplot2)
library(grid)
library(gridExtra)
library(plyr)
library(dplyr)
library(reshape2)
library(rjags)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}


\section{Bayesian nonparametrics}

\begin{frame}
\frametitle{Bayesian nonparametrics}

There are two main approaches to Bayesian nonparametrics for density estimation
\begin{itemize}[<+->]
\item Dirichlet process and
\item Polya trees
\end{itemize}

\vspace{0.2in} \pause

See \url{https://projecteuclid.org/euclid.ba/1369407550} for a general overview of all Bayesian nonparametric problems, e.g. density estimation, clustering, regression, random effects distributions, etc.

\end{frame}


\begin{frame}
\frametitle{Motivation}
<<rnaseq, message=FALSE>>=
library(edgeR)
d = read.table("rna_seq.txt", header=TRUE)

# Keep only total columns
d = d[,c(1, grep("total", names(d)))]


#regmatches(names(d), regexpr('_[B,M]{1,2}\\.', names(d)))


# Rename columns
names(d)[-1] = paste(c("B73","Mo17","B73xMo17","Mo17xB73"), 
                     rep(1:4, each=4), 
                     sep="_")

# Order columns
B_cols = c(2,6,10,14)
d = d[,c(1, B_cols, B_cols+1, B_cols+2, B_cols+3)]

variety = factor(gsub("_[0-9]{1,2}", "", names(d)[-1]), 
                 levels = c("B73","Mo17","B73xMo17","Mo17xB73"))

# phi, alpha, delta parameterization
design = cbind(1,
               ifelse(variety=='Mo17', -1, 1),
               ifelse(variety=='B73',  -1, 1),
               ifelse(variety=='B73xMo17', 1, ifelse(variety=='Mo17xB73',-1,0)))

# GLM fit using edgeR
fit = d[,-1] %>% 
  DGEList() %>%
  calcNormFactors %>%
  estimateCommonDisp %>%
  estimateGLMTagwiseDisp(design) %>%
  glmFit(design)

# Calculate gene-specific estimates for phi, alpha, delta, and psi
hat = data.frame(gene = 1:length(fit$dispersion),
                 phi   = fit$coefficients[,1] + mean(fit$offset[1,]),
                 alpha = fit$coefficients[,2],
                 delta = fit$coefficients[,3],
                 gamma = fit$coefficients[,4],
                 psi   = log(fit$dispersion))

hat$gene = d$GeneID

ggplot(hat, aes(phi)) + 
  geom_histogram(aes(y=..density..))
@
\end{frame}


\begin{frame}
\frametitle{Goal}
Let $Y_i$ come from an unknown probability distribution $G$, i.e. $Y_i\sim G$. \pause As a Bayesian, the natural approach is to put a prior on $G$. \pause That is, we want to make statements like 
\[ 
P(Y_i \in A) = G(A)
\]
for any set $A$. 
\end{frame}



\begin{frame}
\frametitle{Dirichlet process}

One approach is to use a Dirichlet process (Ferguson 1973). \pause We write 
\[ 
G \sim DP(aG_0)
\]
where 
\begin{itemize}[<+->]
\item $a>0$ is total mass (or concentration) parameter and
\item $G_0$ is the base measure, i.e. a probability distribution defined on the support of $G$. 
\end{itemize}

\vspace{0.2in} \pause

For any partition $\{A_1,\ldots,A_K\}$ of the sample space $S$, \pause the probability vector $[G(A_1),\ldots,G(A_K)]$ \pause follows a Dirichlet distribution, i.e.
\[ 
[G(A_1),\ldots,G(A_K)] \sim Dir([aG_0(A_1),\ldots,aG_0(a_K)]).
\]
Thus 
\begin{itemize}[<+->]
\item $E[G(A_1)] = G_0(A_1)$ and 
\item $V[G(A_1)] = \frac{G_0(A_1)[1-G_0(A_1)]}{1+a}$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Conjugacy of the Dirichlet process}

Assume
\[ 
Y_i \ind G \qquad \mbox{and} \qquad G\sim DP(aG_0)
\]
\pause
then for any partition $\{A_1,\ldots,A_K\}$, we have 
\[ \begin{array}{ll}
[G(A_1),\ldots,G(A_K)]|y \sim \\
\quad Dir\left(\left[aG_0(A_1) + \sum_{i=1}^n \I(y_i\in A_1),\ldots,aG_0(A_K) + \sum_{i=1}^n \I(y_i\in A_K)\right]\right)
\end{array} \]
\pause
and thus 
\[
G|y \sim DP\left(aG_0 + \sum_{i=1}^n \delta_{y_i}\right)
\]
which has 
\[ 
E(G(A)) = \left( \frac{a}{a+n} \right)G_0(A) + \left( \frac{n}{a+n} \right)\sum_{i=1}^n \frac{1}{n} \I(y_i\in A)
\]

\end{frame}



\begin{frame}
\frametitle{Stick-breaking representation}

A constructive representation of the Dirichlet process is the stick-breaking representation. \pause Assume $G\sim DP(aG_0)$, then 
\[ 
P(\cdot) = \sum_{h=1}^\infty \pi_h \delta_{\theta_h}(\cdot)
\]
where 
\begin{itemize}
\item $\pi_h = V_h\prod_{\ell<h} (1-V_\ell)$
\item $V_h \ind Be(1,a)$, and
\item $\theta_h\ind G_0$.
\end{itemize}


<<stick_breaking, fig.height=2>>=
# Stick-breaking realizations
calc_pi = function(v) {
    n = length(v)
    pi = numeric(n)
    cumv = cumprod(1 - v)
    pi[1] = v[1]
    for (i in 2:n) pi[i] = v[i] * cumv[i - 1]
    pi
}
par(mar = rep(0, 4))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(-0.31, 0.31), axes = F, xlab = "", 
    ylab = "")
segments(0, 0, 1, 0)
wd = 0.2
segments(c(0, 1), -wd, c(0, 1), wd)
wd = wd/1.1

set.seed(9)
pi = calc_pi(rbeta(5, 1, 10))
cpi = cumsum(pi)
segments(cpi, -wd, cpi, wd)
text(c(0, 1), -0.3, c(0, 1))

midpoint = function(x) {
    n = length(x)
    mp = numeric(n - 1)
    for (i in 2:n) mp[i - 1] = mean(x[c(i - 1, i)])
    mp
}
mp = midpoint(c(0, cpi, 1))
text(mp, -0.3, expression(pi[1], pi[2], pi[3], pi[4], pi[5], ...))
@

\end{frame}



\begin{frame}
\frametitle{Realizations from a DP}
<<stick_breaking_realizations, dependson='stick_breaking'>>=
rdp = function(alpha, rP0, H = 10000) {
    theta = rP0(H)
    v = rbeta(H, 1, alpha)
    pi = calc_pi(v)
    return(data.frame(theta = theta, v = v, pi = pi))
}

plot_rdp = function(rdp, ...) {
    plot(rdp$theta, rdp$pi, type = "h", ...)
}

d = ddply(expand.grid(alpha=10^seq(-2,2)), .(alpha), function(x) {
  rdply(4, function() {
    rdp(x$alpha, rnorm)
  })
})

ggplot(d, aes(x=theta, weight=pi)) +
  geom_histogram(aes(y=..density..), binwidth=0.1) + 
  facet_grid(alpha~.n, scales='free_y') + 
  stat_function(fun=dnorm, color='red')
@
\end{frame}



\begin{frame}
\frametitle{DP mixture}

If we have an absolutely continuous distribution we are trying to approximate, then a DP is not reasonable. \pause Thus, we may use a DP mixture, i.e. 
\[ 
Y_i \ind p(\cdot|\theta_i), \quad \theta_i \ind G, \quad G\sim DP(aG_0).
\]

Alternatively, if we use the stick-breaking construction, we have 
\[ 
Y_i \ind p(\cdot|\theta_i), \quad \theta_i \ind \sum_{h=1}^\infty \pi_h \delta_{\theta_h^*}
\]
where $\theta_h^* \ind G_0$ and $\pi \sim \mbox{stick}(a)$. 

\end{frame}



\begin{frame}
\frametitle{Finite approximation to the stick-breaking representation}

For some $\epsilon>0$, there exists an $H$ such that $\sum_{h=H}^\infty \pi_h < \epsilon$ and can be ignored. \pause The resulting model is 
\[ 
Y_i \ind p(\cdot|\theta_i), \quad \theta_i \ind \sum_{h=1}^H \pi_h \delta_{\theta_h^*}
\]
where 
\begin{itemize}
\item $\pi_h = V_h\prod_{\ell<h} (1-V_\ell)$ 
\item $V_h \ind Be(1,a)$ for $h<H$, and
\item $V_H=1$. 
\end{itemize}
\pause
\end{frame}



\subsection{Normal example}
\begin{frame}
\frametitle{Normal example}

A model for the marginal distribution for $Y_i=\phi_i$ is

\[ 
Y_i \ind N(\mu_i, \sigma_i^2) \quad 
\left(\begin{array}{cc} \mu_i \\ \sigma_i^2 \end{array} \right) \sim \sum_{h=1}^H \pi_h \delta_{(\mu_h,\sigma_h^{2*})}
\]
where $\sum_{h=1}^H \pi_h = 1$. 

\vspace{0.2in} \pause

Alternatively, we can introduce a latent variable $\zeta_i=h$ if observation $i$ came from group $h$. \pause
Then 
\[ \begin{array}{rl}
Y_i|\zeta_i=z &\ind N(\mu_{h},\sigma_{h}^2) \\ 
\zeta_i &\ind Cat(H,\pi)
\end{array} \]
where $\zeta\sim Cat(H,\pi)$ is a categorical random variable with $P(\zeta=h) = \pi_h$ for $h=1,\ldots,H$ and $\pi=(\pi_1,\ldots,\pi_H)$. 

\end{frame}



\subsection{Normal example}
\begin{frame}
\frametitle{Normal example}
Let
\[ 
Y_i \ind N(\mu_i,\sigma_i^2), \qquad (\mu_i,\sigma_i^2) \ind \sum_{h=1}^H \pi_h \delta_{(\mu_h^*,\sigma_h^{2*})}
\]
where the base measure is 
\[
\mu_h^*|\sigma_h^{2*} \ind N(m,v^2\sigma_h^{2*}) \qquad\mbox{and}\qquad \sigma_h^{2*} \ind IG(c,d).
\]
\pause

But since each $(\mu_i,\sigma_i^2)$ must equal $(\mu_h^*,\sigma_h^{2*})$ for some $h$, we can rewrite the model as 
\[ 
Y_i \ind \sum_{h=1}^H \pi_h N(\mu_h^*,\sigma_h^{2*})
\]
with a prior that is equal to the base measure. \pause Thus this model is equivalent to our finite mixture with the exception of the prior for $\pi$. 
\end{frame}



\begin{frame}
\frametitle{MCMC - Blocked Gibbs sampler}
\small

\vspace{-0.05in}

The steps of a Gibbs sampler with stationary distribution 
\[ 
p(\pi,\mu,\sigma^2,\zeta|y) \propto p(y|\zeta,\mu,\sigma^2) p(\zeta|\pi) p(\mu|\sigma^2) p(\sigma^2) p(\pi)
\]
has steps 

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\ldots) \propto \pi_h N(y_i; \mu_h^*, \sigma_h^{2*}) 
\]
\item \alert{Sample $V_h \ind Be(1+n_h,a+n_h^+)$ for $V=1,\ldots,H-1$ where $n_h = \sum_{i=1}^n \I(\zeta_i=h)$ and $n_h^+ = \sum_{h'=h+1}^H n_{h'}$ and set $V_H=1$. Then calculate $\pi_h = V_h\prod_{\ell<h} (1-V_\ell)$.}
\item For $h=1,\ldots,H$, sample $\mu_h,\sigma_h^2$ from their full conditional
\[ 
\mu_h^*|\sigma_h^{2*} \stackrel{ind}{\sim} N(m_h', v_h'^2) \quad \sigma_h^{2*} \stackrel{ind}{\sim} IG(c'_h, d'_h)
\]
where $m_h', v_h'^2, c_h'$, and $d_h'$ are exactly the same as in the normal finite mixture MCMC.

\vspace{0.2in} \pause

This Gibbs sampler is blocked because you sample $\zeta=(\zeta_1,\ldots,\zeta_n)$ jointly.
\end{enumerate}

\end{frame}



\subsection{JAGS}
\begin{frame}[fragile]
<<dp_normal_blocked_model, echo=TRUE>>=
library(rjags)
dp_normal_blocked = "
model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (h in 1:H) {
    mu[h] ~ dnorm(2,1/3)
    tau[h] ~ dgamma(.1,.1)
    sigma[h] <- 1/sqrt(tau[h])
  }

  # Stick breaking
  for (h in 1:(H-1)) { V[h] ~ dbeta(1,a) } 
  V[H] <- 1
  pi[1] <- V[1]
  for (h in 2:H) {
    pi[h] <- V[h] * (1-V[h-1]) * pi[h-1] / V[h-1]
  }
}"
@

<<jags_data, dependson='rnaseq', echo=TRUE>>=
tmp = hat[sample(nrow(hat), 1000),]
dat = list(n=nrow(tmp), H=25, y=tmp$phi, a=1)
@

<<jags_run, dependson=c('dp_normal_blocked_model','jags_data'), echo=TRUE, results='hide'>>=
jm = jags.model(textConnection(dp_normal_blocked), data = dat, n.chains = 3)
r = jags.samples(jm, c('mu','sigma','pi','zeta'), 1e3)
@
\end{frame}



\begin{frame}
\frametitle{Monitor convergence of density}

As previously discussed, the model as constructed as identifiability problems among the $\pi_h$, $\mu_h^*$, and $\sigma_h^{2*}$ due to label switching. \pause
What is identified in the model is the value of the density at any particular value. 

\vspace{0.2in} \pause

So rather than directly monitoring the parameters, we will monitor the estimated density, \pause i.e. at iteration $m$ of the MCMC, the estimated density at location $x$ is 
\[ 
\sum_{h=1}^H \pi_h^{(m)} N(x; \mu_h^{*(m)}, \sigma_h^{2*(m)}).
\]
\pause
Monitoring this quantity at a variety of locations $x$ will provide appropriate convergence assessment. 
\end{frame}




\begin{frame}
\frametitle{Monitor convergence of density}
<<jags_conversion,dependson='jags_run'>>=
xx = seq(min(dat$y)-1, max(dat$y)+1, length=101)
grid = expand.grid(iteration=1:dim(r$mu)[2],
                  chain = 1:dim(r$mu)[3],
                  x = xx)
d = 
ddply(grid, .(iteration,chain,x), function(d) {
  data.frame(density = sum(r$pi[,d$iteration, d$chain] * dnorm(d$x, 
                                                               r$mu[,d$iteration,d$chain], 
                                                               r$sigma[,d$iteration,d$chain])))
})
@

<<example_traceplots, dependson='jags_conversion'>>=
d$chain = as.factor(d$chain)
ggplot(subset(d, x == sample(xx, 4)), aes(iteration, density, color=chain, group=chain)) + 
  geom_line() + 
  facet_wrap(~x, scales='free')
@
\end{frame}



\begin{frame}
\frametitle{Monitoring the number of utilized components}

Since we are using a finite approximation to the DP, we should monitor the index of the maximum occupied component. \pause If the finite approximation is reasonable, then this index will be far from our choice of $H$. \pause If not, then $H$ should be increased. 

\vspace{0.2in} \pause

Specifically, at iteration $m$, we monitor
\[ 
\max \{ \zeta_1^{(m)}, \ldots,\zeta_n^{(m)} \}.
\]
\end{frame}


\begin{frame}
\frametitle{Monitoring the number of utilized components}

<<max_occupied_index, dependson='jags_run'>>=
index = adply(r$zeta, 2:3, function(d) data.frame(max_index = max(d)))
names(index)[1:2] = c('iteration','chain')
index$chain = as.factor(index$chain)
ggplot(index, aes(iteration, max_index, color=chain, group=chain)) + 
  geom_line() 
@

\end{frame}




\begin{frame}
\frametitle{Posterior density estimation}

<<posterior_density_estimation, dependson=c('jags_data','jags_conversion')>>=
sm = ddply(d, .(x), summarize,
           lb = quantile(density, .025),
           ub = quantile(density, .975))
ggplot(sm, aes(x)) + 
  geom_histogram(data=tmp, aes(x=phi, y=..density..), binwidth=0.2, alpha=0.5) + 
  geom_ribbon(aes(ymin=lb, ymax=ub), fill='blue', alpha=0.5) 
@


\end{frame}


\begin{frame}
\frametitle{Chinese restaurant process}

\small

Rather than utilizing the finite approximation to the DP, we can use the DP directly, by marginalizing out $G$. \pause
This results in a prior directly on $\theta_1,\ldots,\theta_n$ via 
\[ 
\theta_i|\theta_1,\ldots,\theta_{i-1} \sim \left( \frac{a}{a+i-1} \right)G_0(\theta_i) + \sum_{j=1}^{i-1} \left(\frac{1}{a+i-1} \right) \delta_{\theta_j}
\]

\pause

The conditional prior distribution for $\theta_i$ is 
\[ 
\theta_i| \theta_{-i} \sim \left( \frac{a}{a+n-1} \right)G_0(\theta_i) + \sum_{j\ne i} \left(\frac{1}{a+n-1} \right) \delta_{\theta_j}
\]
or, equivalently, 
\[ 
\theta_i| \theta_{-i} \sim \left( \frac{a}{a+n-1} \right)G_0(\theta_i) + \sum_{h=1}^{H^{(-i)}} \left(\frac{n_h^{(-i)}}{a+n-1} \right) \delta_{\theta_h^*}
\]
where $H^{(-i)}$ is the number of components without $i$ and $n_h^{(-i)}$ is the number of observations in each component without $i$. 
\end{frame}



\begin{frame}
\frametitle{Marginalized Gibbs sampler}

Using this Chinese restaurant process, we have the following MCMC

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\zeta_{-i},\ldots) \left\{ 
\begin{array}{ll} 
n_h^{(-i)} p(y_i|\theta_h^*) & h=1,\ldots,H^{(-i)} \\
a \int p(y_i;\theta) dG_0(\theta) & h=H^{(-i)}+1
\end{array} \right.
\]
\pause
If $\zeta_i = H^{(-i)}+1$, then sample $\theta^*_{\zeta_i}$ from its posterior using $y_i$ as the only observation.

\pause

\item For $h=1,\ldots,H$, sample $\theta_h^*$ from their full conditional
\[ 
\theta_h^*|\ldots \propto G_0(\theta_h^*)\prod_{i:\zeta_i=h} p(y_i|\theta_h^*)
\]
\pause
i.e. sample the parameters from their posteriors using only the data in that group.
\end{enumerate}

\end{frame}


\begin{frame}
\frametitle{Marginalized Gibbs sampler - Normal example}

For the normal example, we have 

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\zeta_{-i},\ldots) \left\{ 
\begin{array}{ll} 
n_h^{(-i)} N(y_i|\mu_h^*,\sigma_h^{2*}) & h=1,\ldots,H^{(-i)} \\
a\, t_{2c}(y_i;m,v^2[d/c]) & h=H^{(-i)}+1
\end{array} \right.
\]
\pause
If $\zeta_i = H^{(-i)}+1$, then sample $\mu^*_{\zeta_i},\sigma_{\zeta_i}^{2*}$ from its normal-inverse-gamma posterior using $y_i$ as a the only observation. \pause 

\item For $h=1,\ldots,H$, sample $\mu_h,\sigma_h^2$ from their full conditional
\[ 
\mu_h^*|\sigma_h^{2*} \stackrel{ind}{\sim} N(m_h', v_h'^2) \quad \sigma_h^{2*} \stackrel{ind}{\sim} IG(c'_h, d'_h)
\]
where $m_h', v_h'^2, c_h'$, and $d_h'$ are exactly the same as in the normal finite mixture MCMC.
\end{enumerate}

\end{frame}





\begin{frame}
\frametitle{Putting a prior on the concentration parameter}

If $G\sim DP(a G_0)$, then the concentration parameter ($a$) controls the prior on the number of clusters. \pause 
For example, if $a=1$, then in the prior two randomly selected observations have a 0.5 probability of belonging to the same cluster. \pause 
As $a$ increases, then you have more clusters and more concentration around $G_0$. \pause
As $a$ decreases, then you have fewer clusters and the data are more informative. 

\vspace{0.2in} \pause

Rather than setting the concentration parameter, we can learn it. \pause
Let $G\sim DP(\alpha G_0)$ and 
\[ 
\alpha \sim Ga(a,b)
\]
\pause
then the full conditional for $\alpha$ is 
\[ 
\alpha|\ldots \sim Ga\left(a+H-1, b - \sum_{h=1}^{H-1} \log(1-V_h)\right)
\]
\end{frame}



\end{document}

