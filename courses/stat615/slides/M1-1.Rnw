\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Hierarchical models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Normal data model}
\subsection{Normal prior}
\begin{frame}
\frametitle{Normal model with normal prior}

Consider the model 
\[ Y \ind N(\theta,V) \]
with prior 
\[ \theta \sim N(m,C) \]

\vspace{0.2in} \pause

Then the posterior is 
\[ \theta|y \sim N(m',C') \]
\pause where 
\[ \begin{array}{rl}
C' &= 1/(1/C+1/V) \\
m' &= C[m/C + y/V]
\end{array} \]

\end{frame}



\begin{frame}
\frametitle{Normal model with normal prior (cont.)}
For simplicity, let $V=C=1$ and $m=0$, then $\theta|y \sim N(y/2, 1/2)$. \pause 
Suppose $y=1$, then we have

<<normal_model_normal_prior, fig.width=7>>=
y = 1
d = data.frame(x=seq(-2,3,by=.01)) %>%
  mutate(prior = dnorm(x),
         likelihood = dnorm(x,y),
         posterior = dnorm(x,y/2,sqrt(1/2))) %>%
  melt(id.var='x', variable.name='distribution', value.name='density') 

ggplot(d, aes(x,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Normal model with normal prior (cont.)}
Now suppose $y=10$, then we have

<<normal_model_normal_prior2, fig.width=7>>=
y = 10
d = data.frame(theta=seq(-2,12,by=.1)) %>%
  mutate(prior = dnorm(theta),
         likelihood = dnorm(theta,y),
         posterior = dnorm(theta,y/2,sqrt(1/2))) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}


\begin{frame}
\frame{Summary - normal model with normal prior}

\begin{itemize}
\item If the prior and the likelihood agree, then posterior seems reasonable.
\item If the prior and the likelihood disagree, then the posterior is ridiculous.
\item The posterior precision is always the sum of the prior and data precisions and therefore the posterior variance always decreases relative to the prior.
\end{itemize}

\vspace{0.2in} \pause

\alert{Can we construct a prior that allows the posterior to be reasonable always?}

\end{frame}


\subsection{$t$ prior}
\begin{frame}
\frametitle{Normal model with $t$ prior}

Now suppose 
\[ Y \sim N(\theta,V) \]
with 
\[ \theta \sim t_v(m,C), \]
where $E[\theta] = m$ for $v>1$ and $V[\theta] = C \frac{v}{v-2}$ for $v>2$.

\vspace{0.2in} \pause

Now the posterior is 
\[ p(\theta|y) = e^{-(y-\theta)^2/2V} \left( 1+\frac{1}{v}\frac{(\theta-m)^2}{C}\right)^{-(v+1)/2}   \]
which is not a known distribution, but we can normalize via 
\[ p(\theta|y) = \frac{e^{-(y-\theta)^2/2V} \left( 1+\frac{1}{v}\frac{(\theta-m)^2}{C}\right)^{-(v+1)/2} }{\int e^{-(y-\theta)^2/2V} \left( 1+\frac{1}{v}\frac{(\theta-m)^2}{C}\right)^{-(v+1)/2} d\theta} \]
\end{frame}



\begin{frame}
\frametitle{Normal model with $t$ prior (cont.)}
Alternatively, we can calculate the \alert{marginal likelihood}
\[ \begin{array}{rl}
p(y) 
&= \int p(y|\theta)p(\theta) d\theta \\
&= \int N(y;\theta,V) t_v(\theta;m,C) d\theta
\end{array} \]
where 
\begin{itemize}
\item $N(y;\theta,V)$ is the normal density with mean $\theta$ and variance $V$ evaluated at $y$ and
\item $t_v(\theta;m,C)$ is the $t$ distribution with degrees of freedom $v$, location $m$, and scale $C$ evaluated at $\theta$.
\end{itemize}

\pause
and then find the posterior 
\[ p(\theta|y) = N(y;\theta, V) t_v(\theta;m,C) / p(y). \]
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal model with $t$ prior (cont.)}

Since this is a one dimensional integration, we can easily handle it via the {\tt integrate} function in R: \pause

<<normal_model_t_prior, echo=TRUE>>=
# A non-standard t distribution
my_dt = Vectorize(function(x, v=1, m=0, C=1, log=FALSE) {
  logf = dt((x-m)/sqrt(C), v, log=TRUE) - log(sqrt(C))
  return(ifelse(log, logf, exp(logf)))
})

# This is a function to calculate p(y|\theta)p(\theta).
f = Vectorize(function(theta, y=1, V=1, v=1, m=0, C=1, log=FALSE) {
  logf = dnorm(y, theta, sqrt(V), log=TRUE) + my_dt(theta, v, m, C, log=TRUE)
  return(ifelse(log, logf, exp(logf)))
})

# Now we can integrate it
(py = integrate(f, -Inf, Inf))
@
\end{frame}


\begin{frame}
\frametitle{Normal model with $t$ prior (cont.)}
Let $v=1$, $m=0$, $V=C=1$. \pause Suppose $y=1$, then 

<<normal_model_t_prior2>>=
y = 1
d = data.frame(theta=seq(-2,3,by=.01)) %>%
  mutate(prior = my_dt(theta, 1),
         likelihood = dnorm(y,theta),
         posterior = f(theta,y=y)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Normal model with $t$ prior (cont.)}
Let $v=1$, $m=0$, $V=C=1$. \pause Suppose $y=10$, then 

<<normal_model_t_prior3>>=
y = 10
py = integrate(function(x) f(x,y), -Inf, Inf)
d = data.frame(theta=seq(-2,12,by=.1)) %>%
  mutate(prior = my_dt(theta, 1),
         likelihood = dnorm(y,theta),
         posterior = f(theta,y=y)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Shrinkage of MAP as a function of signal}

Let's take a look at the \emph{maximum a posteriori} (MAP) estimates as a function of the signal ($y$) for the normal and $t$ priors. 

\pause

<<normal_model_map>>=
mx = 5
d = adply(seq(-mx,mx,length=101), 1, function(d){
  o = optimize(function(x) f(x,d), c(-mx,mx), maximum=TRUE)
  data.frame(y = d, map_t = o$maximum, mle=d, map_normal=d/2)
})[,-1] %>%
  melt(id.vars='y', variable.name='model', value.name='theta')
ggplot(d, aes(y, theta, color=model)) + 
  geom_line() + 
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Summary - normal model with $t$ prior}

\begin{itemize}
\item A $t$ prior for a normal mean provides a reasonable posterior even if the data and prior disagree. 
\item A $t$ prior provides similar shrinkage to a normal prior when the data and prior agree, but provides little shrinkage when the data and prior disagree.
\item The posterior variance decreases the most when the data and prior agree and decreases less as the data and prior disagree.
\end{itemize}

\vspace{0.2in} \pause

There are many times that we might believe the possibility of $\theta=0$ or, at least, $\theta\approx 0$. \pause In these scenarios, we would like our prior to be able to tell us this. 

\vspace{0.2in} \pause

\alert{Can we construct a prior that allows us to learn about null effects?}

\end{frame}


\subsection{Laplace prior}
\begin{frame}
\frametitle{Laplace distribution}

Let $La(m,b)$ denote a Laplace (or double exponential) distribution with mean $m$, variance $2b^2$, and probability density function
\[ La(x;m,b) = \frac{1}{2b} \exp\left(-\frac{|x-m|}{b}\right). \]

\pause

<<laplace, fig.width=10>>=
source("Laplace.R")
curve(dlaplace, -3, 3, lwd=2)
@

\end{frame}



\begin{frame}
\frametitle{Laplace prior}

Let 
\[ Y \sim N(\theta,V) \]
and 
\[ \theta \sim La(m,b) \]

\vspace{0.2in} \pause

Now the posterior is 
\[ p(\theta|y) \propto N(y;\theta,V) La(\theta;m,b) \propto e^{-(y-\theta)^2/2V} e^{-|\theta-m|/b}. \]
\end{frame}



\begin{frame}
\frametitle{Laplace prior (cont.)}

For simplicity, let $b=V=1$, $m=0$ and suppose we observe $y=1$.

\vspace{0.2in} \pause

<<normal_model_laplace_prior>>=
f = Vectorize(function(theta,y,V,m,b,log=FALSE) {
  logf = dnorm(y,theta,sqrt(V), log=TRUE) + dlaplace(theta, m, b, log=TRUE)
  ifelse(log, logf, exp(logf))
})

y = b = V = 1; m = 0
py = integrate(function(x) f(x, y, V, m, b), -Inf, Inf)

d = data.frame(theta=seq(-2,3,by=.01)) %>%
  mutate(prior = dlaplace(theta, m, b),
         likelihood = dnorm(y, theta),
         posterior = f(theta, y=y, V, m, b)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Laplace prior (cont.)}

For simplicity, let $b=V=1$, $m=0$ and suppose we observe $y=10$.

\vspace{0.2in} \pause

<<normal_model_laplace_prior2>>=
y = 10
py = integrate(function(x) f(x, y, V, m, b), -Inf, Inf)

d = data.frame(theta=seq(-2,y+2,length=101)) %>%
  mutate(prior = dlaplace(theta, m, b),
         likelihood = dnorm(y, theta),
         posterior = f(theta, y=y, V, m, b)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}

\end{frame}


\end{document}
