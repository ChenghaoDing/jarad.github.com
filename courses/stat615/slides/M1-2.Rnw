\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Hierarchical models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

%\section{Normal hierarchical model}
\begin{frame}
\frametitle{Normal hierarchical model}

Let 
\[ Y_{ij} \ind N(\theta_i, \sigma^2) \]
for $i=1,\ldots,\I$, $j=1,\ldots,n_i$, and $\sum_{i=1}^\I n_i = n$. \pause Now consider the following model assumptions:
\begin{itemize}[<+->]
\item $\theta_i\ind N(\mu,\tau^2)$
\item $\theta_i\ind La(\mu,\tau)$
\item $\theta_i\ind t_v(\mu,\tau^2)$
\item $\theta_i\ind \pi \delta_0 + (1- \pi) N(\mu,\tau^2)$
\item $\theta_i\ind \pi \delta_0 + (1- \pi) t_v(\mu,\tau^2)$
\end{itemize}

\vspace{0.2in} \pause

To perform a Bayesian analysis, we need a prior on $\mu$, $\tau^2$, and (possibly) $\pi$. 

\end{frame}


\section{Scale mixtures of normals}
\begin{frame}
\frametitle{Scale mixtures of normals}

Recall that if 
\[ \theta|\phi \sim N(\phi,V) \mbox{ and } \phi \sim N(m,C) \]
\pause
then 
\[ \theta \pause\sim N(m,V+C).  \]
\pause
This is called a location mixture.

\vspace{0.2in} 

Now, if 
\[ \theta|\phi \sim N(m,C\phi) \]
\pause
and we assume a mixing distribution for $\phi$, \pause we have a scale mixture. 
\pause
Since the top level distributional assumption is normal, we refer to this as a \alert{scale mixture of normals}. 

\end{frame}



\subsection{$t$ distribution}
\begin{frame}
\frametitle{$t$ distribution}

Let 
\[ \theta|\phi \sim N(m,\phi C) \mbox{ and } \phi \sim IG(a,b) \]
\pause 
then 
\[ \begin{array}{rl}
p(y) &= \int p(y|\phi)p(\phi) d\phi \pause \\
&= (2\pi\sqrt{C})^{-1/2} \frac{b^a}{\mG(a)} \int \phi^{-1/2} e^{-(y-m)^2/2\phi C} \phi^{-(a+1)} e^{-b/\phi} d\phi \pause\\
&= (2\pi C)^{-1/2} \frac{b^a}{\mG(a)} \int \phi^{-(a+1/2+1)} e^{-[b+(y-m)^2/2C]/\phi} d\phi \pause\\
&= (2\pi C)^{-1/2} \frac{b^a}{\mG(a)} \frac{\mG(a+1/2)}{[b+(y-m)^2/2C]^{a+1/2}} \pause\\
&= \frac{\mG([2a+1]/2)}{\mG(2a/2)\sqrt{2a \pi bC/a}} \left[ 1+\frac{1}{2a}\frac{(y-m)^2}{bC/a} \right]^{-[2a+1]/2} 
\end{array} \]
\pause
Thus 
\[ \theta \sim t_{2a}(m,bC/a) \]
\pause
i.e. $\theta$ has a $t$ distribution with $2a$ degrees of freedom, location $m$, and scale (variance) $bC/a$. 
\end{frame}


\begin{frame}
\frametitle{Laplace distribution}

Let 
\[ \theta|\phi \sim N(m,\phi C) \mbox{ and } \phi \sim Exp(1/2b^2) \]
\pause 
then, by equation (4) in Park and Casella (2008),
\[ \theta \sim La(m,b/\sqrt{C}) \]
\pause
where $z=(y-m)/\sqrt{C}$ and $a=1/b$ \pause and thus $\theta$ has a Laplace distribution with location $m$ and scale $b/\sqrt{C}$. 

\end{frame}

\section{Normal hierarchical model}
\begin{frame}
\frametitle{Normal hierarchical model}

Recall our hierarchical model
\[ Y_{ij} \ind N(\theta_i, \sigma^2) \]
for $i=1,\ldots,\I$ and $j=1,\ldots,n_i$. \pause Now consider the following model assumptions: \pause
\begin{itemize}[<+->]
\item $\theta_i\ind N(\mu,\phi_i), \phi_i = \tau^2 \pause \implies \theta_i \ind N(\mu,\tau^2)$ \pause
\item $\theta_i|\phi_i\ind N(\mu,\phi_i), \phi_i\ind Exp(1/2\tau^2) \pause \implies \theta_i \ind La(\mu,\tau)$ \pause
\item $\theta_i|\phi_i\ind N(\mu,\phi_i), \phi_i\ind IG(v/2,v\tau^2/2) \pause \implies \theta_i \ind t_v(\mu,\tau^2)$
\end{itemize}

\vspace{0.2in} \pause

For simplicity, let's assume $\sigma^2 \sim IG(a,b)$, $\mu \sim N(m,C)$, and $\tau \sim Ca^+(0,c)$ and that $\sigma^2$, $\mu$, and $\tau$ are \emph{a priori} independent.

\end{frame}


\section{MCMC}
\begin{frame}
\frametitle{Gibbs sampling}

The following Gibbs sampler will converge to the posterior $p(\theta,\sigma,\mu,\phi,\tau|y)$:

\begin{enumerate}
\item Sample $\mu\sim p(\mu|\ldots)$.
\item Independently, sample $\theta_i \sim p(\theta_i|\ldots)$.
\item Sample $\sigma \sim p(\sigma|\ldots)$.
\item Independently, sample $\phi_i \sim p(\phi_i|\ldots)$.
\item Sample $\tau \sim p(\tau|\ldots)$.
\end{enumerate}

\vspace{0.2in} \pause

The first three steps will be common to all models while the last two steps will be unique to each model (without a point mass).

\end{frame}


\subsection{$\mu$}
\begin{frame}
\frametitle{Sample $\mu$}

\[ \theta_i \ind N(\mu,\phi_i) \mbox{ and } \mu \sim N(m,C) \]

\vspace{0.2in} \pause

Immediately, we should know that 

\[ \mu|\ldots \sim \pause N(m',C') \]
with 
\[ \begin{array}{rl}
C' &= \left(\frac{1}{C} + \sum_{i=1}^\I \frac{1}{\phi_i} \right)^{-1} \\
m' &= C'\left(\frac{m}{C} + \sum_{i=1}^\I \frac{\theta_i}{\phi_i} \right)
\end{array} \]

\end{frame}


\subsection{$\theta$}
\begin{frame}
\frametitle{Sample $\theta$}

\[ Y_{ij} \ind N(\theta_i,\sigma^2) \mbox{ and } \theta_i \sim N(\mu,\phi_i) \]

\vspace{0.2in} \pause

\[ \begin{array}{rl}
p(\theta|\ldots) 
&\propto \left[\prod_{i=1}^\I \prod_{j=1}^{n_i} e^{-(y_{ij}-\theta_i)^2/2\sigma^2} \right] \left[ \prod_{i=1}^\I e^{-(\theta_i-\mu)^2/2\phi_i} \right] \\
&\propto \prod_{i=1}^\I \left[\prod_{j=1}^{n_i} e^{-(y_{ij}-\theta_i)^2/2\sigma^2} e^{-(\theta_i-\mu)^2/2\phi_i} \right] 
\end{array} \]
\pause
Thus $\theta_i$ are conditionally independent given everything else. \pause It should be obvious that 
\[ \theta_i|\ldots \sim \pause N\left(\left[ \frac{\mu}{\phi_i} + \frac{n_i}{\sigma^2}\overline{y}_i  \right], \left[\frac{1}{\phi_i} + \frac{n_i}{\sigma^2} \right]^{-1} \right) \]
where $\overline{y}_i = \sum_{j=1}^{n_i} y_{ij} / n_i$.
\end{frame}


\subsection{$\sigma$}
\begin{frame}
\frametitle{Sample $\sigma^2$}

\[ Y_{ij} \ind N(\theta_i,\sigma^2) \mbox{ and } \sigma^2 \sim IG(a,b) \]

\vspace{0.2in} \pause

This is just a normal data model with an unknown variance that has the conjugate prior. \pause The only difficulty is that we have several groups here. \pause But very quickly you should be able to determine that 
\[ \sigma^2|\ldots \sim IG(a',b') \]
where 
\[ \begin{array}{rl}
a' &= \sum_{i=1}^\I n_i /2 = n/2\\
b' &= \sum_{i=1}^\I \sum_{j=1}^{n_i} (y_{ij}-\theta_i)^2/2. 
\end{array} \]
\end{frame}

\subsection{Distributional assumption for $\theta_i$}
\begin{frame}
\frametitle{Distributional assumption for $\theta_i$}

\[ Y_{ij} \ind N(\theta_i, \sigma^2) \mbox{ and } \theta_i \ind N(\mu,\phi_i) \]

\[ \begin{array}{rl}
\phi_i &= \tau \\
\phi_i &\sim Exp(1/2\tau^2) \\
\phi_i &\sim IG(v/2,\tau^2/2)
\end{array} \]

The steps that are left are 1) sample $\phi$ and 2) sample $\tau^2$,

\end{frame}


\subsection{$\phi$}
\begin{frame}
\frametitle{Sample $\phi$ for normal model}
For normal model, $\phi_i = \tau$, so we will adress this when we sample $\tau$. 
\end{frame}



\begin{frame}
\frametitle{Sample $\phi$ for Laplace model}
For Laplace model, 
\[ \theta_i \ind N(\mu,\phi_i) \mbox{ and } \phi_i \ind Exp(1/2\tau^2), \]
\pause 
so the full conditional is 
\[ 
p(\phi|\ldots) \propto \left[ \prod_{i=1}^\I N(\theta_i; \mu, \phi_i) Exp(\phi_i;1/2\tau^2) \right].
\]
\pause
So the individual $\phi_i$ are conditionally independent \pause with
\[ 
p(\phi_i|\ldots) \propto N(\theta_i; \mu, \phi_i) Exp(\phi_i;1/2\tau^2) \propto \phi_i^{-1/2} e^{-(\theta_i-\mu)^2/2\phi_i} e^{-\phi_i/2\tau^2}
 \]
\pause
If we perform the transformation $\eta_i = 1/\phi_i$, \pause we have 
\[
p(\eta_i|\ldots) \propto \eta_i^{-3/2} e^{-\frac{(\theta_i-\mu)^2}{2}\eta_i - \frac{1}{2\tau^2\eta_i}}
\] 
\pause
which is the kernel of an inverse Gaussian distribution with mean $\sqrt{1/\tau^2(\theta_i-\mu)^2}$ and scale $1/\tau^2$ where the parameterization is such that the variance is $\mu^3/\lambda$ (different from the {\tt mgcv::rig} parameterization).

\end{frame}

 
 



\begin{frame}
\frametitle{Sample $\phi$ for $t$ model}
For the $t$ model, 
\[ \theta_i \ind N(\mu,\phi_i) \mbox{ and } \phi_i \ind IG(v/2,\tau^2/2), \]
\pause
so we have 
\[ \phi_i|\ldots \ind IG([v+1]/2, [\tau^2+(\theta_i-\mu)^2]/2). \]
Since this is just $\I$ independent normal data models with a known mean and independent conjugate inverse gamma priors on the variance.
\end{frame}




\subsection{$\tau$}
\begin{frame}
\frametitle{Sample $\tau$ for normal model}

Let 
\[ \theta_i \ind N(\mu,\tau^2) \mbox{ and } \tau \sim Ca^+(0,c). \]
\pause
so the full conditional is 
\[ p(\eta|\ldots) \propto \eta^{-\I/2} e^{-\sum_{i=1}^\I (\theta_i-\mu)^2/2\eta} \left( 1+\eta/c^2\right)^{-1} \eta^{-1/2} \]
where we performed the transformation $\eta = \tau^2$ on the prior.

\vspace{0.1in} \pause

Let's use Metropolis-Hastings with proposal distribution 
\[  IG\left(\frac{\I-1}{2}, \sum_{i=1}^\I \frac{(\theta_i-\mu)^2}{2}\right) \]
and acceptance probability $\min\{1,\rho\}$ where
\[ \rho = \frac{\left( 1+\eta^*/c^2\right)^{-1}}{\left( 1+\eta^{(i)}/c^2\right)^{-1}} = \frac{ 1+\eta^{(i)}/c^2}{1+\eta^*/c^2} \]
where $\eta^{(i)}$ and $\eta^*$ are the current and proposed value respective. 

\vspace{0.1in} \pause

Then we calculate $\tau = \sqrt{\eta}$.

\end{frame}



\begin{frame}
\frametitle{Sample $\tau$ for Laplace model}
Let 
\[ \phi_i \sim Exp(1/2\tau^2) \mbox{ and } \tau \sim Ca^+(0,c) \]
so the full conditional is 
\[ p(\eta|\ldots) \propto \eta^{-\I} e^{-\sum_{i=1}^\I \phi_i/2\eta} \left( 1+\eta/c^2\right)^{-1} \eta^{-1/2}. \]

\vspace{0.1in} \pause

Let's use Metropolis-Hastings with proposal distribution 
\[ IG\left(\I-\frac{1}{2}, \sum_{i=1}^\I \frac{\phi_i}{2}\right) \]
and acceptance probability $\min\{1,\rho\}$ where again
\[ \rho = \frac{ 1+\eta^{(i)}/c^2}{1+\eta^*/c^2}.\]

\vspace{0.1in} \pause

Then we calculate $\tau = \sqrt{\eta}$.

\end{frame}




\begin{frame}
\frametitle{Sample $\tau$ for $t$ model}
Let 
\[ \phi_i \sim IG(v/2, \tau^2/2) \mbox{ and } \tau \sim Ca^+(0,c) \]
so the full conditional is 
\[ p(\eta|\ldots) \propto \eta^{\I v/2} e^{-\frac{\eta}{2} \sum_{i=1}^\I \frac{1}{\phi_i}} \left( 1+\eta/c^2\right)^{-1} \eta^{-1/2}. \]

\vspace{0.1in} \pause

Let's use Metropolis-Hastings with proposal distribution 
\[ Ga\left(\frac{\I v + 1}{2}, \frac{1}{2}\sum_{i=1}^\I \frac{1}{\phi_i}\right) \]
and acceptance probability $\min\{1,\rho\}$ where again
\[ \rho = \frac{ 1+\eta^{(i)}/c^2}{1+\eta^*/c^2} .\]

\vspace{0.1in} \pause

Then we calculate $\tau = \sqrt{\eta}$.

\end{frame}



\subsection{Point-mass distributions}



\begin{frame}
\frametitle{Dealing with point-mass distributions}
We would also like to consider models with 
\[ \theta_i \ind \pi \delta_0 + (1-\pi) N(\mu,\phi_i) \]
where $\phi_i = \tau^2$ corresponds to a normal and 
\[ \phi_i \ind IG(v/2,\tau^2/2) \]
corresponds to a $t$ distribution for the non-zero $\theta_i$. 

\vspace{0.2in} \pause

Similar to the previous, the $\theta_i$ are conditionally independent. \pause
To sample $\theta_i$, we calculate 
\[ \begin{array}{rl}
\pi' &= \frac{\pi \prod_{j=1}^{n_i} N(y_{ij}; 0, \sigma^2)}{\pi \prod_{j=1}^{n_i} N(y_{ij}; 0, \sigma^2) + (1-\pi) \prod_{j=1}^{n_i} N(y_{ij}; \mu, \phi_i+\sigma^2) }. \\
\phi_i' &= \left( \frac{1}{\phi_i} + \frac{n_i}{\sigma^2} \right)^{-1} \\
\mu_i' &= \phi_i' \left( \frac{\mu}{\phi_i} + \frac{n_i}{\sigma^2}\overline{y}_i \right)
\end{array} \]
\end{frame}


\begin{frame}
\frametitle{Dealing with point-mass distributions (cont.)}
Let 
\[ \theta_i \ind \pi \delta_0 + (1-\pi) N(\mu,\phi_i) \]
and independently $\pi \sim Beta(s,f)$, $\mu \sim N(m,C)$, and $\phi_i=\tau^2$ for normal model or $\phi_i \ind IG(v/2,\tau^2/2)$ for the $t$ model. 

\vspace{0.2in} \pause

The full conditional for $\pi$ is
\[ \pi|\ldots \sim Beta\left(s+\sum_{i=1}^\I \I(\theta_i=0), f+\sum_{i=1}^\I \I(\theta_i\ne 0)\right)\]
and $\mu$ and $\phi_i$ get updated using only those $\theta_i$ that are non-zero. 

\end{frame}





\end{document}
