\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian model averaging}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Bayesian model averaging}
\begin{frame}
\frametitle{Bayesian model averaging}

Let $\{M_\gamma: \gamma \in \mG\}$ indicate a set of models for a particular data set $y$. \pause If $\Delta$ is a quantity of interest, e.g. effect size, a future observable, or the utility of a course of action, then its posterior distribution is 
\[ 
p(\Delta|y) = \sum_{\gamma\in\mG} p(\Delta|M_\gamma, y) p(M_\gamma|y)  
\]
where 
\[ 
p(M_\gamma|y) = \frac{p(y|M_\gamma)p(M_\gamma)}{\sum_{\lambda\in \mG} p(M_\lambda)}
\]
and 
\[
p(y|M_\gamma) = \int p(y|\theta_\gamma,M_\gamma) p(\theta_\gamma|M_\gamma) d\theta_\gamma
\]
where $\theta_\gamma$ is the set of parameters in model $M_\gamma$. 

\end{frame}


\begin{frame}
\frametitle{Bayesian model averaged moments}

Since $p(\Delta|y)$ is a discrete mixture, we may be interested in simplifying inference concerning $\Delta$ to a couple of moments. 
\pause
Let $\hat{\Delta}_\gamma = E[\Delta|y,M_\gamma]$. 
\pause 
Then the expectation is 
\[ 
E[\Delta|y] = \sum_{\gamma\in\mG} \hat{\Delta}_\gamma p(M_\gamma|y) 
\]
\pause
and the variance is 
\[ 
V[\Delta|y] = \left[ \sum_{\gamma\in\mG} (Var[\Delta|y,M_\gamma) + \hat{\Delta}_\gamma^2) p(M_\gamma|y) \right] - E[\Delta|y]^2
\]
\pause
The appealing aspect here is that the moments only depend on the moments from each individual model.

\end{frame}


\begin{frame}
\frametitle{Difficulties with BMA}

\begin{itemize}
\item Evaluating the summation since the cardinality of $\mG$ might be huge.
\item Calculating the marginal likelihood.
\item Specifying the prior over models.
\item Choosing the class of models to average over.
\end{itemize}
\end{frame}



\subsection{Reducing cardinality}
\begin{frame}
\frametitle{Reducing cardinality}

Rather than summing over $\mG$, we can only include those models whose posterior probability is sufficiently large 
\[
\mathcal{A} = \left\{ M_\gamma: \frac{\max_\lambda p(M_\lambda|y)}{p(M_\gamma|y)} \le C\right\}
\]
\pause
relative to other models where $C$ is chosen by the researcher. \pause Also, appealing to Occam's razor, we should exclude complex models which receive less support than sub-models of that complex model\pause, i.e. 
\[ 
\mathcal{B} = \left\{ M_\gamma: \forall M_\lambda \in \mathcal{A}, M_\lambda \subset M_\gamma, \frac{p(M_\lambda|y)}{p(M_\gamma|y)} < 1\right\}
\]
\pause 
So, we typically sum over the smaller set of models $\mG' = \mathcal{A}\setminus \mathcal{B}$. 
\end{frame}



\begin{frame}
\frametitle{Searching through models}

One approach is to search through models and keep a list of the best models. \pause To speed up the search the following criteria can be used to decide what models should be kept in $\mG'$: \pause

\begin{itemize}
\item When comparing two nested models, if a simpler model is rejected, then all submodels of the simpler model are rejected. \pause
\item When comparing two non-nested models, we calculate the ratio of posterior model probabilities 
\[ \frac{p(M_\gamma|y)}{p(M_{\gamma'}|y)} \]
if this quantity is less than $O_L$, we reject $M_\gamma$ and if it is greater than $O_R$ we reject $M_{\gamma'}$. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Using MCMC to search through models}

Construct a neighborhood around $M_\gamma$, call it $nbh(M_\gamma)$. \pause Now construct a transition matrix $q$ such that 
\[ 
q(M^*|M^{(i)}) = \left\{ \begin{array}{cl}
0 & \forall M^* \notin nbh(M^{(i)}) \\
\frac{1}{|nbh(M^{(i)})|} & \forall M^* \in nbh(M^{(i)})
\end{array}\right.
\]
\pause
Set $M^{(i+1)} = M^*$ with probability $\min\{1,\rho(M^{(i)},M^*)\}$ where 
\[ 
\rho(M^{(i)},M^*) = \frac{p(M^*|y)}{p(M^{(i)}|y)} \frac{|nbh(M^{(i)})|}{|nbh(M^*)|}
\]
and otherwise set $M^{(i+1)} = M^{(i)}$. \pause This Markov chain converges to draws from $p(M_\gamma|y)$ and therefore can estimate posterior model probabilities. 

\end{frame}


\subsection{Evaluating integrals}
\begin{frame}
\frametitle{Evaluating the marginal likelihoods}

\small

Recall that as the sample size $n$ increases, the posterior converges to a normal distribution. \pause Let 
\[ 
g(\theta) = \log(p(y|\theta,M)+p(\theta|M))
\]
Let $\hat{\theta}_{MAP}$ be the MAP for $\theta$ in model $M_\gamma$. \pause Taking a Taylor series expansion of $g(\theta)$ around $\hat{\theta}_{MAP}$, we have 
\[ 
g(\theta) \approx g(\hat{\theta}_{MAP}) - \frac{1}{2}(\theta-\hat{\theta}_{MAP}) A (\theta-\hat{\theta}_{MAP})^\top
\]
where $A$ is the negative Hession of $g(\theta)$ evaluated at $\hat{\theta}_{MAP}$. \pause Combining this with the first equation and exponentiating, we have 
\[ 
p(y|\theta,M)p(\theta|M) \approx p(y|\hat{\theta}_{MAP},M)p(\hat{\theta}_{MAP}) \exp\left( - \frac{1}{2}(\theta-\hat{\theta}_{MAP}) A (\theta-\hat{\theta}_{MAP})^\top  \right)
\]
Hence, the approximation to $p(\theta|y,M) \propto p(y|\theta,M)p(\theta|M)$ is normal. 

\end{frame}



\begin{frame}
\frametitle{Evaluating the marginal likelihoods (cont.)}

\small

If we take the integral of both sides and take the logarithm, we have 
\[ 
\log p(y|M) \approx \log p(y|\hat{\theta}_{MAP},M) + \log p(\hat{\theta}_{MAP}|M) + \frac{p}{2} \log(2\pi) - \frac{1}{2} \log|A|
\]
where $p$ is the dimension of $\theta$, i.e. the number of parameters. \pause We call this approximation the \alert{Laplace approximation}. 

\vspace{0.2in} \pause

Another approximation that is more computationally efficient but less accurate is to only retain terms that increase with $n$: \pause
\begin{itemize}
\item $\log p(y|\hat{\theta},M)$ increases linearly with $n$ \pause
\item $\log |A|$ increases as $d\log n$ \pause
\end{itemize}
As $n$ gets large $\hat{\theta}_{MAP} \to \hat{\theta}_{MLE}$. \pause Taking these two together we have 
\[ 
\log p(y|M) \approx \log p(y|\hat{\theta}_{MLE},M) - \frac{p}{2} \log n
\]
\pause
Multiplying by -2, we obtain Schwarz's Bayesian Information Criterion (BIC)
\[
BIC = -2 \log p(y|\hat{\theta}_{MLE},M) + p \log n
\]


\end{frame}




\subsection{Bayesian regression}
\begin{frame}
\frametitle{Bayesian regression}

Consider the model $M_\gamma$:
\[ y = X_\gamma\beta_\gamma + \epsilon \]
with 
\[ \epsilon \sim N(0,\sigma^2 \I) \]
where 
\begin{itemize}
\item $y$ is a vector of length $n$
\item $\gamma$ is a binary vector indicating which explanatory variables are included in model $M_\gamma$
\item $\beta_\gamma$ is an unknown vector of length $p_\gamma$
\item $X_\gamma$ is a known $n\times p_\gamma$ design matrix including those columns indicated by $\gamma$
\item $\sigma^2$ is an unknown scalar
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{BMA in regression}



\end{frame}



\end{document}
