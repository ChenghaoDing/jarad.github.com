\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}
\usepackage{animate}
\usepackage{verbatim}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Hamiltonian Monte Carlo}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE,
               cache=TRUE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(animation)
library(plyr)
library(ggplot2)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle
Adapted from Radford Neal's MCMC Using Hamltonian Dynamics in Handbook of Markov Chain Monte Carlo (2011).}

\section{Hamiltonian dynamics}
\begin{frame}
\frametitle{Hamiltonian system}

Considering a body in a frictionless 1-dimensional environment, let 
\begin{itemize}
\item $m$ be its mass,
\item $q$ be its position, and
\item $p$ be its momentum.
\end{itemize}
\pause
The mass has 
\begin{itemize}
\item potential energy $U(q)$ (which is proportional to its height) and
\item kinetic energy $K(p)=p^2/(2m)$. 
\end{itemize}
\end{frame}


\subsection{Hamilton's equations}
\begin{frame}
\frametitle{Hamilton's equations}

Extending this to $d$ dimensions, 
we have 
\begin{itemize}
\item position vector $q$ and 
\item momentum vector $p$. 
\end{itemize}
\pause
The Hamiltonian $H(q,p)$ describes the time evolution of the system through 
\[ \begin{array}{rl}
\frac{dq_i}{dt} &= \phantom{-}\frac{\partial H}{\partial p_i} \\
\frac{dp_i}{dt} &= -\frac{\partial H}{\partial q_i}
\end{array} \]
for $i=1,\ldots,d$. 
\end{frame}


\begin{frame}
\frametitle{Potential and kinetic energy}

\small

For Hamiltonian Monte Carlo, 
we usually use Hamiltonian functions that can be written as follows:
\[ 
H(q,p) = U(q) + K(p)
\]
where 
\begin{itemize}
\item $U(q)$ is called the potential energy and will be defined to be minus the log probability density of the distribution for $q$ (plus any constant that is 
convenient) and 
\item $K(p)$ is called the kinetic energy and is usually defined as 
\[ 
K(p) = p^\top M^{-1} p / 2
\]
where $M$ is a symmetric, positive-definite ``mass matrix'', 
\pause which is 
typically diagonal, and is often a scalar multiple of the identity matrix.
This form for $K(p)$ corresponds to minus the log probability density 
(plus a constant) of the zero-mean Gaussian distribution with covariance
matrix $M$.
\end{itemize}

\pause

The resulting Hamilton's equations are 
\[ 
\frac{dq_i}{dt} = [M^{-1}p]_i, \qquad
\frac{dp_i}{dt} = -\frac{\partial U}{\partial q_i}.
\]
\end{frame}



\subsection{One-dimensional example}
\begin{frame}
\frametitle{One-dimensional example}

Suppose 
\[ 
H(q,p) = U(q) + K(p), \quad U(q) = q^2/2, \quad K(p) = p^2/2
\]
\pause
The dynamics resulting from this Hamiltonian are
\[ 
\frac{dq}{dt} = p, \quad \frac{dp}{dt}=-q.
\]
\pause
Solutions of the form
\[ 
q(t) = r\, cos(a+t),\quad p(t) = -r\, sin(a+t)
\]
for some constants $r$ and $a$.
\end{frame}


\begin{frame}[fragile]
\frametitle{One-dimensional example simulation}

<<oned_sim, fig.show='animate', interval=.1>>=
tt <- seq(0,2*pi, length=21)
qt <- cos(tt); pt = -sin(tt)

draw_step = function(i, e=.01) {
  opar = par(mfrow=c(1,2))
  curve(x^2/2, -1, 1, xlab='Position', ylab='Potential Energy')
  points(qt[i], qt[i]^2/2+e, pch=19)
  curve(x^2/2, -1, 1, xlab='Momentum', ylab='Kinetic Energy')
  points(pt[i], pt[i]^2/2+e, pch=19)
  par(opar)
}

# 
for (i in 1:length(tt)) {
  draw_step(i,0)
}
@
\end{frame}


\subsection{Reversibility}
\begin{frame}
Hamiltonian dynamics is reversible, 
i.e. the mapping $T_s$ from the state at time $t$, $(q(t),p(t))$, to the state 
at time $t+s$, $(q(t+s),p(t+s))$, is one-to-one, and hence as an inverse,
$T_{-s}$.
\pause
Under our usual assumptions for HMC, the inverse mapping can be obtained by
negative $p$, applying $T_s$, and then negating $p$ again.
\pause
The reversibility of Hamiltonian dynamics is important for showing convergence
of HMC.
\end{frame}


\begin{frame}
\frametitle{Conservation of the Hamiltonian}

The dynamics conserve the Hamiltonian since
\[ \begin{array}{rl}
\frac{dH}{dt} &= 
\sum_{i=1}^d \left[ \frac{dq_i}{dt}\frac{\partial H}{\partial q_i}
+ \frac{dp_i}{dt} \frac{\partial H}{\partial p_i}\right] \\
&= 
\sum_{i=1}^d \left[ \frac{\partial H}{\partial p_i}\frac{\partial H}{\partial q_i}
-\frac{\partial H}{\partial q_i}\frac{\partial H}{\partial p_i} \right]
\end{array} \]

\pause
If $h$ is conserved, then the acceptance probability based on Hamiltonian 
dynamics is 1. 
\pause
In practice, we can oly make $H$ approximately invariant.

\end{frame}



\begin{frame}[fragile]
\frametitle{Conservation of the Hamiltonian}
<<oned_volume, fig.show='animate', interval=.1, fig.asp=1, fig.width=5, out.width='.6\\linewidth'>>=
t3 <- seq(0,2*pi, length=1000)
d <- data.frame(x = cos(t3), y=-sin(t3))
for (i in 1:length(tt)) {
  plot(y~x, d, type='l', col='gray', xlab='Location', ylab='Momentum')
  points(qt[i], pt[i], pch=19)
}
@
\end{frame}



\begin{frame}
\frametitle{Volume preservation}

If we apply the mapping $T_s$ to point in some region $R$ of $(q,p)$ space with
volume $V$, the image of $R$ under $T_s$ will also have volume $V$. 
\pause 
This feature simplifies calculation of the acceptance probability for 
Metropolis updates.

\end{frame}


\subsection{Euler's method}
\begin{frame}
\frametitle{Euler's method}

For simplicity, assume 
\[ 
H(q,p) = U(q) + K(p), \qquad K(p) = \sum_{i=1}^d \frac{p_i^2}{2m_i}.
\]
\pause
One way to simulate Hamiltonian dynamics is to discretize time into increments
of $e$, \pause i.e.
\[ \begin{array}{rll}
p_i(t+e) 
&= p_i(t) + e \frac{dp_i}{dt}(t) 
&= p_i(t) - e \frac{\partial U}{\partial q_i}(q(t)) \\
q_i(t+e) 
&= q_i(t) + e\frac{dq_i}{dt}(t) 
&= q_i(t) + e\frac{p_i(t)}{m_i}
\end{array}\]

\end{frame}



\begin{frame}
\frametitle{Leapfrog method}

An improved approach is the \alert{leapfrog} method which has the following 
updates:
\[ \begin{array}{rl}
p_i(t+e/2) 
&=p_i(t) - (e/2)\frac{\partial U}{\partial q_i}(q(t)) \pause \\
q_i(t+e) &= q_i(t) + e \frac{p_i(t+e/2)}{m_i} \pause \\
p_i(t+e) &= p_i(t+e/2)- (e/2)\frac{\partial U}{\partial q_i}(q(t+e))
\end{array} \]
\pause
The leapfrog method is reversible and preserves volume exactly.
\end{frame}




\begin{frame}[fragile]
\frametitle{Leap-frog simulator}
<<echo=TRUE>>=
leap_frog = function(U, grad_U, e, L, theta, omega) {
  omega = omega - e/2 * grad_U(theta) 
    
    for (l in 1:L) {
      theta = theta + e * omega
      if (l<L) omega = omega - e * grad_U(theta)
    }
    omega = omega - e/2 * grad_U(theta)
  return(list(theta=theta,omega=omega))
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Leap-frog simulator}
<<leapfrog-animation, fig.show='animate', interval=.1>>=
  # Create the data
  set.seed(20150915)
n_steps = 63
theta = rep(2.5, n_steps)
omega = rep(0, n_steps)
for (i in 2:n_steps) {
  tmp = leap_frog(function(x) x^2/2, function(x) x, .1, 1, theta[i-1], omega[i-1])
  theta[i] = tmp$theta
  omega[i] = tmp$omega
}

draw_step = function(i, e=.01) {
  opar = par(mfrow=c(1,2))
  curve(x^2/2, -3, 3, main='Position', ylab='Potential Energy')
  points(theta[i], theta[i]^2/2+e, pch=19)
  curve(x^2/2, -3, 3, main='Momentum', ylab='Kinetic Energy')
  points(omega[i], omega[i]^2/2+e, pch=19)
  par(opar)
}

# 
for (i in 1:n_steps) {
  draw_step(i)
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Conservation of the Hamiltonian}

\vspace{-0.5in}

<<leapfrog-volume, fig.show='animate', interval=.1, fig.asp=1, fig.width=5, out.width='.6\\linewidth'>>=
r = sqrt(theta[1]^2 + omega[1]^2)
n = 1001
d = data.frame(x = c(seq(-r,r,length=n), seq(r, -r, length=n)))
d$y = sqrt(r^2-d$x^2)
d$y[1:n] = -d$y[1:n]
for (i in 1:length(theta)) {
  plot(y~x, d, type='l', col='gray', xlab='Location', ylab='Momentum')
  points(theta[i], omega[i], pch=19)
}
@
\end{frame}



% \begin{frame}
% \frametitle{Parameter augmentation}
% 
% Suppose we are interested in sampling from a posterior distribution for $\theta\in \mathbb{R}^d$
% \[ 
% p(\theta|y) \propto p(y|\theta)p(\theta) . 
% \]
% 
% \pause
% Now augment $\theta$ with moment variable $\omega\sim N_d(0,D)$ independent of $\theta|y$ such that 
% \[ 
% p(\theta|y) 
% = \int p(\theta|\omega,y)p(\omega)d\omega \pause 
% = \int p(\theta|y) p(\omega) d\omega
% \]
% \pause
% To compare with Neal (2010), we have $q=\theta$, $p=\omega$, 
% \[ 
% U(\theta) = -\log[p(y|\theta)p(\theta)] = -\log p(y|\theta) -\log p(\theta),
% \]
% \pause
% and 
% \[
% K(\omega) = -\log p(\omega).
% \]
% 
% \end{frame}

\section{Hamiltonian Monte Carlo}
\begin{frame}
\frametitle{Probability distributions}

The Hamiltonian is an energy function for the joint state of ``position'', $q$,
and ``momentum'', $p$, and so defines a joint distribution for them, via
\[
P(q,p) = \frac{1}{Z} \exp\left(-H(q,p)\right)
\]
where $Z$ is the normalizing constant.
\pause

If $H(q,p) = U(q)+K(p)$, the joint density is 
\[
P(q,p) = \frac{1}{Z} \exp\left(-U(q)\right)\exp\left(-K(p)\right).
\]
\pause

If we are interested in a posterior distribution, we set $q=\theta$ and 
\[
U(\theta) = -\log\left[p(y|\theta)p(\theta)\right].
\]

\end{frame}




\begin{frame}
\frametitle{Hamiltonian Monte Carlo algorithm}

\small

Set tuning parameters 
\begin{itemize}
\item $L$: the number of steps
\item $e$: stepsize
\item $D=\{d_i\}$: covariance matrix for $\omega$
\end{itemize}
\pause

Let $\theta^{(i)}$ be the current value of the parameter $\theta$. 
\pause 
The leap-frog Hamiltonian Monte Carlo algorithm is 
\begin{enumerate}
\item Sample $\omega \sim N_d(0,D)$. \pause
\item Simulate Hamiltonian dynamics on location $\theta^{(i)}$ and momentum  $\omega$ via the leapfrog method (or any reversible method that preserves volume)
for $L$ steps with stepsize $e$. \pause 
Call these updated values $\theta^*$ and $-\omega^*$. \pause
\item Set $\theta^{(i+1)} = \theta^*$ with probability $\min\{1,\rho(\theta^{(i)},\theta^*)\}$ 
\pause
where 
\[ 
\rho(\theta^{(i)},\theta^*) = 
\frac{p(\theta^*|y)}{p(\theta^{(i)}|y)}
\frac{p(\omega^*)}{p(\omega^{(i)})} \pause = 
\frac{p(y|\theta^*)p(\theta^*)}{p(y|\theta^{(i)})p(\theta^{(i)})} 
\frac{N_d(\omega^*;0,D)}{N_d(\omega^{(i)};0,D)} 
\]
\pause
otherwise set $\theta^{(i+1)} = \theta^{(i)}$.
\end{enumerate}
\end{frame}



% \begin{frame}
% \frametitle{Leap-frog simulation of Hamiltonian dynamics}
% Given a current location $\theta(0)$ and momentum $\omega(0)$ at time $0$, the leap-frog method can be used to approximate simulating Hamiltonian dynamics up to time $Le$ using a series of $L$ steps each of time $e$. 
% 
% \vspace{0.2in} 
% 
% \pause The algorithm is 
% \begin{enumerate}
% \item For $\ell = 1,\ldots,L$, 
% \begin{enumerate}
% \item For $i=1,\ldots, d$, $\omega_i\left(\left[\ell-\frac{1}{2}\right]e\right) = \omega_i([\ell-1]e)-\frac{e}{2}\frac{\partial U}{\partial \theta_i} (\theta([\ell-1]e))$ \pause
% \item For $i=1,\ldots, d$, $\theta_i(\ell e) = \theta_i([\ell-1]e) + e \frac{\omega_i\left(\left[\ell-\frac{1}{2}\right]e\right)}{d_i}$ \pause
% \item For $i=1,\ldots, d$, $\omega_i(\ell e) = \omega_i\left(\left[\ell-\frac{1}{2}\right]e\right)-\frac{e}{2}\frac{\partial U}{\partial \theta_i} (\theta(\ell e))$ 
% \end{enumerate}
% \end{enumerate}
% \pause
% where $\theta_i$ and $\omega_i$ are the $i^{th}$ element of the location and momentum, respectively. 
% 
% \end{frame}




\begin{frame}
\frametitle{Reversibility}

A reversible simulation means that
\begin{itemize}
\item if you simulate from $(\theta,\omega)$ to $(\theta',\omega')$ for some step size $e$ and number of steps $L$ \pause then
\item if you simulate from $(\theta',\omega')$ for the same $e$ and $L$, you will end up at $(\theta,\omega)$.
\end{itemize}
\pause
If we use $q$ to denote our simulation ``density'', then reversibility means
\[
q(\theta',\omega'|\theta,\omega) = q(\theta,\omega|\theta',\omega')
\]
\pause
and thus in the Metropolis-Hastings calculation, the proposal is symmetric.
\pause
In order to ensure reversibility of our proposal, we need to negate momentum after we complete the leap-frog simulation,
but so long as $p(\omega) = p(-\omega)$ this will not affect our acceptance probability.

\end{frame}






\begin{frame}
\frametitle{Volume preserving results in perfect acceptance}

Recall that we accept with probability $\min\{1,\rho(\theta^{(i)},\theta^*)\}$ 
\pause
where 
\[ 
\rho(\theta^{(i)},\theta^*) = 
\frac{p(\theta^*|y)}{p(\theta^{(i)}|y)} 
\frac{p(\omega^*)}{p(\omega^{(i)})} 
\]
\pause
Volume is preserved if 
\[ 
p(\theta^{(i)}|y)p(\omega^{(i)}) 
= p(\theta^*|y)p(\omega^*) \pause 
\implies \frac{p(\theta^*|y)}{p(\theta^{(i)}|y)} \frac{p(\omega^*)}{p(\omega^{(i)})} = 1
\]
\pause 
This will only be the case if the simulation is perfect! 
\pause 
But we have discretization error. 
\pause
The acceptance probability accounts for this error. 
\end{frame}





\begin{frame}[fragile]
<<echo=TRUE>>=
HMC_neal = function(U, grad_U, epsilon, L, current_q) {
  q = current_q
  p = rnorm(length(q),0,1)
  current_p = p
  
  p = p-epsilon*grad_U(q)/2
  
  for (i in 1:L) {
    q = q+epsilon*p
    if (i!=L) p = p -epsilon * grad_U(q)
  }
  p = p-epsilon * grad_U(q)/2
  
  p = -p
  
  current_U = U(current_q)
  current_K = sum(current_p^2)/2
  proposed_U = U(q)
  proposed_K = sum(p^2)/2
  
  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
  {
    return(q)
  }
  else {
    return(current_q)
  }
}
@
\end{frame}


\begin{frame}[fragile]
<<echo=TRUE>>=
HMC = function(n_reps, log_density, grad_log_density, tuning, initial) {
  theta = rep(0, n_reps)
  theta[1] = initial$theta
  
  for (i in 2:n_reps) theta[i] = HMC_neal(U = function(x) -log_density(x), 
                                          grad_U = function(x) -grad_log_density(x),
                                          e = tuning$e, 
                                          L = tuning$L, 
                                          theta[i-1])
  theta
}
@
\end{frame}

\begin{frame}[fragile]
<<echo=TRUE>>=
theta = HMC(1e4, function(x) -x^2/2, function(x) -x, list(e=1,L=1), list(theta=0))
hist(theta, freq=F, 100)
curve(dnorm, add=TRUE, col='red', lwd=2)
@
\end{frame}



\begin{frame}
\frametitle{Tuning parameters}
There are three tuning parameters:
\begin{itemize}
\item $e$: step size
\item $L$: number of steps
\item $D$: covariance matrix for momentum
\end{itemize}

\vspace{0.2in} \pause

Let $\mySigma=V(\theta|y)$, then an optimal normal distribution for $\omega$ is $N(0,\mySigma^{-1})$. 
\pause
Typically, we do not know $\mySigma$, but we can estimate it using posterior samples. 
\pause
We can update this estimate throughout burn-in (or warm-up). 
\end{frame}




\begin{frame}[fragile]
\frametitle{Effect of $e$ and $L$}
<<tuning, cache=TRUE, echo=TRUE>>=
n_reps = 1e4
d = expand.grid(e=10^seq(-3,0,by=1), L=10^seq(0,2))
r = ddply(d, .(e,L), function(xx) {
  data.frame(
    iteration = 1:n_reps,
    theta = HMC(n_reps, function(x) -x^2/2, function(x) -x, list(e=xx$e,L=xx$L), list(theta=0)))
})
@
\end{frame}

\begin{frame}
<<dependson='tuning'>>=
ggplot(r, aes(iteration,theta)) + 
  geom_line() + 
  facet_grid(L~e) +
  theme_bw()
@
\end{frame}

\begin{frame}
<<dependson='tuning'>>=
ggplot(r, aes(theta)) + 
  geom_histogram(aes(y=..density..), binwidth=.1) + 
  facet_grid(L~e, scales='free_y') + 
  stat_function(fun = dnorm, colour = "red") +
  theme_bw()
@
\end{frame}

\begin{frame}
<<e, dependson='tuning'>>=
s = ddply(r, .(e,L), summarize, 
          acceptance_rate = length(unique(theta))/length(theta),
          autocorrelation = as.numeric(acf(theta, lag.max=1, plot=FALSE)[1]$acf))
m = melt(s, id.var=c('e','L'), variable.name='statistic')
ggplot(m, aes(e, value, color=L, shape=as.factor(L))) + 
  geom_point() + 
  facet_wrap(~statistic) +
  scale_x_log10() +
  theme_bw()
@
\end{frame}

\begin{frame}
\frametitle{Random-walk vs HMC}

\url{https://www.youtube.com/watch?v=Vv3f0QNWvWQ}

\end{frame}


\section{Summary}
\begin{frame}
\frametitle{Summary}

Hamiltonian Monte Carlo (HMC) is a Metropolis-Hastings method using parameter augmentation and a sophisticated proposal distribution based on Hamiltonian dynamics such that 
\begin{itemize}
\item the acceptance probability can be kept near 1 \pause
\item while still efficiently exploring the posterior. 
\end{itemize}
\pause
HMC still requires us to set tuning parameters 
\begin{itemize}
\item $e$: step size
\item $L$: number of steps
\item $D$: covariance matrix for momentum
\end{itemize}
\pause
and can only be run in models with continuous parameters in $\mathbb{R}^d$ (or transformed to $\mathbb{R}^d$).

\end{frame}

\end{document}
