\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../frontmatter}
\input{../commands}

\title{Set 09 - Bayesian statistics}

% \newenvironment{remark}[1][Remark]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(dplyr)
library(tidyr)
library(ggplot2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Bayesian parameter estimation
  \begin{itemize}
  \item Condition on what is known
  \item Describe uncertainty using probability
  \item Binomial example
  \end{itemize}
\end{itemize}
\end{frame}



\section{Bayesian statistics}
\begin{frame}
\frametitle{A Bayesian statistician}

Let 
\begin{itemize}[<+->]
\item $y$ be the data we will collect from an experiment, 
\item $K$ be everything we know for certain about the world (aside from $y$), and
\item $\theta$ be anything we don't know for certain.
\end{itemize}

\vspace{0.2in} \pause

My definition of a Bayesian statistician is an individual who makes decisions based on the probability distribution of those things we don't know conditional on what we know, \pause i.e. 
\[ p(\theta|y, K). \]
\pause
Typically, the $K$ is dropped from the notation.
\end{frame}


\begin{frame}
\frametitle{Bayes' Rule}

Bayes' Rule applied to a partition $P=\{A_1,A_2,\ldots\}$, 
\[ P(A_i|B) = \frac{P(B|A_i)P(A_i)}{P(B)} = \frac{P(B|A_i)P(A_i)}{\sum_{i=1}^\infty P(B|A_i)P(A_i)} \]

\vspace{0.2in} \pause

Bayes' Rule also applies to probability density (or mass) functions, e.g. 
\[ p(\theta|y) =\frac{p(y|\theta)p(\theta)}{p(y)} = \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}  \]
where the integral plays the role of the sum in the previous statement.
\end{frame}



\subsection{Parameter estimation}
\begin{frame}
\frametitle{Parameter estimation}
Let $y$ be data from some model with unknown parameter $\theta$. \pause Then
\[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}= \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta} \]
\pause and we use the following terminology 
\begin{center}
\begin{tabular}{ll}
Terminology & Notation \\
\hline 
Posterior & $p(\theta|y)$ \\
Prior & $p(\theta)$ \\
Model & $p(y|\theta)$ \\
Prior predictive distribution & $p(y)$ \\
(marginal likelihood) & \\
\hline
\end{tabular}
\end{center}

\vspace{0.1in} \pause

If $\theta$ is discrete (continuous), 

\hspace{0.2in} then $p(\theta)$ and $p(\theta|y)$ are probability mass (density) functions.

If $y$ is discrete (continuous), 

\hspace{0.2in}  then $p(y|\theta)$ and $p(y)$ are probability mass (density) functions.
\end{frame}


\begin{frame}
\frametitle{Binomial model}

Suppose $Y\sim Bin(n,\theta)$, then 
\[ 
p(y|\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}.
\]
A reasonable default prior is the uniform distribution on the interval $(0,1)$
\pause 
\[ 
p(\theta) = \I(0<\theta<1).
\]
\pause
Using Bayes Rule, you can find 
\[
\theta|y \sim Be(1+y, 1+n-y).
\]
\end{frame}


\begin{frame}
\frametitle{Beta distribution}

\begin{definition}
The \alert{beta distribution} defines a distribution for a probability, 
i.e. a number on the interval (0,1). 
The probability density function is 
\[ 
p(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{Beta(a,b)}\I(0<\theta<1)
\]
where $a,b>0$ and $Beta$ is the beta function, \pause 
i.e. 
\[ 
Beta(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} 
\quad\mbox{and} \quad
\Gamma(a) = \int_0^\infty x^{a-1}e^{-x} dx.
\]
\pause
The beta distribution has the following properties:
\begin{itemize}
\item $E[\theta] = \frac{a}{a+b}$
\item $Var[\theta] = \frac{ab}{(a+b)^2(a+b+1)}$.
\end{itemize}
\end{definition}
\end{frame}


\begin{frame}
\frametitle{}

<<>>=
my_dbeta <- function(x) {
  data.frame(theta = seq(0, 1, length=101)) %>%
    mutate(density = dbeta(theta, x$a, x$b))
}

d <- expand.grid(a = c(.5,1,2, 10),
            b = c(.5,1,2, 10)) %>%
  group_by(a,b) %>%
  do(my_dbeta(.)) %>%
  ungroup() %>%
  mutate(a = factor(paste0("a = ", a), levels=paste0("a = ", c(.5,1,2,10))),
         b = factor(paste0("b = ", b), levels=paste0("b = ", c(.5,1,2,10))))

ggplot(d, aes(x=theta, y=density)) +
  geom_line() +
  facet_grid(a~b) +
  theme_bw() +
  ylim(0,5)
@

\end{frame}


\begin{frame}
\frametitle{Beta posterior}

Suppose we have made 100 sensors according to a particular protocol and 2 have 
a sensitivity below a pre-determined threshold. 
\pause
Let $Y$ be the number below the threshold.
Assume $Y\sim Bin(n,\theta)$ with $n=100$ \pause and 
$\theta \sim Unif(0,1)\stackrel{d}{=} Be(1,1)$, 
\pause 
then 
\[ 
\theta|y \sim Be(1+y,1+n-y) \stackrel{d}{=} Be(3,99).
\]
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior density}
<<echo=TRUE, fig.height=3.5>>=
n <- 100
y <- 2
curve(dbeta(x, 1+y, 1+n-y), 0, 0.1,
      main = "Posterior density",
      xlab = expression(theta),
      ylab = expression(paste("p(",theta,"|y)")))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Posterior expectation}

Often times it is inconvenient to provide a full posterior and so we often 
summarize using a point estimate from the posterior. 
\pause
For a point estimate, we can use the posterior expectation:
\pause
\[
\hat{\theta}_{Bayes} = E[\theta|y] \pause 
= \frac{1+y}{(1+y)+(1+n-y)} \pause
= \frac{1+y}{1+n}
\]

<<echo=TRUE>>=
(1+y)/(2+n)
@
\pause
Not that this is close, but not exactly equal to $\hat{\theta}_{MLE} = y/n$. 
\pause
Since the MLE is unbiased, this posterior expectation will generally be biased
but it is still consistent since $\hat{\theta}_{Bayes} \to \hat{\theta}_{MLE}$. 
\end{frame}




\begin{frame}[fragile]
\frametitle{Credible intervals}

\begin{definition}
A \alert{$100(1-\alpha)$\% credible interval} is any interval $(L,U)$ such that

\[ 
1-\alpha = \int_L^U p(\theta|y) d\theta.
\]
\pause
An \alert{equal-tail $100(1-\alpha)$\% credible interval} is the interval 
$L,U)$ such that 
\[ 
\alpha/2 = \int_{-\infty}^L p(\theta|y) d\theta = \int_U^\infty p(\theta|y) d\theta.
\]
\end{definition}
\pause

<<>>=
# 95% credible interval is 
ci = qbeta(c(.025,.975), 1+y, 1+n-y) 
round(ci, 3)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Equal-tail 95\% credible interval}
<<echo=FALSE>>=
curve(dbeta(x, 1+y, 1+n-y), 0, 0.1,
      main = "Posterior density",
      xlab = expression(theta),
      ylab = expression(paste("p(",theta,"|y)")))

ci_area <- data.frame(x = seq(ci[1],ci[2],length=101)) %>%
  mutate(y = dbeta(x, 1+y, 1+n-y)) %>%
  bind_rows(data.frame(x = ci[2:1], y = 0))

polygon(ci_area$x, ci_area$y, col='red', border=NA)
@
\end{frame}


\begin{frame}
\frametitle{Summary}

Bayesian parameter estimation involves
\begin{enumerate}[<+->]
\item Specifying a model $p(y|\theta)$ for your data.
\item Specifying a prior $p(\theta)$ for the parameter. 
\item Deriving the posterior 
\[
p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \propto p(y|\theta)p(\theta).
\]
\item Calculating quantities of interest, e.g.
  \begin{itemize}
  \item Posterior expectation, $E[\theta|y]$
  \item Credible interval
  \end{itemize}
\end{enumerate}

\end{frame}

\end{document}
