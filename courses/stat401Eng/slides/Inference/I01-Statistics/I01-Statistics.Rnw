\documentclass[handout]{beamer}

\input{../../frontmatter}
\input{../../commands}

\newtheorem{principle}[theorem]{Principle}

\title{I01 - Statistics}

\begin{document}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("ggplot2")
library("gridExtra")
@

<<set_seed>>=
set.seed(2)
@


\begin{frame}
\maketitle
\end{frame}


\section{Descriptive statistics}
\begin{frame}
\frametitle{Statistics}
\begin{definition}
The \alert{field of statistics} is the study of the collection, analysis, interpretation, 
presentation, and organization of data.

{\tiny \url{https://en.wikipedia.org/wiki/Statistics}}
\end{definition}

\vspace{0.1in} \pause

There are two different phases of statistics: \pause
\begin{itemize}
\item descriptive statistics \pause 
	\begin{itemize}
	\item statistics
	\item graphical statistics
	\end{itemize}
\item inferential statistics.
\end{itemize}

\end{frame}




\subsection{Sample}
\begin{frame}
\frametitle{Population and sample}
\begin{definition}
The \alert{population} consists of all units of interest. \pause 
Any numerical characteristic of a population is a \alert{parameter}. \pause
The \alert{sample} consists of observed units collected from the population. 
\pause
Any function of a sample is called a \alert{statistic}.
\end{definition}

\vspace{0.1in} \pause

\begin{example}

Consider the population of all in-use routers by graduate students 
at Iowa State University. \pause
We are interested in what proportion have Gigabit speed. \pause
We collect data from students in STAT 401 (our sample) and record the 
proportion (a statistic) that have Gigabit routers. 
\end{example}
\end{frame}



\subsection{Random sample}
\begin{frame}
\frametitle{Simple random sampling}
\begin{definition}
A \alert{simple random sample} is a sample from the population where all subsets
of the same size are equally likely to be sampled. \pause
Random samples ensure that statistical conclusions will be valid.
\end{definition}

\vspace{0.1in} \pause 

\begin{example}
Consider the population of all in-use routers by graduate students 
at Iowa State University. \pause
We are interested in what proportion have Gigabit speed. \pause
A pseudo-random number generator gives each student a Unif(0,1) number and the
lowest 100 are contacted (our sample) and the proportion (a statistic) of these 
students who have Gigabit routers is recorded.
\end{example}
\end{frame}


\begin{frame}
\frametitle{Sampling and non-sampling errors}

\begin{definition}
\alert{Sampling errors} are caused by the mere fact that only a sample, a 
portion of a population, is observed.
\pause 
For most reasonable statistical procedures, sampling errors decrease (and 
converge to zero) as the sample size increases.

\vspace{0.1in} \pause

\alert{Non-sampling errors} are caused by inappropriate sampling schemes and
wrong statistical techniques. \pause 
Often, no statistical technique can rescue a poorly collected sample of data. 
\end{definition}

\pause
\begin{example}
In our example, no statistical technique can help us estimate
the proportion of graduate students at ISU who have Gigabit routers based on our 
convenience sample of STAT 401 students who have a Gigabit router.
\end{example}

\end{frame}


\subsection{Statistics}
\begin{frame}
\frametitle{Descriptive statistics}
\small

\begin{definition}
A \alert{statistic} is any function of the data. 
\end{definition}

\pause

\begin{example}
Statistics:
\begin{itemize}
\item Sample mean, median, mode
\item Sample quantiles
\item Sample variance, standard deviation
\end{itemize}
\end{example}

\pause

\begin{definition}
When a statistics is meant to estimate a corresponding population parameter, we
call that statistic an \alert{estimator}. 
\end{definition}

\end{frame}



\subsection{Mean and variance}
\begin{frame}
\frametitle{Sample mean}

Let $X_1,\ldots,X_n$ be a sample from a distribution with 
\[ E[X_i] = \mu \qquad \mbox{ and } \qquad Var[X_i] = \sigma^2 \] 
where we assume independence between the $X_i$. 

\vspace{0.1in} \pause

The sample mean is 
\[ 
\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]
\pause
and estimates the population mean $\mu$. 

\vspace{0.1in} \pause

The sample variance is 
\[ 
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2 
= \frac{\sum_{i=1}^n X_i^2 - n\overline{X}^2}{n-1}
\]
and estimates the population variance $\sigma^2$. \pause 
The sample standard deviation  is $S = \sqrt{S^2}$.

\end{frame}



\subsection{Unbiased}
\begin{frame}
\frametitle{Unbiased}

\begin{definition}
An estimator is \alert{unbiased} for a parameter if its expectation 
(when the data are considered random) equals the parameter. 
\end{definition}

\pause

\begin{example}
The sample mean is unbiased for the population mean $\mu$ since
\[ 
E\left[\,\overline{X}\,\right] 
= \pause E\left[\frac{1}{n} \sum_{i=1}^n X_i \right]
= \pause \frac{1}{n} \sum_{i=1}^n E[X_i] = 
\pause \mu.
\]
\pause
and the sample variance is unbiased for the population variance $\sigma^2$.
\end{example}
\end{frame}



\subsection{Consistent}
\begin{frame}
\frametitle{Consistent}

\begin{definition}
An estimator $\hat{\theta}$, or $\hat{\theta}_n(x)$, \pause
is \alert{consistent} for a parameter $\theta$ if 
the probability of its sampling error of any magnitude converges to 0 
as the sample size $n$ increases to infinity, \pause i.e.
\[ 
P\left( \left|\hat{\theta}_n(x)-\theta\right|>\epsilon \right) \to 0 \mbox{ as } n\to\infty
\]
for any $\epsilon>0$.
\end{definition}

\pause 

\begin{example}
The sample mean is consistent for $\mu$ since $Var\left[\,\overline{X}\,\right] = \sigma^2/n$
and 
\[ 
P\left(\left|\overline{X} - \mu\right| > \epsilon \right) \le 
\frac{Var\left[\,\overline{X}\,\right]}{\epsilon^2} = \frac{\sigma^2/n}{\epsilon^2} \to 0
\]
where the inequality is from \href{https://en.wikipedia.org/wiki/Chebyshev's_inequality}{Chebyshev's inequality.}
\end{example}
\end{frame}



\subsection{Quantiles}
\begin{frame}
\frametitle{Quantiles}
\small
\begin{definition}
A \alert{$p$-quantile} of a population is a number $x$ that solves
\[ 
P(X<x) \le p \quad \mbox{and} \quad P(X>x) \le 1-p.
\]
\pause
A \alert{sample $p$-quantile} is any number that exceeds at most $100p$\% of the
sample, and is exceeded by at most $100(1-p)$\% of the sample.  \pause
A \alert{$100p$-percentile} is a $p$-quantile. 
\pause
First, second, and third \alert{quartiles} are the 25th, 50th, and 75th 
percentiles. 
They split a population or a sample into four equal parts.
\pause
A \alert{median} is a 0.5-quantile, 50th percentile, and 2nd quartile.
\pause
The \alert{interquartile range} is the third quartile minus the first quartile,
i.e.
\[ 
IQR = Q_3 - Q_1
\]
and the \alert{sample interquartile range} is the third sample quartile minus
the first sample quartile, i.e.
\[ 
\hat{IQR} = \hat{Q}_3 - \hat{Q}_1
\]
\end{definition}
\end{frame}



\begin{frame}[fragile]
\frametitle{Standard normal quartiles}
<<quantiles, fig.height=4, echo=TRUE>>=
curve(expr = dnorm, from = -3, to = 3, ylab = "f(x)")
quantiles = c(.025,.16,.84,.975)
abline(v = qnorm(p = quantiles)) # default is standard normal
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Sample quartiles from a standard normal}
<<sample_quantiles, dependson='quantiles', fig.height=4, echo=TRUE>>=
n = 1000
sample = rnorm(n)
hist(x = sample, breaks = 101, probability = TRUE, border = "gray", col = "gray")
curve(expr = dnorm, from = -3, to = 3, ylab = "f(x)", col = "black", add = TRUE)
abline(v = qnorm(p = quantiles), col = "black")
abline(v = quantile(sample, prob = quantiles), col = "gray")
legend("topright", c("sample","population"), lty=1, col=c("gray","black"))
@
\end{frame}



\subsection{Standard error}
\begin{frame}
\frametitle{Standard error}
\begin{definition}
The \alert{standard error} of a statistic $\hat{\theta}$ is the standard
deviation of that statistic (when the data are considered random).
\end{definition}
\pause
\begin{example}
The standard error of the sample mean is $\sigma/\sqrt{n}$ since (if the $X_i$
are independent) we have
\[
Var\left[\,\overline{X}\,\right] = \pause
Var\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] = \pause
\frac{1}{n^2} \sum_{i=1}^n Var[X_i] = \pause 
\frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \pause
\sigma^2/n
\]
\pause
and thus
\[
SD\left[\,\overline{X}\,\right] = \sqrt{Var\left[\,\overline{X}\,\right]} = \sigma/\sqrt{n}.
\]
\end{example}
\end{frame}




\subsection{Binomial example}
\begin{frame}
\frametitle{Binomial example}

Suppose $Y\sim Bin(n,\theta)$ where $\theta$ is the probability of success.
\pause
The statistic $\hat{\theta} = Y/n$ is an estimator of $\theta$. 

\vspace{0.1in} \pause

Since 
\[ 
E\left[\hat{\theta}\right] = \pause 
E\left[\frac{Y}{n}\right] = \pause 
\frac{1}{n} E[Y] = \pause
\frac{1}{n} n\theta = \pause
\theta
\]
\pause
the estimator is unbiased.

\vspace{0.1in} \pause

The variance of the estimator is 
\[ 
Var\left[\hat{\theta}\right] = \pause
Var\left[ \frac{Y}{n} \right] = \pause
\frac{1}{n^2} Var[Y] = \pause
\frac{1}{n^2} n\theta(1-\theta) = \pause
\frac{\theta(1-\theta)}{n}.
\]
\pause
Thus the standard error is 
\[ 
SE(\hat{\theta}) = \sqrt{Var[\hat{\theta}]} = \sqrt{\frac{\theta(1-\theta)}{n}}.
\]
\pause
By Chebychev's inequality, this estimator is consistent for $\theta$.
\end{frame}


\subsection{Summary}
\begin{frame}
\frametitle{Summary}
\begin{itemize}
\item Statistics are functions of data.
\item Statistics have some properties:
	\begin{itemize}
	\item Standard error
	\end{itemize}
\item Statistics often try to estimate population parameters and are then called
estimators.
\item Estimators may have these properties relative to the population parameter
they are trying to estimate:
\begin{itemize}
\item Unbiased
\item Consistent 
\end{itemize}
\end{itemize}

\end{frame}





\section{Graphical statistics}
\begin{frame}
\frametitle{Look at it!}
{\Huge
\alert{Before you do anything with a data set, \pause LOOK AT IT!}
}
\end{frame}



\begin{frame}
\frametitle{Why should you look at your data?}

\pause

\begin{enumerate}
\item Find errors \pause
	\begin{itemize}
	\item Do variables have the correct range, e.g. positive?
	\item How are Not Available encoded?
	\item Are there outliers? \pause
	\end{itemize}
\item Do known or suspected relationships exist? \pause
	\begin{itemize}
	\item Is X linearly associated with Y?
	\item Is X quadratically associated with Y? \pause
	\end{itemize}
\item Are there new relationships? \pause
	\begin{itemize}
	\item What is associated with Y and how? \pause
	\end{itemize}
\item Do variables adhere to distributional assumptions? \pause
	\begin{itemize}
	\item Does Y have an approximately normal distribution?
	\item Right/left skew
	\item Heavy tails
	\end{itemize}
\end{enumerate}
\end{frame}







\begin{frame}
\frametitle{Principles of professional statistical graphics}

\begin{itemize}
\item Show the data
	\begin{itemize}
	\item Avoid distorting the data, e.g. pie charts, 3d pie charts, exploding wedge 3d pie charts, bar charts that do not start at zero
	\end{itemize}
\item Plots should be self-explanatory
	\begin{itemize}
	\item Use informative caption, legend
	\item Use normative colors, shapes, etc
	\end{itemize}
\item Have a high information to ink ratio
	\begin{itemize}
	\item Avoid bar charts
	\end{itemize}
\item Encourage eyes to compare
	\begin{itemize}
	\item Use size, shape, and color to highlight differences
	\end{itemize}
\end{itemize}

{\tiny \url{https://moz.com/blog/data-visualization-principles-lessons-from-tufte}}

\end{frame}


\begin{frame}
\frametitle{Stock market return}
\begin{center}
\includegraphics{market_return}
\end{center}
{\tiny \url{http://www.nytimes.com/interactive/2011/01/02/business/20110102-metrics-graphic.html?_r=0}}
\end{frame}
 



\end{document}



