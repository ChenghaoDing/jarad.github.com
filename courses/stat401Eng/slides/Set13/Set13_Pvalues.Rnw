\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../frontmatter}
\input{../commands}

\title{Set 13 - pvalues}

% \newenvironment{remark}[1][Remark]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\begin{frame}
\maketitle
\end{frame}



\section{Statistical hypothesis testing}
\begin{frame}
\frametitle{Statistical hypothesis testing}

\begin{definition}
A (classical) \alert{hypothesis test} consists of two hypotheses:
\begin{itemize}
\item null hypothesis ($H_0$) and
\item an alternative hypothesis ($H_A$)
\end{itemize}
which make a claim about parameters in a model \pause 
and a decision to either
\begin{itemize}
\item reject the null hypothesis or 
\item fail to reject the null hypothesis.
\end{itemize}
We reject the null hypothesis if our pvalue is less than a pre-determined 
\alert{significance level} $a$ where the \alert{pvalue} is the probability
\emph{when the data are considered random} of observing a test statistic as or 
more extreme than that observed if the null hypothesis is true.
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Binomial model}
If $Y\sim Bin(n,\theta)$, then the standard hypotheses are 
\begin{itemize}
\item $H_0: \theta=\theta_0=0.5$ and 
\item $H_A: \theta\ne \theta_0$.
\end{itemize}

\pause

In this case, the 
\begin{itemize}
\item test statistic is $Y$,
\item its sampling distribution \emph{when the null hypothesis is true is $Y\sim Bin(n,\theta_0)$}, and
\item the \emph{as or more extreme} region is values farther from $n\theta_0$ than $y$.
\end{itemize}
So the pvalue is 
\[
pvalue = P(|Y-n\theta_0|\ge |y-n\theta_0|)
\]
where $y$ is the observed successes.
\end{frame}



\begin{frame}[fragile]
\frametitle{}

<<echo=TRUE, fig.height=3.5>>=
library(dplyr); library(ggplot2)
n <- 13; y <- 2; theta0 <- 0.5
d <- data.frame(Y = 0:n) %>%
  mutate(pmf = dbinom(Y, n, theta0),
         as_or_more_extreme = abs(Y-n*theta0) >= abs(y-n*theta0))

ggplot(d, aes(Y, pmf, fill=as_or_more_extreme)) + geom_bar(stat = "identity") + 
  theme_bw() + theme(legend.position="bottom")
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial example}

If $Y\sim Bin(n,\theta)$ with $n=13$ and $y=2$ and we are testing 
\begin{itemize}
\item $H_0: \theta=0.5$ versus
\item $H_A: \theta\ne 0.5$,
\end{itemize}
\pause
then the pvalue is 
\[ 
pvalue = \sum_{y=0}^2 P(Y=y|\theta=0.5) + \sum_{11}^{13} P(Y=y|\theta=0.5)
\]
which is 
<<echo=TRUE>>=
(p <- sum(dbinom(c(0:2,11:13), size = 13, prob = 0.5)))
@
Thus, we would \emph{reject the null hypothesis} for any significance level 
greater than \Sexpr{p}.
\end{frame}


\begin{frame}[fragile]
\frametitle{{\tt binom.test}}

The R function `binom.test` can perform this test for us:
<<echo=TRUE>>=
binom.test(2,13)
@

\end{frame}



\subsection{One-sided pvalues}
\begin{frame}
\frametitle{One-sided pvalues}
If $Y\sim Bin(n,\theta)$, a one-sided hypothesis test is
\begin{itemize}
\item $H_0: \theta\ge\theta_0=0.5$ and 
\item $H_A: \theta< \theta_0$.
\end{itemize}

\pause

In this case, the 
\begin{itemize}
\item test statistic is $Y$,
\item its sampling distribution \emph{when the null hypothesis is true is $Y\sim Bin(n,\theta_0)$}, and
\item the \emph{as or more extreme} region is values farther from $n\theta_0$ than $y$ in the direction of $H_A$. 
\end{itemize}
So the pvalue is 
\[
pvalue = P(Y-n\theta_0\le y-n\theta_0) = P(Y\le y)
\]
where $y$ is the observed successes.
\end{frame}



\begin{frame}[fragile]
\frametitle{}

<<echo=TRUE, fig.height=3.5>>=
library(dplyr); library(ggplot2)
n <- 13; y <- 2; theta0 <- 0.5
d <- data.frame(Y = 0:n) %>%
  mutate(pmf = dbinom(Y, n, theta0),
         as_or_more_extreme = Y <= y)

ggplot(d, aes(Y, pmf, fill=as_or_more_extreme)) + geom_bar(stat = "identity") + 
  theme_bw() + theme(legend.position="bottom")
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial example}

If $Y\sim Bin(n,\theta)$ with $n=13$ and $y=2$ and we are testing 
\begin{itemize}
\item $H_0: \theta\ge 0.5$ versus
\item $H_A: \theta< 0.5$,
\end{itemize}
\pause
then the pvalue is 
\[ 
pvalue = \sum_{y=0}^2 P(Y=y|\theta=0.5)
\]
which is 
<<echo=TRUE>>=
(p <- sum(dbinom(0:2, size = 13, prob = 0.5)))
@
Thus, we would \emph{reject the null hypothesis} for any significance level 
greater than \Sexpr{p}.
\end{frame}


\begin{frame}[fragile]
\frametitle{{\tt binom.test()}}

The R function `binom.test()` can perform this test for us:
<<echo=TRUE>>=
binom.test(2, 13, alternative="less")
@

\end{frame}



\subsection{Asymptotic pvalues}
\begin{frame}
\frametitle{Asymptotic pvalues}

If we have an asymptotically normal estimator $\hat\theta=\hat\theta(Y)$, i.e. 
\[ 
\hat\theta(Y) \stackrel{\cdot}{\sim} N(E[\hat\theta], Var[\hat\theta])
\]
\pause
then we can calculate pvalues using this approximate sampling distribution.

\vspace{0.1in}

\begin{itemize}
\item $H_0: \theta = \theta_0 \implies p=P(|\hat\theta(Y)-E[\hat\theta]| \ge |\hat\theta(y)-E[\hat\theta]|)$
\item $H_0: \theta \ge \theta_0 \implies p=P(\hat\theta(Y) \le \hat\theta(y))$
\item $H_0: \theta \le \theta_0 \implies p=P(\hat\theta(Y) \ge \hat\theta(y))$
\end{itemize}
where 
\begin{itemize}
\item $\hat\theta(Y)$ is the random estimator and 
\item $\hat\theta(y)$ is the observed estimator.
\end{itemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{Binomial example}

If $Y\sim Bin(n,\theta)$ and $n$ is large (and $y$ is not close to 0 or $n$), 
\pause
then 
\[ 
Y \stackrel{\cdot}{\sim} N(n\theta, n\theta(1-\theta)).
\]
\pause
If we have 
\[ 
H_0: \theta = \theta_0 \quad \mbox{versus} \quad 
H_A: \theta \ne \theta_0,
\]
\pause
then we our pvalue is 
\[ \begin{array}{rl}
pvalue &= P\left(|Y-n\theta_0|\ge |y-n\theta_0|\right) \\
&= 2P\left(\frac{Y-n\theta_0}{Var[\theta]} < \frac{-|y-n\theta_0|}{SE[\hat\theta]}\right) \\
&\approx 2P\left(Z < \frac{-|y-n\theta_0|}{\sqrt{n\theta_0(1-\theta_0)}}\right)
\end{array} \]
\pause
<<echo=TRUE>>=
n = 10000; y = 4900; theta0 = 0.5
2*pnorm(-abs(y-n*theta0)/sqrt(n*theta0*(1-theta0)))
@

\end{frame}


\begin{frame}[fragile]
\frametitle{{\tt prop.test()}}

For the binomial distribution, 
the {\tt prop.test()} function performs these hypothesis tests. 
\pause
For example, if $Y\sim Bin(n,\theta)$ and you want to test $H_0: \theta = \Sexpr{theta0}$
vs $H_A: \theta \ne \Sexpr{theta0}$ when observing $y=\Sexpr{y}$ successes out of $n=\Sexpr{n}$ attempts,
\pause the code is

<<echo=TRUE>>=
prop.test(y, n, p = theta0, correct = FALSE)
@

\pause
But you should use the continuity correction:

<<echo=TRUE>>=
prop.test(y, n, p = theta0, correct = TRUE)$p.value
@

\end{frame}



\subsection{Normal mean}
\begin{frame}
\frametitle{Normal mean}

Let $Y_i \ind N(\mu,\sigma^2)$, then 
\[ 
T = \frac{\overline{Y}-\mu}{S/\sqrt{n}} \sim t_{n-1}(0,1)
\]
is our test statistic and its sampling distribution.
\pause
We have the following null hypothesis tests and pvalues
\begin{itemize}
\item $H_0: \mu=\mu_0$ and $pvalue =  P(|T|\ge |t|) = 2P(T< -|t|)$
\item $H_0: \mu\ge\mu_0$ and $pvalue =  P(T\le t) = P(T<t)$
\item $H_0: \mu\le\mu_0$ and $pvalue = P(T\ge t) = 1-P(T<t)$
\end{itemize}
where 
\[
t = \frac{\overline{y}-\mu}{s/\sqrt{n}}
\]
is the observed value of our test statistic. 
\pause
This is called a \alert{one-sample t-test}.

\end{frame}




\begin{frame}[fragile]
\frametitle{{\tt t.test}}

<<echo=TRUE>>=
set.seed(1); y <- rnorm(15, mean = 1)
t.test(y)
t.test(y, mu = 1, alternative = "greater")
@

\end{frame}




\subsection{Relationship to confidence intervals}
\begin{frame}
\frametitle{Relationship to confidence intervals}

There is a one-to-one correspondence between pvalues and confidence intervals.
\pause
Consider the following null hypotheses and corresponding confidence intervals
(CIs) 
\begin{itemize}
\item $H_0: \theta = \theta_0$ (two-sided CI),
\item $H_0: \theta \ge \theta_0$ (one-sided lower), and
\item $H_0: \theta \le \theta_0$ (one-sided upper),
\end{itemize}
\pause

\begin{theorem}
The appropriate (two-sided vs one-sided in the correct direction) 
$100(1-a)$\% confidence interval contains $\theta_0$ if and only if 
the pvalue is greater than $a$. 
\end{theorem}


\end{frame}


\end{document}



