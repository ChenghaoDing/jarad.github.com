\documentclass[handout]{beamer}

\input{../../frontmatter}
\input{../../commands}

\title{I02 - Likelihood}

\begin{document}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("tidyr")
library("ggplot2")
@

<<set_seed>>=
set.seed(2)
@


\begin{frame}
\maketitle
\end{frame}


\section{Modeling}
\begin{frame}
\frametitle{Statistical modeling}

\begin{definition}
A \alert{statistical model} is a pair $(\mathcal{S}, \mathcal{P})$ where 
$\mathcal{S}$ is the set of possible observations, i.e. the sample space, and 
$\mathcal{P}$ is a set of probability distributions on $\mathcal{S}$.
\end{definition}

\vspace{0.1in} \pause

Typically, we will assume the data have a specific form, say $p(y|\theta)$, 
\pause
but the parameter (vector) $\theta$ is unknown. 
\pause
Thus $p(y|\theta)$ for all allowable values for $\theta$ provide the set 
$\mathcal{P}$ 
\pause
and the support of $p(y|\theta)$ is the set $\mathcal{S}$. 


\end{frame}


\subsection{Binomial}
\begin{frame}
\frametitle{Binomial model}

Suppose our data are 
\begin{itemize}
\item the number of success $y$ 
\item out of some number of attempts $n$ 
\item where each attempt is independent 
\item given a common probability of success $\theta$.
\end{itemize}
\pause
Then a reasonable statistical model is 
\[ 
Y \sim Bin(n,\theta)
\]
\pause 
since for any $0<\theta<1$ this model provides positive probability over the 
entire sample space, i.e. all possible observations.

\vspace{0.1in} \pause

Formally, 

\begin{itemize}
\item $\mathcal{S} = \{0,1,2,\ldots,n\}$ 
\item $\mathcal{P} = \{ Bin(n,\theta): 0<\theta<1 \}$.
\end{itemize}

\end{frame}


\subsection{Normal}
\begin{frame}
\frametitle{Normal model}
\small

Suppose our data are
\begin{itemize}
\item a set of real numbers, i.e. between $-\infty$ and $\infty$,
\item the population mean is $\mu$ and population variance is $\sigma^2$,
\item a histogram is reasonably approximated by a bell-shaped curve,
\item and each observation is independent of the others.
\end{itemize}
\pause
Then a reasonable statistical model is 
\[
Y_i \stackrel{ind}{\sim} N(\mu,\sigma^2)
\]
\pause
since for $-\infty<\mu<\infty, 0<\sigma^2<\infty$  this model provides 
positive density over the entire sample space, i.e. all possible obserations.

\vspace{0.1in} \pause

Formally, 

\begin{itemize}
\item $\mathcal{S} = \{y_i:y_i\in\mathbb{R}, i\in\{1,2,\ldots,n\}\}$ 
\item $\mathcal{P} = \{ N(\mu,\sigma^2): -\infty<\mu<\infty, 0<\sigma^2<\infty \}$ 
where $\theta = (\mu,\sigma^2)$.
\end{itemize}

\end{frame}


\section{Likelihood}
\begin{frame}
\frametitle{Likelihood}

\small

\begin{definition}
The \alert{likelihood function}, or simply \alert{likelihood}, is the joint 
probability mass/density function
for fixed data when viewed as a function of the parameter vector $\theta$.
\pause
Generally, we will write the joint probability mass or density function of 
the data as 
$p(y|\theta)$ and thus the likelihood is 
\[ 
L(\theta) = p(y|\theta)
\]
\pause
but where $y$ is fixed and known, i.e. it is your data.

\vspace{0.1in} \pause

The \alert{log-likelihood} is the (natural) logarithm of the likelihood, i.e. 
\[ 
\ell(\theta) = \log L(\theta).
\]

\end{definition}

\vspace{0.1in} \pause

The likelihood describes the relative support in the data for different values 
for your parameter, i.e. the larger the likelihood is the more consistent 
that parameter value is with the data.

\end{frame}


\subsection{Binomial}
\begin{frame}
\frametitle{Binomial likelihood}

Suppose $Y\sim Bin(n,\theta)$, then 
\[ 
p(y|\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}.
\]
\pause
where $\theta$ is considered fixed (but often unknown) and the argument to this
function is $y$.

\vspace{0.1in} \pause

Thus the likelihood is 
\[ 
L(\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}
\]
\pause
where $y$ is considered fixed and known and the argument to this function is
$\theta$.

\vspace{0.1in} \pause

{\bf Note}: I write $L(\theta)$ without any conditioning, e.g. on $y$, 
so that you don't confuse this with a probability mass (or density) function.
\end{frame}



\begin{frame}[fragile]
\frametitle{Binomial likelihood}

<<binomial_likelihood, fig.height=4.5>>=
d2 <- data.frame(theta = seq(0,1,by=0.01)) %>%
  mutate(`y=3, n=10` = dbinom(3,10,prob=theta),
         `y=6, n=10` = dbinom(6,10,prob=theta)) %>%
  gather(data,likelihood,-theta) 

ggplot(d2, aes(theta, likelihood, color=data, linetype=data)) +
  geom_line() +
  theme_bw()
@
\end{frame}


\subsection{Independent observations}
\begin{frame}
\frametitle{Likelihood for independent observations}

Suppose $Y_i$ are independent with marginal probability mass/density function
$p(y_i|\theta)$. 

\vspace{0.1in} \pause

The joint distribution for $y=(y_1,\ldots,y_n)$ is 
\[ 
p(y|\theta) = \prod_{i=1}^n p(y_i|\theta).
\]

\vspace{0.1in} \pause

The likelihood for $\theta$ is 
\[ 
L(\theta) = p(y|\theta) = \prod_{i=1}^n p(y_i|\theta)
\]
where we are thinking about this as a function  of $\theta$ for fixed $y$.

\end{frame}


\subsection{Normal}
\begin{frame}
\frametitle{Normal model}

Suppose $Y_i\ind N(\mu,\sigma^2)$, \pause then 
\[
p(y_i|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(y_i-\mu)^2}
\]
and 
\[ \begin{array}{rl}
p(y|\mu,\sigma^2) 
&= \prod_{i=1}^n p(y_i|\mu,\sigma^2) \\ 
&= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(y_i-\mu)^2} \\
&= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2} 
\end{array} \]
where $\mu$ and $\sigma^2$ are fixed (but often unknown) and the argument to 
this function is $y=(y_1,\ldots,y_n)$.

\vspace{0.1in} \pause

The likelihood is 
\[
L(\mu,\sigma) = p(y|\mu,\sigma^2) 
= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2}
\]
where $y$ is fixed and known and $\mu$ and $\sigma^2$ are the arguments to this
function.

\end{frame}



\begin{frame}
\frametitle{Normal likelihood}

<<normal_likelihood, fig.height=4.5>>=
x <- rnorm(3)

d <- expand.grid(mu = seq(-2,2,length=100),
                 sigma = seq(0,2,length=100)) %>%
  mutate(density = dnorm(x[1], mu, sigma)*dnorm(x[2], mu, sigma)*dnorm(x[3], mu, sigma))

ggplot(d, aes(mu,sigma,z=density)) +
  geom_contour() +
  theme_bw()
@

\end{frame}


\section{MLE}
\begin{frame}
\frametitle{Maximum likelihood estimator}

\begin{definition}
The \alert{maximum likelihood estimator (MLE)}, $\hat{\theta}_{MLE}$ 
is the parameter value $\theta$ that maximizes the likelihood function\pause,
i.e. 
\[ 
\hat{\theta}_{MLE} = \mbox{argmax}_\theta L(\theta).
\]
\end{definition}


\vspace{0.1in} \pause

When the data are discrete, the MLE the is parameter value that maximizes the probability of the observed data.

\end{frame}



\subsection{Binomial}
\begin{frame}
\frametitle{Binomial MLE via derivatives}

If $Y\sim Bin(n,\theta)$, then 
\[ 
L(\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}.
\]

\pause

To find the MLE, 
\begin{enumerate}
\item Take the derivative of $\ell(\theta)$ with respect to $\theta$.
\item Set it equal to zero and solve for $\theta$.
\end{enumerate}

\pause
\[ \begin{array}{rl}
\ell(\theta) &= \log {n\choose y} + y\log(\theta) + (n-y)\log(1-\theta) \pause \\
\frac{d}{d\theta} \ell(\theta) &= \frac{y}{\theta} - \frac{n-y}{1-\theta} \pause \stackrel{set}{=} 0 \pause \implies \\
\hat{\theta}_{MLE} &= y/n \pause 
\end{array} \]

Take the second derivative of $\ell(\theta)$ with respect to $\theta$ and 
check to make sure it is negative.

\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial MLE graphically}

<<binomial_mle, fig.height=4.5>>=
y <- 3
n <- 10
d3 <- data.frame(theta = seq(0,1,by=0.01)) %>%
  mutate(likelihood = dbinom(y,n,prob=theta)) 

ggplot(d3, aes(theta, likelihood)) +
  geom_line() +
  geom_vline(xintercept = y/n, col='red') +
  theme_bw()
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Numerical maximization}

<<binomial_mle_numerical, echo=TRUE>>=
log_likelihood <- function(theta) {
  dbinom(3, size = 10, prob = theta, log = TRUE)
}

optim(0.5, log_likelihood, 
      method='L-BFGS-B',            # this method to use bounds
      lower = 0.001, upper = .999,  # cannot use 0 and 1 exactly
      control = list(fnscale = -1)) # maximize
@
\end{frame}



\begin{frame}
\frametitle{Normal MLE}

\small

If $Y_i\ind N(\mu,\sigma^2)$, then 

{\tiny

\[ \begin{array}{rl}
L(\mu,\sigma^2) 
&= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2} \pause \\
&= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\overline{y}+\overline{y}-\mu)^2} \pause \\
&= (2\pi \sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n  \left[(y_i-\overline{y})^2 + 2(y_i-\overline{y})(\overline{y}-\mu) + (\overline{y}-\mu)^2 \right]\right) \pause \\
&= (2\pi \sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i-\overline{y})^2 + -\frac{n}{2\sigma^2}(\overline{y}-\mu)^2 \right) \quad\mbox{since } \sum_{i=1}^n (y_i-\overline{y}) = 0\pause \\ \\

\ell(\mu,\sigma^2) &= -\frac{n}{2} \log(2\pi\sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i-\overline{y})^2 -\frac{1}{2\sigma^2} n (\overline{y}-\mu)^2 \pause \\
\\
\frac{\partial}{\partial \mu} \ell(\mu,\sigma^2) &= \frac{n}{\sigma^2}(\overline{y}-\mu) \pause \stackrel{set}{=} 0 \implies \hat{\mu}_{MLE} = \overline{y} \pause \\ \\
\frac{\partial}{\partial \sigma^2} \ell(\mu,\sigma^2) &= 
-\frac{n}{2\sigma^2}+\frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (y_i-\overline{y})^2 \pause \stackrel{set}{=} 0 \pause \\
& \implies 
\hat{\sigma}_{MLE}^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\overline{y})^2 \pause 
= \frac{n-1}{n}S^2
\end{array} \]

}

Thus, the MLE for a normal model is 
\[
\hat{\mu}_{MLE} = \overline{y}, \quad \hat{\sigma}_{MLE}^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\overline{y})^2
\]
\end{frame}



\begin{frame}[fragile]
\frametitle{Numerical maximization}

<<normal_numerical_maximization, echo=TRUE, dependson="normal_likelihood">>=
x
log_likelihood <- function(theta) {
  sum(dnorm(x, mean = theta[1], sd = exp(theta[2]), log = TRUE))
}

o <- optim(c(0,0), log_likelihood,
            control = list(fnscale = -1))
o$convergence # make sure this is 0 indicating convergence

o$par[1]; exp(o$par[2])^2 # mean and variance
n <- length(x)
mean(x); (n-1)/n*var(x) # var uses n-1 in the denominator
@

\end{frame}


\begin{frame}
\frametitle{Normal likelihood}

<<normal_mle, fig.height=4.5, dependson=c("normal_likelihood","normal_numerical_maximization")>>=
ggplot(d, aes(mu,sigma,z=density)) +
  geom_contour() +
  geom_point(aes(x = mean(x), y = sqrt((n-1)/n*var(x))), shape=4, color='red') + 
  theme_bw()
@
\end{frame}

\begin{frame}
\frametitle{Summary}

\begin{itemize}[<+->]
\item For independent observations, the joint probability mass (density) function
is the product of the marginal probability mass (density) functions.
\item The \alert{likelihood} is the joint probability mass (density) function
when the argument of the function is the parameter (vector).
\item The \alert{maximum likelihood estimator (MLE)} is the value of the
parameter (vector) that maximizes the likelihood.
\end{itemize}
\end{frame}



\end{document}



