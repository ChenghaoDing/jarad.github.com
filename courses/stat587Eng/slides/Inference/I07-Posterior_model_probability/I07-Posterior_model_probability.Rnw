\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{I07 - Posterior model probability}

% \newenvironment{remark}[1][Remark]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("plyr")
library("dplyr")
library("tidyr")
library("ggplot2")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{Posterior model probabilities}
\subsection{One-sided alternative hypotheses}
\begin{frame}
\frametitle{One-sided alternative hypotheses}

For ``one-sided alternative hypotheses'' analyzed from a 
Bayesian perspective,
I think we should ignore the ``hypothesis test'' completely.
\pause
That is, we should obtain a posterior and calculate the posterior probabilities
from that posterior. 

\vspace{0.1in} \pause

For example,
let $Y_i \ind N(\mu,\sigma^2)$ with hypotheses
\begin{itemize}
\item $H_0: \mu \le 0$
\item $H_A: \mu > 0$.
\end{itemize}
\pause
Assume $p(\mu,\sigma^2)\propto 1/\sigma^2$ and obtain the posterior, 
\pause
i.e. 
\[ 
\mu |y \pause \sim t_{n-1}(\overline{y}, s^2/n).
\]
\pause
Then calculate 
\[ 
P(\mu\le 0|y) = 
P\left(T_{n-1} \le \frac{-\overline{y}}{s/\sqrt{n}}\right).
\]

\end{frame}



\subsection{Posterior model probabilities}
\begin{frame}
\frametitle{Posterior model probabilities}

From a Bayesian perspective, 
each hypothesis is a different model that describes how the world works.
\pause
There is no limit to two hypotheses and hypotheses are all treated the same,
\pause
i.e. there is no ``special'' hypothesis.

\vspace{0.1in} \pause

We are interested in determine our belief about each model after we see
the data. 
\pause
Thus, we calculate \alert{posterior model probabilities} over some set of $J$
models
\pause
i.e,
\[ 
p(H_j|y) \pause
= \frac{p(y|H_j)p(H_j)}{p(y)} \pause
= \frac{p(y|H_j)p(H_j)}{\sum_{k=1}^J p(y|H_k)p(H_k)}.
\]
\pause
In order to accomplish this, we need to determine 
\begin{itemize}
\item \alert{prior model probabilities}: $p(H_j) \,\forall\, j=1,\ldots,J$ 
\pause and
\item \alert{priors over parameters in each model}: 
\[ 
p(y|H_j) = \int p(y|\theta)\alert{p(\theta|H_j)} d\theta.
\]
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Two hypotheses}

A pvalue can loosely be interpreted as ``the probability of observing these
data if the null hypothesis is true''\pause,i.e. 
\[ 
p(y|H_0),
\]
\pause
But what we really want is ``the probability the null hypothesis is true, 
given that we observed this data''. 
\pause
Of course, we can never know the truth (or even the probability of the truth), 
but we can make statements about our belief that the null hypothesis is true,
\pause
i.e.
\[
p(H_0|y) \pause = \frac{p(y|H_0)p(H_0)}{p(y)}.
\]
\pause
If there are only two hypotheses (say $H_0$ and $H_A$), 
\pause
then we have 
\[
p(H_0|y) = \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+p(y|H_A)p(H_A)} 
\pause
= \left[1+\frac{p(y|H_A)p(H_A)}{p(y|H_0)p(H_0)}\right]^{-1}.
\]

\end{frame}



\begin{frame}
\frametitle{Point null hypotheses}

If $H_0: \theta=\theta_0$ and $H_A: \theta\ne \theta_0$, 
\pause 
then 
\[ \begin{array}{rl}
p(y|H_0) &= p(y|\theta_0) \pause \\
p(y|H_A) 
&= \int p(y,\theta|H_A) d\theta \pause \\
&= \int p(y|\theta,H_A)p(\theta|H_A) d\theta \pause \\
&= \int p(y|\theta)p(\theta|H_A) d\theta
\end{array} \]
\pause 
where $p(\theta|H_A)$ is the distribution of the parameter $\theta$ when the 
alternative hypothesis is true.

\pause

\begin{example}
If $Y_i\ind N(\mu,1)$ and we have the hypotheses $H_0: \mu=0$ vs $H_A: \mu\ne 0$
\pause 
with $\mu|H_A \sim N(0,1)$,
\pause
then 
\[ \begin{array}{rl}
y|H_0 &\sim N(0,1) \pause \\
y|H_A &\sim N(0,2).
\end{array} \]
\end{example}
\end{frame}





\subsection{Bayesian hypothesis tests}
\begin{frame}
\frametitle{Bayesian hypothesis tests}

To conduct a Bayesian hypothesis test, 
\pause
you need to specify 
\begin{itemize}
\item $p(H_j)$ \pause and 
\item $p(\theta|H_j)$ 
\end{itemize}
for every hypothesis $j=1,\ldots,J$.
\pause
Then, you can calculate 
\[ 
p(H_j|y) = \frac{p(y|H_j)p(H_j)}{\sum_{k=1}^J p(y|H_k)p(H_k)} \pause
= \left[ 1+ \sum_{k\ne j} \frac{p(y|H_k)}{p(y|H_j)}\frac{p(H_k)}{p(H_j)} \right]^{-1}
\]
\pause
where 
\[
BF(H_k:H_j) = \frac{p(y|H_k)}{p(y|H_j)} 
\]
\pause 
are the \alert{Bayes factor} for hypothesis $H_k$ compared to hypothesis $H_j$ \pause 
and
\[
p(y|H_j) = \int p(y|\theta)p(\theta|H_j) d\theta
\]
for all $j$.

\end{frame}

\subsection{Normal example}
\begin{frame}
\frametitle{Normal example}

Let $Y \sim N(\mu,1)$ and consider the hypotheses $H_0:\mu = 0$ and 
$H_A:\mu \ne 0$ with $\mu|H_A\sim N(0,C)$ and, for simplicity, 
$p(H_0) = p(H_A) = 0.5$.
\pause 
Then the two hypotheses are really 
\begin{itemize}
\item $H_0: Y \sim N(0,1)$ and
\item $H_A: Y \sim N(0,1+C)$.
\end{itemize}

\vspace{0.1in} \pause 

Thus 
\[
p(H_0|y) = \left[1+ \frac{p(y|H_A)}{p(y|H_0)}  \right]^{-1} \pause 
= \left[ 1 + \frac{N(y;0,1+C)}{N(y;0,1)}  \right]^{-1}
\]
\pause
where $N(y;\mu,\sigma^2)$ is evaluating the probability density function for 
a normal distribution with mean $\mu$ and variance $\sigma^2$ at the value $y$.
\end{frame}

\begin{frame}[fragile]
\frametitle{Normal example}
<<normal_bayes_factor, fig.width=8>>=
d = ddply(expand.grid(y=seq(0,5,by=1), C=10^seq(0,4,by=0.1)), .(y,C), summarize,
          post_prob_H0 = 1/(1+1/exp(dnorm(y,0,1,log=TRUE)-dnorm(y,0,1+C,log=TRUE))))

ggplot(d, aes(sqrt(C), post_prob_H0, color=factor(y))) + 
  geom_line() + 
  labs(x = expression(sqrt(C)), y = expression(paste("p(",H[0],"|y)"))) + 
  scale_color_discrete(name="y") +
  theme_bw()
@
\end{frame}








\begin{frame}
\frametitle{Do pvalues and posterior probabilities agree?}
Suppose $Y\sim Bin(n,\theta)$ and we have the hypotheses 
$H_0:\theta=0.5$ and $H_A:\theta\ne 0.5$ \pause
We observe $n=10,000$ and $y=4,900$ and find the pvalue is 
\[ pvalue \approx 2P(Y\le 4900) = 0.0466 \]
\pause so we would reject $H_0$ at the 0.05 level. 

\vspace{0.1in} \pause

The posterior probability of $H_0$ if we assume $\theta|H_A \sim Unif(0,1)$ and 
$p(H_0) = p(H_A) = 0.5$ is 
\[ p(H_0|y) \approx \frac{1}{1+1/10.8} = 0.96, \]
\pause so the probability of $H_0$ being true is 96\%. 

\vspace{0.1in} \pause 

It appears the posterior model probability and pvalue completely disagree!
\end{frame}

% \begin{frame}[fragile]
% \frametitle{Binomial $\overline{y}=0.49$ with $n\to\infty$} 
% <<paradox,fig.width=8>>=
% paradox = expand.grid(n=10^(seq(0,5,by=0.1)), ybar=0.49)
% paradox = ddply(paradox, .(n,ybar), summarize, pvalue=pvalue(lrt(n,ybar)), post_prob=post_prob(bf(n,ybar)))
% m = melt(paradox, id=c("n","ybar"))
% p = ggplot(m, aes(log10(n),value,col=variable)) + 
%   geom_line() +
%   theme_bw()
% print(p)
% @
% \end{frame}

\subsection{Jeffrey-Lindley Paradox}
\frame{\frametitle{Jeffrey-Lindley Paradox}
  \begin{definition}
  The \alert{Jeffrey-Lindley Paradox} concerns a situation when comparing two hypotheses $H_0$ and $H_1$ given data $y$ \pause and find
  \begin{itemize}[<+->]\small
  \item a frequentist test result is significant leading to rejection of $H_0$, but
  \item the posterior probability of $H_0$ is high. 
  \end{itemize}
  \end{definition}
  
  \vspace{0.2in} \pause
  
  This can happen when 
  \begin{itemize}[<+->]\small
  \item the effect size is small, 
  \item $n$ is large, 
  \item $H_0$ is relatively precise, 
  \item $H_1$ is relative diffuse, and
  \item the prior model odds is $\approx 1$. 
  \end{itemize}
}


\begin{frame}
\frametitle{No real paradox}

\pause

Pvalues:
\begin{itemize}[<+->]
\item Pvalues measure how incompatible your data are with the null hypothesis.
\item The smaller the pvalue, the more incompatible.
\item But they say nothing about how likely the alternative is.
\end{itemize}

\vspace{0.1in} \pause

Posterior model probabilities:
\begin{itemize}[<+->]
\item Posterior model probabilities measure how likely the data are under 
the predictive distribution for each hypothesis.
\item The larger the posterior probability, the more predictive that hypothesis 
was compared to the other hypotheses.
\item But this requires you to have at least two well-thought out models\pause, 
i.e. no vague priors.
\end{itemize}

\vspace{0.1in} \pause

Thus, these two statistics provide completely different measures of model 
adequecy.
\end{frame}

\end{document}



