\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{R02 - Regression diagnostics}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=TRUE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, echo=FALSE>>=
library("dplyr")
library("ggplot2")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}


\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{All models are wrong!}

George Box (Empirical Model-Building and Response Surfaces, 1987):
\begin{quote}
All models are wrong, but some are useful. 
\end{quote}

\vspace{0.2in} \pause

{\tiny \url{http://stats.stackexchange.com/questions/57407/what-is-the-meaning-of-all-models-are-wrong-but-some-are-useful}}

{\small
\begin{quotation}
``All models are wrong" that is, 
every model is wrong because it is a simplification of reality. 
Some models, especially in the "hard" sciences, 
are only a little wrong. 
They ignore things like friction or the gravitational 
effect of tiny bodies. 
Other models are a lot wrong - they ignore bigger things. 
\pause \\

``But some are useful" - simplifications of reality can be quite useful. 
They can help us explain, 
predict and understand the universe and all its various components.
\pause \\

This isn't just true in statistics! 
Maps are a type of model; 
they are wrong. 
But good maps are very useful. 
\end{quotation}
}


\end{frame}



\frame{\frametitle{Regression}
  The simple linear regression model is 
  \[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
	\pause this can be rewritten as 
	\[ Y_i = \beta_0 + \beta_1 X_i + e_i \quad e_i \stackrel{iid}{\sim} N(0,\sigma^2). \]
	
	\vspace{0.1in} \pause
	
	Key assumptions are:
	\begin{itemize}[<+->]
  \item The errors are 
    \begin{itemize}
    \item normally distributed,
    \item have constant variance, and
    \item are independent of each other.
    \end{itemize}
  \item There is a linear relationship between the expected response and the
  explanatory variables.
	\end{itemize}
}


\begin{frame}[fragile]
\frametitle{Telomere data}
<<data, echo=FALSE, warning=FALSE>>=
# From `abd` package
Telomeres <- structure(list(years = c(1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 2L, 
4L, 4L, 5L, 5L, 3L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 
6L, 8L, 8L, 8L, 7L, 7L, 8L, 8L, 8L, 10L, 10L, 12L, 12L, 9L), 
    telomere.length = c(1.63, 1.24, 1.33, 1.5, 1.42, 1.36, 1.32, 
    1.47, 1.24, 1.51, 1.31, 1.36, 1.34, 0.99, 1.03, 0.84, 0.94, 
    1.03, 1.14, 1.17, 1.23, 1.25, 1.31, 1.34, 1.36, 1.22, 1.32, 
    1.28, 1.26, 1.18, 1.03, 1.1, 1.13, 0.98, 0.85, 1.05, 1.15, 
    1.14, 1.24)), .Names = c("years", "telomere.length"), class = "data.frame", row.names = c(NA, 
-39L))

ggplot(Telomeres, aes(years, telomere.length)) + 
  geom_point() + 
  stat_smooth(method = "lm") + 
  theme_bw()
@
\end{frame}


\section{Case statistics}
\begin{frame}
\frametitle{Case statistics}

To evaluate these assumptions, 
we will calculate a variety of \alert{case statistics}:

\vspace{0.1in} \pause

\begin{itemize}
\item Leverage
\item Fitted values
\item Residuals
  \begin{itemize}
  \item Standardized residuals
  \item Studentized residuals
  \end{itemize}
\item Cook's distance
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Default diagnostic plots in R}
<<default_diagnostics, dependson = "data">>=
m <- lm(telomere.length ~ years, Telomeres)
opar = par(mfrow=c(2,3)); plot(m, 1:6, ask = FALSE); par(opar)
@
\end{frame}




\subsection{Leverage}
\begin{frame}
\frametitle{Leverage}

\begin{definition}
The \alert{leverage} ($0\le h_i\le 1$) of an observation $i$ is a measure of
how far away the observations explanatory variable value is away from the other
observations. \pause
Larger leverage indicates a larger \alert{potential} influence of a single 
observation on the regression model.
\end{definition}

\vspace{0.1in} \pause

In simple linear regression, 
\[
h_i = \frac{1}{n} + \frac{(x_i-\overline{x})^2}{SXX}
\]
which is involved in the standard error for the line for a location $x_i$. 

\vspace{0.1in} \pause

The variability in the residuals is a function of the leverage, i.e. 
\[
Var[r_i] = \sigma^2(1-h_i)
\]

\end{frame}



\begin{frame}[fragile]
\frametitle{Telomere data}
<<leverage, dependson="data">>=
m <- lm(telomere.length~years, Telomeres)

cbind(Telomeres, leverage = hatvalues(m)) %>%
  select(years, leverage) %>% 
  unique() %>% 
  arrange(-years)
@
\end{frame}


\subsection{Residuals}
\begin{frame}
\frametitle{Residuals and Fitted values}

A regression model can be expressed as 
\[ \begin{array}{rl}
Y_i &\ind N(\mu_i,  \sigma^2) \\
\mu_i &= \beta_0 + \beta_1 X_i
\end{array} \]

\pause

A fitted value $\hat{Y}_i$ for an observation $i$ is 
\[ 
\hat{Y}_i = \hat\mu_i = \hat{\beta}_0+\hat{\beta}_1 X_i
\]
\pause
and thus the residual is 
\[ \begin{array}{rl}
r_i &= Y_i - \hat{Y}_i \pause \\
&= Y_i - \hat\mu_i \pause \\
&= Y_i - (\hat{\beta}_0+\hat{\beta}_1 X_i) \pause \\
&= Y_i - \hat{\beta}_0-\hat{\beta}_1 X_i \\
\end{array} \]

\end{frame}


\subsection{Standardized residuals}
\begin{frame}
\frametitle{Standardized residuals}

Often we will \alert{standardize} residuals, \pause i.e.

\[
\frac{r_i}{\sqrt{\widehat{Var[r_i]}}} = \frac{r_i}{\hat\sigma\sqrt{1-h_i}}
\]

\pause

If $|r_i|$ is large, 
it will have a large impact on $\hat\sigma^2 = \sum_{i=1}^n r_i^2/(n-2)$.
\pause
Thus, we can calculate an \alert{externally studentized residual}

\[
\frac{r_i}{\hat\sigma_{(i)}\sqrt{1-h_i}}
\]

where $\hat\sigma_{(i)} = \sum_{j\ne i} r_j^2/(n-3)$.

\vspace{0.1in} \pause

Both of these residuals can be compared to a standard normal distribution.
\end{frame}


\begin{frame}[fragile]
\frametitle{Telomere data}
<<residuals, dependson="data">>=
m <- lm(telomere.length~years, Telomeres)

Telomeres %>%
  mutate(leverage     = hatvalues(m), 
         residual     = residuals(m), 
         standardized = rstandard(m),
         studentized  = rstudent(m)) 
@
\end{frame}


\subsection{Cook's distance}
\begin{frame}
\frametitle{Cook's distance}

If a particular observation is highly influential in estimating the parameters
of the regression model, 
we can assess how influential it is by fitting the regression model with and
without that observation and evaluating how the model parameters change.

\pause

\begin{definition}
The \alert{Cook's distance} for an observation $i$ ($d_i>0$) is a measure of 
how much the estimates of the regression parameters change when that observation
is included versus when it is excluded.
\end{definition}

\pause

Operationally, we might be concerned when $d_i$ is 
\begin{itemize}
\item larger than 1 or
\item larger then 4/n.
\end{itemize}

\end{frame}



\section{Default regression diagnostics in R}
\subsection{Residuals vs fitted values}
\begin{frame}
\tiny
<<residual_vs_fitted, dependson="residuals">>=
plot(m, 1)
@
\pause
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Linearity & Quadratic curve \\
Constant variance & Funnel shape \\
\hline
\end{tabular}
\end{frame}


\subsection{QQ-plot}
\begin{frame}
\tiny
<<qqplot, dependson="residuals">>=
plot(m, 2)
@
\pause
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Normality & Points don't generally fall along the line \\
\hline
\end{tabular}
\end{frame}


\subsection{Absolute standardized residuals vs fitted values}
\begin{frame}
\tiny
<<absolute_residuals, dependson="residuals", echo=FALSE>>=
plot(m, 3)
@
\pause
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Constant variance & Increasing (or decreasing) trend \\
\hline
\end{tabular}
\end{frame}



\subsection{Cook's distance}
\begin{frame}
\tiny
<<cooks_distance, dependson="residuals">>=
plot(m, 4)
@
\pause
\begin{tabular}{ll}
Outlier & Violation \\
\hline
Influential observation & Cook's distance larger than (1 or 4/n) \\
\hline
\end{tabular}
\end{frame}



\subsection{Residuals vs leverage}
\begin{frame}
\tiny
<<residuals_vs_leverage, dependson="residuals">>=
plot(m, 5)
@
\pause
\begin{tabular}{ll}
Outlier & Violation \\
\hline
Influential observation & Points outside red dashed lines \\
\hline
\end{tabular}
\end{frame}


\subsection{Cook's distance vs leverage}
\begin{frame}
\tiny
<<cooks_distance_vs_leverage, dependson="residuals">>=
plot(m, 6)
@
\pause
This plot is pretty confusing.
\end{frame}


\subsection{Summary}
\begin{frame}
\frametitle{Summary}

\footnotesize

Case statistics:
\begin{itemize}
\item Fitted values
\item Leverage
\item Residuals
  \begin{itemize}
  \item Standardized residuals
  \item Studentized residuals
  \end{itemize}
\item Cook's distance
\end{itemize}

\pause

Model assumptions:
	\begin{itemize}
	\item Normality
	\item Constant variance
	\item Independence
  \item Linearity
	\end{itemize}
	
\pause

Default plots in R do not assess all model assumptions.
Two additional suggested plots:
\begin{itemize}
\item Residuals vs row number
\item Residuals vs (each) explanatory variable
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Plot residuals vs row number (index)}
\tiny
<<residuals_vs_index, dependson="residuals", fig.height=3.5, echo=TRUE>>=
plot(residuals(m))
@
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Independence & A pattern suggests temporal correlation \\
\hline
\end{tabular}
\end{frame}


\begin{frame}[fragile]
\frametitle{Residual vs explanatory variable}
\tiny
<<residual_vs_explanatory, dependson="residuals", fig.height=3.5, echo=TRUE>>=
plot(Telomeres$years, residuals(m))
@
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Linearity & A pattern suggests non-linearity \\
\hline
\end{tabular}
\end{frame}



\section{ggResidpanel}

\subsection{R}
\begin{frame}[fragile]
\frametitle{ggResidpanel}

The {\tt ggResidpanel} package has a variety of options for diagnostic plots.

<<fig.height=3.5>>=
library("ggResidpanel")
resid_panel(m, plots = "R")
@
\end{frame}

\subsection{Customized}
\begin{frame}[fragile]
\frametitle{ggResidpanel}

The {\tt ggResidpanel} package has a variety of options for diagnostic plots.

<<fig.height=3.5>>=
resid_panel(m, plots = c("qq", "hist", "resid", "index", "yvp", "cookd"),
            bins = 30, ind.ncol = 3, smoother = TRUE,
            type = "standardized") # what I was calling studentized
@
\end{frame}



\end{document}



