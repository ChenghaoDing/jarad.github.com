\documentclass[handout]{beamer}


\input{../../frontmatter}
\input{../../commands}

\title{P1 - Probability}

\begin{document}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(dplyr)
library(ggplot2)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Probability}

\subsection{Interpretation}
\begin{frame}
\frametitle{Interpretation}

What do we mean when we say the word \alert{probability}? \pause
For example,
\begin{itemize}
\item The \alert{probability} of a recession in the next 12 months is 0.40\%. \pause
\item The \alert{probability} the Chiefs will beat the Titans is 98\%. \pause
\item The \alert{probability} of finding the Malaysia airlines wreckage in the next
3 months is 10\%. \pause
\end{itemize}

\vspace{0.1in} 

Interpretations:
\begin{itemize}
\item \alert{Relative frequency}: Probability is the proportion of times the 
event occurs as the number of times the event is attempt tends to infinity. \pause
\item \alert{Personal belief}: Probability is a statement about your personal
belief in the event occuring. 
\end{itemize}
\end{frame}


\subsection{Set operations}
\begin{frame}
\frametitle{Probability}

\begin{example}
\begin{itemize}[<+->]
\item Consider the event C: a successful connection to the internet from
a laptop.

\item From our experience with the wireless network and our internet service 
provider, we believe the probability we successfully connect is 90 \%.

\item We write: $P(C) = 0.9$

\item To be able to work with probabilities, in particular, 
to be able to compute \alert{probabilities of events},
a mathematical foundation is necessary.
\end{itemize}
\end{example}
\end{frame}



\begin{frame}
\frametitle{Set theory}

\begin{definition}
A \alert{set} is a collection of things \pause where we must define what things
we are talking about.
\pause
We use the following notation
\begin{itemize}[<+->]
\item $\omega\in \Omega$ means $\omega$ is an element of the set $\Omega$,
\item $\omega\notin \Omega$ means $\omega$ is not an element of the set $\Omega$,
\item $A \subseteq \Omega$ (or $\Omega \supseteq A$) means the set $A$ is a subset of $\Omega$ (with the sets possibly being equal), and
\item $A \subset \Omega$ (or $\Omega \supset A$) means the set $A$ is a \alert{proper subset} of $\Omega$, i.e. there is at least one element in $\Omega$ that is 
not in $A$.
\end{itemize}
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Examples}
\begin{example}
The set of all possible sums of two 6-sided dice rolls is
$\Omega = \{2,3,4,5,6,7,8,9,10,11,12\}$ 
\pause
and 
\begin{itemize}[<+->]
\item $2 \in    \Omega$
\item $1 \notin \Omega$
\item $\{2,3,4\} \subset \Omega$
\end{itemize}
\end{example}




\end{frame}



\begin{frame}
\frametitle{Set comparison, operations, terminology}
For the following $A,B \subseteq \Omega$ where $\Omega$ is the implied universe of all elements under study,
\begin{enumerate}[<+->]
\item \alert{Union} ($\union$): 
A union of events is an event consisting of all the outcomes in these events.
\[ A\union B = \{\omega\mid \omega \in A \text{ or } \omega \in B\} \]

\item \alert{Intersection} ($\intersection$): 
An intersection of events is an event consisting of the common outcomes in these events.
\[ A\intersection B = \{\omega\mid \omega \in A \text{ and } \omega \in B\} \]

\item \alert{Complement} ($A^C$): 
A complement of an event $A$ is an event that occurs when event $A$ does not happen.
\[ A^C = \{\omega \mid \omega \notin A \text{ and } \omega \in \Omega \} \]


\item \alert{Set difference} ($A\setminus B$):
All elements in $A$ that are not in $B$, i.e. 
\[ 
A\setminus B = \{\omega| \omega\in A \mbox{ and } \omega\notin B\}
\]

\end{enumerate}
\end{frame}





\begin{frame}
\frametitle{Venn diagrams}

\vspace{-0.2in}

<<venn_setup, out.width='0.8\\textwidth'>>=
l = 101
circle = data.frame(
  loc = c(rep("top", l),rep("bot",l)),
  x   = c(seq(-1, 1, length=l), seq(1, -1, length=l))) %>%
  mutate(y   = sqrt(1-x^2)*ifelse(loc=="bot",-1,1))
  
indent = circle %>%
  mutate(x = x-0.5,
         x = ifelse(x>0,-x,x))

yint = sqrt(1-.5^2)

Aonly = indent %>% mutate(set="Aonly")
Bonly = indent %>% mutate(x=-x, set="Bonly")

AandB = data.frame(loc=c(rep("left",l),rep("right",l)),
                   y = c(seq(-yint,yint,length=l), seq(yint,-yint,length=l))) %>%
  mutate(set="AandB",
         x = (sqrt(1-y^2)-.5)*ifelse(loc=="left",-1,1))

# ggplot(Bonly, aes(x,y,fill=set)) + 
#   geom_polygon(color="black") + 
#   coord_fixed() +
#   theme_minimal()
  

all = rbind(Aonly,Bonly,AandB) %>%
  mutate(union = TRUE,
         intersection = set == "AandB",
         difference = set == "Aonly",
         complement = set == "Bonly") %>%
  tidyr::gather(operation,value,union,intersection,complement,difference) %>%
  mutate(operation = factor(operation, 
                            levels=c("union","intersection","complement","difference")))


ggplot(all, aes(x,y, group=set, fill=value)) + 
  geom_rect(data = all %>% filter(operation == "complement"), fill="red", 
            xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, alpha = 0.3) + 
  geom_polygon(color="black") + 
#  geom_polygon(data=all %>% filter(set=="Aonly"), color="black") + 
  facet_wrap(~operation) + 
  coord_fixed() +
#  theme_minimal() +
  scale_fill_manual(values=c("gray","red")) + 
  annotate("text", label="A", x=-1, y=.5,size=8) + 
  annotate("text", label="B", x= 1, y=.5,size=8) +
  theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank())
@
\end{frame}



\begin{frame}
\frametitle{Example}

Consider the set $\Omega$ equal to all possible sum of two 6-sided die rolls
\pause
i.e. $\Omega = \{2,3,4,5,6,7,8,9,10,11,12\}$
and two subsets 
\begin{itemize}
\item all odd rolls: \pause $A = \{3,5,7,9,11\}$ \pause
\item all rolls below 6: \pause $B = \{2,3,4,5\}$
\end{itemize}

\pause
Then we have 
\begin{itemize}
\item $A \union B = \pause \{2,3,4,5,7,9,11\}$ \pause
\item $A \intersection B = \pause \{3,5\}$ \pause
\item $A^C = \pause \{2,4,6,8,10,12\}$ \pause
\item $B^C = \pause \{6,7,8,9,10,11,12\}$ \pause
\item $A\setminus B = \pause \{7,9,11\}$ \pause
\item $B\setminus A = \pause \{2,4\}$
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Set comparison, operations, terminology (cont.)}
\begin{enumerate}[<+->]
\setcounter{enumi}{4}
\item \alert{Empty Set} $\emptyset$ is a set having no elements, i.e. $\{\}$.
The empty set is a subset of every set:
\[ \emptyset \subseteq A \]

\item \alert{Disjoint sets}: Sets $A, B$ are disjoint if their intersection is empty:
\[ A\intersection B = \emptyset \]

\item \alert{Mutually exclusive sets}: Sets $A_1, A_2, \ldots$ are
  mutually exclusive if all pairs of these events are disjoint:
\[ A_i\intersection A_j  = \emptyset \text{ for any } i\neq j \]

\item \alert{De Morgan's Laws}:
\[
(A\union B)^C = A^C\intersection B^C
\qquad \mbox{and} \qquad
(A\intersection B)^C = A^C\union B^C
\]
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Examples}

Let $A=\{2,3,4\}, B = \{5,6,7\}, C = \{8,9,10\}, D = \{11,12\}$. 
\pause
Then 
\begin{itemize}
\item $A \intersection B = \emptyset$
\item $A,B,C,D$ are mutually exclusive
\item De Morgan's:
\[ \begin{array}{rl}
(A \union B) &= \{2,3,4,5,6,7\} \\
(A \union B)^C &= \{8,9,10,11,12\} \\ \\
A^C &= \{5,6,7,8,9,10,11,12\} \\
B^C &= \{2,3,4,8,9,10,11,12\} \\
A^C \intersection B^C &= \{8,9,10,11,12\}
\end{array} \]
so, by example,
\[ (A \union B)^C =  A^C \intersection B^C. \]
\end{itemize}

\end{frame}



\subsection{Kolmogorov's Axioms}
\begin{frame}
\frametitle{Kolmogorov's Axioms}

To be able to work with probabilities properly - to compute with 
them - one must lay down a set of postulates.

\vspace{0.2in} \pause

A system of probabilities (\alert{a probability model}) is an assignment of 
numbers $P(A)$ to events $A \subseteq \Omega$ \pause such that

\begin{itemize}[<+->]
    \item[(i)]  $0 \le P(A) \le 1$ for all $A$ 
    \item[(ii)] $P(\mOmega) = 1$. 
    \item[(iii)]  if $A_{1},A_{2}, \ldots$ are (possibly, infinite many)
      mutually exclusive events 
    (i.e. $A_{i} \intersection A_{j} = \emptyset$ for all $i\neq j$) then 
    \[
    P(A_{1} \union A_{2} \union \ldots ) = P(A_{1}) + P(A_{2}) + \ldots = 
    \sum_{i}P(A_{i}).
    \]
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Kolmogorov's Axioms (cont.)}
These are the basic rules of operation of a probability model
\begin{itemize}
\item every valid model must obey these,
\item any system that does, is a valid model.
\end{itemize}
\pause
Whether or not a particular model is realistic application is another question.

\vspace{0.2in} \pause


Example: Draw a single card from a standard deck of playing cards:
$\mOmega = \{ \textcolor{red}{red}, black \}$
Two different, equally valid probability models are:
\begin{center}
{\small
\begin{tabular}{l@{\extracolsep{.5in}}l}
    \underline{Model 1} & \underline{Model 2} \\
%    $P(0) = 0$ & $P(0) = 0$ \\
    $P(\mOmega) = 1$ & $P(\mOmega) = 1$ \\
    $P(\textcolor{red}{red}) = 0.5$ &  $P(\textcolor{red}{red}) = 0.3$ \\
    $P(black) = 0.5$ &  $P(black) = 0.7$
\end{tabular}}
\end{center}

\pause

Mathematically, both schemes are equally valid. 
But, of course, our real world experience would prefer model 1 
over model 2 as the `correct' model.


\end{frame}


\begin{frame}
\frametitle{Useful Consequences of Kolmogorov's Axioms}
Let $A,\,B\subseteq \Omega$. 

\vspace{0.2in} \pause

\begin{itemize}
\item Probability of the Complementary Event: 
$ P\left(A^C\right) = 1-P(A)$ \pause

{\textcolor{red}{Corollary:}} $P(\emptyset)=0$ \pause

\item Addition Rule of Probability 

\[ P(A \union B) = P(A) + P(B) - P(A \intersection B) \]

\item If $A\subseteq B$, then $P(A)\leq P(B)$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Example: Using Kolmogorov's Axioms}
We attempt to access the internet from a laptop at home. \pause
We connect successfully if and only if the wireless (WiFi) network works \emph{ and } the internet service provider (ISP) network works. \pause
Assume 
\begin{eqnarray*}
P(\text{ WiFi up } ) &=& .9   \\ 
P(\text{ ISP up } ) &=& .6, \mbox{ and } \\ 
P(\text{ WiFi up and ISP up } ) &=& .55.
\end{eqnarray*}

\pause

\begin{enumerate}[<+->]
\item What is the probability that the WiFi is up or the ISP is up?
\item What is the probability that both the WiFi and the ISP are down? 
\item What is the probability that we fail to connect?
%\item What is the probability that only the WiFi is up?
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Solution}
Let  $A\equiv \text{WiFi up};\ B\equiv \text{ISP up}$ \pause
\begin{enumerate}[<+->]
\item What is the probability that the WiFi is up or the ISP is up?
\[ P( \text{ WiFi up or ISP up}) =P(A\union B) = 0.9 + 0.6 - 0.55 = 0.95 \]
\item  What is the probability that both the WiFi and the ISP are down?
\[ \begin{array}{rl}
P( \text{ WiFi down and ISP down}) &=P\left(A^C\intersection B^C\right) = P\left([A\union B]^C\right) \\
&=1-.95=.05
\end{array} \]
\item What is the probability that we fail to connect?
\[ \begin{array}{rl}
\multicolumn{2}{l}{P( \text{ WiFi down or ISP down})} \\
 &=P\left(A^C\union B^C\right) =
 P\left(A^C\right) + P\left(B^C\right) -P\left(A^C\intersection B^C\right) \\
&=P\left(A^C\union B^C\right) =(1-.9)+(1-.6)-.05=.1+.4-.05=.45
\end{array} \]
\end{enumerate}

\end{frame}








\section{Probability}
\subsection{Conditional probability}



\begin{frame}
\frametitle{Conditional probability}

\begin{definition}
The \alert{conditional probability} of an event $A$ given an event $B$ is 
\[ 
P(A|B) = \frac{P(A\intersection B)}{P(B)}
\]
if $P(B) > 0$. 
\end{definition}

\pause

Intuitively, the fraction of outcomes in $B$ that are also in $A$. 

\pause

\begin{corollary} 
\[ 
P(A\intersection B) = P(A|B)P(B) = P(B|A)P(A).
\]
\end{corollary}
\end{frame}



\begin{frame}
\frametitle{Random CPUs}
{\scriptsize
A box has 500 CPUs with a speed of 1.8 GHz and 500 with a speed of 2.0 GHz. 
The numbers of good (G) and defective (D) CPUs at the two different speeds are 
as shown below.
\begin{center}
\begin{tabular}{c|c|c|c}
& 1.8 GHz & 2.0 GHz & Total\\
\hline 
G & 480 & 490 & \phantom{1}970  \\
D & \phantom{4}20 & \phantom{4}10 & \phantom{19}30  \\
\hline
Total & 500 & 500 & 1000    
\end{tabular}
\end{center}

\pause

We select a CPU at random and observe its speed. What is the probability that 
the CPU is defective given that its speed is 1.8 GHz?

\pause

Let 
\begin{itemize}
\item $D$ be the event the CPU is defective and
\item $S$ be the event the CPU speed is 1.8 GHz.
\end{itemize}

\pause

Then 
\begin{itemize}
\item $P(S) = 500/1000 = 0.5$
\item $P(S\intersection D) = 20/1000 = 0.02$.
\item $P(D|S) = P(S\intersection D)/P(S) = 0.02/0.5 = 0.04$. 
\end{itemize}
}
\end{frame}


\subsection{Independence}

\begin{frame}
\frametitle{Statistical independence}

\begin{definition}
Events $A$ and $B$ are statistically \alert{independent} if 
\[ 
P(A\intersection B) = P(A) \times P(B)
\]
\pause
or, equivalently,
\[ 
P(A|B) = P(A). 
\]
\end{definition}

\pause

Intuitively, the occurrence of one event does not affect the probability of the other. 

\pause

\begin{example}
If I toss a fair coin and it comes up tails, does that affect the probability the next coin flip is heads?
\end{example}
\end{frame}




\begin{frame}
\frametitle{WiFi example}

In trying to connect my laptop to the internet, I need 
\begin{itemize}
\item my WiFi network to be up (event $A$) and
\item the ISP network to be up (event $B$). 
\end{itemize}

\pause

As previous, assume the probability the WiFi network is up is 0.6 and the ISP network is up is 0.9. \pause
If the two events are independent, 
what is the probability we can connect to the internet?

\vspace{0.1in} \pause

Since we have independence, we know
\[ 
P(A \intersection B) = P(A)\times P(B) = 0.6 \times 0.9 = 0.54. 
\]

\end{frame}



\begin{frame}
\frametitle{Independence and disjoint}
\small

\alert{Warning:} Independence and disjointedness are two very different concepts!

\vspace{0.1in} \pause

\begin{columns}[t]
\begin{column}{0.5\textwidth}
{\textcolor{cyan}{Disjointedness:}}   
    \centerline{\includegraphics[width=1.5in]{disjoint}}

    If $A$ and $B$ are disjoint, their intersection is empty and therefore has 
    probability 0:
    \[
    P(A \intersection B) = P(\emptyset) = 0.
    \]
\end{column}

\pause

\begin{column}{0.5\textwidth}
{\textcolor{cyan}{Independence: }}
    \centerline{\includegraphics[width=1.5in]{intersection}}    
    If $A$ and $B$ are independent events, the probability of their 
    intersection can be computed as the product of their individual 
    probabilities: 
    \[ P(A \intersection B) = P(A) \cdot P(B) \]
\end{column}
\end{columns}
\end{frame}


\subsection{Reliability}
\begin{frame}
\frametitle{Parallel system}
\begin{definition}
A \alert{parallel} system consists of $K$ components $c_{1}, \ldots, c_{K}$ 
arranged in such a way that the system works if {\bf at least one} of the $K$ 
components functions properly.
\end{definition}
\centerline{\includegraphics{parallel-system}}
\end{frame}

\begin{frame}
\frametitle{Serial system}
\setkeys{Gin}{width=\textwidth}
\begin{definition}
A \alert{serial} system consists of $K$ components $c_{1}, \ldots, c_{K}$ 
arranged in such a way that the system works if and only if {\bf all} of the 
components function properly.
\end{definition}

\vspace{0.2in} \pause

\centerline{\includegraphics{series-system}}
\end{frame}



\begin{frame}
\frametitle{Reliability}

\begin{definition}
\alert{Reliability} of a system is the probability the system works. 
\end{definition}

\pause

\begin{example}
The reliability of the WiFi-ISP network (assuming independence) is 0.54
\end{example}

\end{frame}


\begin{frame}
\frametitle{Reliability of parallel systems with independent components}

Let $c_{1}, \ldots, c_{K}$ denote the $K$ components in a \alert{parallel} system. \pause
\alert{Assume} the $K$ components operate \alert{independently} and 
$P(c_{k} \text{ works})=p_{k}$. \pause
What is the reliability of the system?

\pause

\[ \begin{array}{rl}
P(\text{ system works }) &= P(\text{ at least one component works }) \\
&= 1-P(\text{ all components fail }) \\
&= 1-P(c_{1} \text{ fails and } c_{2} \text{ fails } \ldots \text{ and } c_{k} \text{ fails }) \\
&=1- \prod_{k=1}^K P(c_k \text{ fails}) \\
&= 1- \prod_{k=1}^K(1-p_{k}).
\end{array} \]

\end{frame}


\begin{frame}
\frametitle{Reliability of serial systems with independent components}

Let $c_{1}, \ldots, c_{K}$ denote the $K$ components in a \alert{serial} system. \pause
\alert{Assume} the $K$ components operate \alert{independently} 
and $P(c_{k} \text{ works })=p_{k}$. \pause
What is the reliability of the system?

\pause

\[ \begin{array}{rl}
P(\text{ system works }) &= P(\text{ all components work}) \\
&= \prod_{k=1}^K P(c_k \text{ works}) \\
&= \prod_{k=1}^K p_{k}.
\end{array} \]

\end{frame}


\begin{frame}
\frametitle{Reliability example}
\small
Each component in the system shown below is opearable with probability 0.92 
independently of other components. Calculate the reliability.

\begin{center}
\includegraphics*[scale=.25]{circuit1}
\end{center}

\pause

\begin{enumerate}
\item Serial components A B can be replaced by a component F that operates with probability $P(A\intersection B)=(0.92)^2=0.8464$.
\item Parallel components D and E can be replaced by component G that operates with probability $P(D\union E)=1-(1-0.92)^2=0.9936$.
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Reliability example (cont.)}
\small
Updated circuit:

\begin{center}
\includegraphics{circuit2}
\end{center}

\begin{enumerate}
\setcounter{enumi}{2}
\item Serial components C and G connected can be replaced by a component H that operates with probability $P(C\intersection G)=(0.92)(0.9936)=0.9141$.
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Reliability example (cont.)}

Updated circuit:

\begin{center}
\includegraphics{circuit3}
\end{center}

\begin{enumerate}
\setcounter{enumi}{3}
\item  Parallel componenents F and H are in parallel, so the reliability of the system is $P(F\union H)=1-(1-0.8424)(1-0.9141)=0.9868$.
\end{enumerate}
\end{frame}



\subsection{Bayes' Rule}
\begin{frame}
\frametitle{Partition}
\setkeys{Gin}{width=0.3\textwidth}
\begin{definition}
A collection of events $B_{1}, \ldots B_{K}$ is called a \alert{partition} (or \alert{cover}) of $\mOmega$ if 
\begin{itemize}
\item the events are mutually exclusive (i.e., $B_{i}\intersection B_{j} = \emptyset$ for $i\neq j$)\pause, and
\item the union of the events is $\mOmega$ (i.e., $\bigcup_{k=1}^{K}B_{k} = \mOmega$).
\end{itemize}
\end{definition}

\begin{center}
\includegraphics{cover}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Example}

Consider the sum of two 6-sided die, i.e. 
\[ \Omega=\{2,3,4,5,6,7,8,9,10,11,12\}. \]

\vspace{0.1in} \pause

Here are some covers:
\begin{itemize}[<+->]
\item $\{2,3,4\},\{5,6,7,8,9,10,11,12\}$
\item $\{2,3,4\}, \{5,6,7\},\{8,9,10\},\{11,12\}$
\item $A_2,A_3,\ldots,A_{12}$ where $A_i = \{i\}$
\item any $A$ and $A^C$ where $A \subseteq \Omega$
\end{itemize}

\end{frame}






\begin{frame}
\frametitle{Law of Total Probability}

\begin{theorem}[Law of Total Probability]
If  the collection of events $B_{1}, \ldots, B_{K}$ is a partition of $\mOmega$, and $A$ is an event, then 
\[ P(A) = \sum_{k=1}^{K}P(A|B_{k})P(B_{k}). \]
\end{theorem}
\pause
\begin{proof}
\[ \begin{array}{rll}
P(A) &= P\left( \bigcup_{k=1}^K A\intersection B_k \right) & \mbox{$B_1,\ldots,B_K$ is a partition} \pause \\
&= \sum_{k=1}^K P(A\intersection B_k) & \mbox{$A\intersection B_1,\ldots,A\intersection B_K$ are disjoint} \pause \\
&= \sum_{k=1}^K P(A|B_k)P(B_k) & \mbox{definition of conditional probability}
\end{array} \]
\end{proof}

\end{frame}


\begin{frame}
\frametitle{Law of Total Probability graphic}
\setkeys{Gin}{width=0.6\textwidth}

\begin{center}
\includegraphics{total-prob}
\end{center}
\end{frame}



\begin{frame}
\frametitle{Example}

In the come out roll of craps, you win if the roll is a 7 or 11. 
\pause
By the total law of probability, the probability you win is 
\[ 
P(\mbox{Win}) = \sum_{i=2}^{12} P(\mbox{Win}|i)P(i) = P(7) + P(11)
\]
since $P(\mbox{Win}|i) = 1$ if $i=7,11$ and 0 otherwise.

\end{frame}


\begin{frame}
\frametitle{Bayes' rule}

\begin{theorem}[Bayes' Rule]
If $B_{1}, \ldots, B_{K}$ is a partition of $\mOmega$, and $A$ is an event, then 
\[
P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{k=1}^{K}P(A|B_k)P(B_k)}.
\]
\end{theorem}

\pause

\begin{proof}
\[ \begin{array}{rll}
P(B_k|A) &= \frac{P(A\intersection B_k)}{P(A)} & \mbox{by definition of conditional probability} \pause \\
&= \frac{P(A|B_k)P(B_k)}{P(A)} & \mbox{by definition of conditional probability} \pause \\
&= \frac{P(A|B_k)P(B_k)}{\sum_{k=1}^K P(A|B_k)P(B_k)} & \mbox{by Law of Total Probability} 
\end{array} \]
\end{proof}

\end{frame}



\begin{frame}
\frametitle{Example}

If you win on a come-out roll in craps, 
what is the probability you rolled a 7?

\vspace{0.1in} \pause

\[ \begin{array}{rl}
P(7|\text{Win}) 
&= \frac{P(\text{Win}|7)P(7)}{\sum_{i=2}^{12} P(\text{Win}|i)P(i)}  \\ \\
&= \frac{P(7)}{P(7)+P(11)}.
\end{array} \]

\end{frame}



\begin{frame}
\frametitle{CPU testing}
\begin{example}
A given lot of CPUs contains 2\%  defective CPUs. \pause
Each CPU is tested before delivery. \pause
However, the tester is not wholly reliable: \pause
\[ \begin{array}{rl}
    P(\text{ tester says CPU is good } | \text{  CPU is good }) &= 0.95 \pause \\
    P(\text{ tester says CPU is defective } | \text{ CPU is defective }) &=  0.94
\end{array} \]
\pause
If the test device says the CPU is defective, what is the probability that the 
CPU is actually defective?
\end{example}
\end{frame}


\begin{frame}
\frametitle{CPU testing (cont.)}

Let
\begin{itemize}
\item $C_g$ ($C_d$) be the event the CPU is good (defective) \pause
\item $T_g$ ($T_d$) be the event the tester says the CPU is good (defective)
\end{itemize}

We know \pause

\begin{itemize}
\item $0.02 = P(C_d) \phantom{|C_g} = 1-P(C_g)$
\item $0.95 = P(T_g|C_g) = 1-P(T_d|C_g)$
\item $0.94 = P(T_d|C_d) = 1-P(T_g|C_d)$
\end{itemize}

\pause
Using Bayes' Rule, we have 

\[ \begin{array}{rl}
P(C_d|T_d) &= \frac{P(T_d|C_d)P(C_d)}{P(T_d|C_d)P(C_d)+P(T_d|C_g)P(C_g)} \pause \\
&= \frac{P(T_d|C_d)P(C_d)}{P(T_d|C_d)P(C_d)+[1-P(T_g|C_g)][1-P(C_d)]} \pause \\
&= \frac{0.94\times 0.02}{0.94\times 0.02 + [1-0.95]\times[1-0.02]} \pause \\
&= 0.28
\end{array} \]
\end{frame}






\end{document}
