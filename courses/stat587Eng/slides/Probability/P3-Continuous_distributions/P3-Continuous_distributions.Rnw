\documentclass[handout]{beamer}

\input{../../frontmatter}
\input{../../commands}

\title{P3 - Continuous distributions}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("ggplot2")
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Continuous random variables}
\subsection{Cumulative distribution function}
\begin{frame}
\frametitle{Cumulative distribution function}

All properties of discrete random variables have direct counterparts for continuous random variables.

\vspace{0.1in} \pause

In particular, 
\begin{definition}
The \alert{cumulative distribution function} for a continuous random variable is 
\[ 
F_X(x) = P(X\le x) \pause = P(X<x)
\]
since $P(X=x)=0$ for any $x$.
\end{definition}

\pause

we still have the properties 

\begin{itemize}
\item $ 0 \le F_{X}(x) \le 1$ for all $x \in \mathbb{R}$
\item $F_{X}$ is monotone increasing, i.e. if $x_{1} \le x_{2}$ then $F_{X}(x_{1}) \le F_{X}(x_{2})$.
\item $\lim_{x \to -\infty}F_{X}(x) = 0$ and $\lim_{x \to \infty}F_{X}(x) = 1$.
\end{itemize}
\pause
\end{frame}




\subsection{Probability density functions}
\begin{frame}
\frametitle{Probability density function}

\pause

\begin{definition}
The \alert{probability density function (pdf)} for a continuous random variable is 
\[ 
f_X(x) = \frac{d}{dx} F_X(x)
\]
\pause
and 
\[ 
F_X(x) = \int_{-\infty}^x f_X(t) dt.
\]
\end{definition}

\vspace{0.1in} \pause

Thus, the probability density function has the following properties
\begin{itemize}
\item $f_X(x) \ge 0$ for all $x$ \pause and
\item $\int_{-\infty}^\infty f(x) dx = 1.$
\end{itemize}
\end{frame}


\subsection{Example}
\begin{frame}
\frametitle{Example}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

$f_X(x)$ defines a valid pdf because $f_X(x) \ge 0$ for all $x$ \pause and
\[ 
\int_{-\infty}^\infty f_X(x) dx = \int_0^1 3x^2 dx = x^3 |_0^1 = 1.
\]

\pause
The cdf is 
\[ 
F_X(x) = \left\{ \begin{array}{ll}
0 & x\le 0 \\
x^3 & 0< x < 1 \\
1 & x\ge 1
\end{array}. \right.
\]

\end{frame}


\subsection{Expectation}
\begin{frame}
\frametitle{Expected value}

\begin{definition}
Let $X$ be a continuous random variable and $h$ be some function. 
The \alert{expected value} of a function of a continuous random variable is 
\[ 
E[h(X)] = \int_{-\infty}^\infty h(x) \cdot f_X(x) dx.
\]
\pause
If $h(x)=x$, then 
\[ 
E[X] = \int_{-\infty}^\infty x \cdot f_X(x) dx.
\]
\pause
and we call this the \alert{expectation} of $X$. \pause
We commonly use the symbol $\mu$ for this expectation.
\end{definition}
\end{frame}



\begin{frame}
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

The expected value is 
\[ \begin{array}{rl}
E[X] &= \int_{-\infty}^\infty x \cdot f_X(x) dx \\
&= \int_0^1 3x^3 dx \\
&= 3\frac{x^4}{4} |_0^1 = \frac{3}{4}.
\end{array} \]
\end{frame}


\begin{frame}
\frametitle{Example - Center of mass}

<<center_of_mass, fig.height=4>>=
d <- data.frame(x = seq(0,1,by=0.05)) %>%
  mutate(y = 3*x^2)


ggplot(d, aes(x, ymin = 0, ymax = y)) + 
  geom_ribbon() +
  geom_point(data = data.frame(x=.75,y=-0.1), aes(x,y), 
             shape=24, color='red', fill='red') + 
  labs(y = "probability density function") +
  theme_bw()
@

\end{frame}


\subsection{Variance}
\begin{frame}
\frametitle{Variance}

\begin{definition}
The \alert{variance} of a random variable is defined as the expected squared deviation from the mean. \pause 
For continuous random variables, variance is
\[
Var[X] = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2 f_X(x) dx
\]
where $\mu = E[X]$. \pause
The symbol $\sigma^2$ is commonly used for the variance.
\end{definition}

\pause

\begin{definition}
The \alert{standard deviation} is the positive square root of the variance
\[
SD[X] = \sqrt{Var[X]}.
\]
The symbol $\sigma$ is commonly used for the standard deviation.
\end{definition}

\end{frame}




\begin{frame}
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

The variance is 
\[ \begin{array}{rl}
Var[X] &= \int_{-\infty}^\infty \left(x-\mu\right)^2 f_X(x) dx \\
&= \int_0^1 \left(x-\frac{3}{4}\right)^2 3x^2 dx \\
&= \int_0^1 \left[x^2-\frac{3}{2}x + \frac{9}{16} \right] 3x^2 dx \\
&= \int_0^1 3x^4-\frac{9}{2}x^3 + \frac{27}{16}x^2 dx \\
&= \left[\frac{3}{5}x^5-\frac{9}{8}x^4 + \frac{9}{16}x^3\right]|_0^1 dx \\
&= \frac{3}{5}-\frac{9}{8}+\frac{9}{16} \\
&= \frac{3}{80}.
\end{array} \]
\end{frame}




\section{Comparison of discrete and continuous random variables}
\begin{frame}
\frametitle{Comparison of discrete and continuous random variables}

For simplicity here and later, we drop the subscript $X$. \pause


{\scriptsize
\begin{center}
\begin{tabular}{lcc}
& discrete & continuous \pause \\
\hline
image & finite or countable & uncountable \pause \\ \\
pmf &  $p(x) = P(X=x)$ & \pause \\ \\
pdf && $p(x) = f(x) = F'(x)$ \pause \\ \\
cdf & 
$\begin{array}{rl}F(x) &= P(X\le x) \\
                        &= \sum_{t\le x} p(x)\end{array} $ & 
$\begin{array}{rl}F(x) &= P(X\le x) \\
                        &= \int_{-\infty}^x p(t) dt\end{array} $
\pause \\ \\
expected value & $E[h(X)] = \sum_x h(x) p(x)$ & $E[h(X)] = \int_x h(x) p(x) dx$ \pause \\ \\
expectation & $\mu = E[X] = \sum_x x \, p(x)$ & $\mu = E[X] = \int_x x \, p(x) dx$ \pause \\ \\
variance & 
$ \begin{array}{rl} Var[X] &= E[(X-\mu)^2] \\
                            &= \sum_x (x-\mu)^2 p(x)
\end{array} $ & 
$ \begin{array}{rl}
Var[X] &= E[(X-\mu)^2] \\
       &= \int_x (x-\mu)^2 p(x) dx
\end{array} $ \\ \\
\hline
\end{tabular}
\end{center}
}

\vspace{0.05in}\pause

Note: we replace summations with integrals when using continuous as opposed to discrete random variables
\end{frame}




\subsection{Uniform}
\begin{frame}
\frametitle{Uniform}

A \alert{uniform} random variable on the interval $(a,b)$ has equal probability
for any value in that interval and we denote this $X\sim Unif(a,b)$. 
\pause
The pdf for a uniform random variable is 
\[ 
f(x) = \frac{1}{b-a}\I(a<x<b)
\]
\pause
where $\I(A)$ is in indicator function that is 1 if $A$ is true and 0 otherwise,
i.e. 
\[ 
\I(A) = \left\{ \begin{array}{ll} 1 & A\text{ is true} \\ 
0 & \text{otherwise}.
\end{array} \right.
\]
\pause 
The expectation is 
\[ 
E[X] = \int_a^b \frac{1}{b-a}\, x\, dx = \frac{a+b}{2}
\]
\pause
and the variance is 
\[ 
Var[X] = \int_a^b \frac{1}{b-a} \left(x-\frac{a+b}{2}\right)^2 dx = 
\frac{1}{12}(b-a)^2.
\]

\end{frame}




\begin{frame}[fragile]
\frametitle{Example (cont.)}

Pseudo-random number generators generate pseudo uniform values on (0,1). 
\pause
These values can be used in conjunction with the inverse of the cumulative
distribution function to generate pseudo-random numbers from any 
distribution.

\vspace{0.1in} \pause

The inverse of the cdf $F_X(x) =x^3$ is 
\[ 
F^{-1}_X(u) = u^{1/3}.
\]
\pause
A uniform random number on the interval (0,1) generated using the inverse cdf produces a random draw of $X$. \pause
So, in R

<<echo=TRUE>>=
inverse_cdf = function(u) u^(1/3)
x = inverse_cdf(runif(1e6))
mean(x)
var(x); 3/80
@

\end{frame}


\begin{frame}
\frametitle{}
<<>>=
hist(x,100, prob = TRUE)
curve(3*x^2, col='red', add=TRUE)
@
\end{frame}


\subsection{Normal distribution}
\begin{frame}
\frametitle{Normal distribution}

The \alert{normal (or Gaussian) density} is a ``bell-shaped'' curve. \pause
The density has two parameters: mean $\mu$ and variance $\sigma^{2}$ and is
\[
f(x) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} 
e^{-(x-\mu)^{2}/2 \sigma^{2}} \qquad \text{ for } -\infty<x<\infty
\]
\pause
The expected value and variance of a normal distributed r.v. $X$ are:
\[ \begin{array}{rll}
E[X] &= \int_{-\infty}^{\infty} x\, f(x) dx               = \ldots &= \mu \\
Var[X] &= \int_{-\infty}^{\infty} (x - \mu)^{2}\, f(x) dx = \ldots &= \sigma^{2}. \\
\end{array} \]
\pause
Thus, the parameters $\mu$ and $\sigma^{2}$ are actually the mean and the variance of the $N(\mu,\sigma^2)$ distribution. 

\vspace{0.1in} \pause

There is no closed form cumulative distribution function for a normal random variable. 

\end{frame}


\begin{frame}
\frametitle{Example probability density functions}

<<>>=
d = data.frame(mu=c(0,0,1,1), sigma=c(1,2,1,2))
d$legend_name = paste("mu=", d$mu, ", sigma=", d$sigma)
plot(0,0, type="n", xlim=c(-4,4), ylim=c(0,0.5))
for (i in 1:nrow(d)) {
	mu = d$mu[i]; sigma = d$sigma[i]
	curve(dnorm(x, mu, sigma), add=TRUE, lty=mu+1, col = sigma)
}
legend("topleft", legend = d$legend_name,
			 lty=d$mu+1, col = d$sigma)
@

\end{frame}



\begin{frame}
\frametitle{Properties of the normal distribution}

Let $Z\sim N(0,1)$, i.e. a \alert{standard normal} random variable.
\pause
(If you see $Z$ without explanation, it is a standard normal random variable.)
\pause
Then for constants $\mu$ and $\sigma$ 
\[ 
X = \mu+\sigma Z \sim N(\mu,\sigma^2)
\]
and
\[ 
Z = \frac{X-\mu}{\sigma} \sim N(0,1).
\]
which is called \alert{standardizing}.

\vspace{0.1in} \pause

Let $X_i \ind N(\mu_i,\sigma_i^2)$. Then 
\[
Z_i = \frac{X_i-\mu_i}{\sigma_i} \iid N(0,1) \quad\mbox{for all } i
\]
and 
\[ 
Y = \sum_{i=1}^n X_i \sim N\left(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma_i^2 \right).
\]

\end{frame}



\begin{frame}[fragile]
\frametitle{Calculating the standard normal cumulative distribution function}

If $Z\sim N(0,1)$, what is $P(Z\le 1.5)$? \pause 
Although the cdf does not have a closed form, very good approximations exist and are available as tables or in software, \pause e.g.
<<echo=TRUE>>=
pnorm(1.5) # default is mean=0, sd=1
@
A standard normal random variable is often denoted $Z$, the standard normal cdf is often denoted $\mPhi(z)$, and tables are called \emph{standard normal tables} or \emph{Z tables}. \pause
Sometimes these tables only have positive $z$ values, but we can still compute $\mPhi(z)$ for any $z$ since the normal distribution is \alert{symmetric}, i.e. $\mPhi(-z)=1-\mPhi(z)$. \pause
Finally, these tables usually only extend to $|z|<4$, but that's okay since $P(Z<-4) = P(Z>4) \approx 0.00003$. 
\end{frame}

\begin{frame}[fragile]
\frametitle{Calculating any normal cumulative distribution function}
If $X\sim N(15,4)$ what is $P(X>18)$? \pause
\[ \begin{array}{rl}
P(X>18) &= 1-P(X\le 18) \\
&= 1-P\left(\frac{X-15}{2} \le \frac{18-15}{2} \right) \\
&= 1-P(Z\le 1.5) \\
&\approx 1-0.933 = 0.067
\end{array} \]

<<echo=TRUE>>=
1-pnorm((18-15)/2)         # by standardizing
1-pnorm(18, mean=15, sd=2) # using the mean and sd arguments
@

\end{frame}


\subsection{Example}
\begin{frame}[fragile]
\frametitle{Manufacturing}

Suppose you are producing nails that must be within 5 and 6 centimeters in 
length. 
If the average length of nails the process produces is 5.3 cm and the standard
deviation is 0.1 cm. 
What is the probability of producing a nail outside of the specification?

\vspace{0.1in} \pause

Assume $X\sim N(\mu,\sigma^2)$ be the next nail produced with 
$\mu=5.3$ cm and $\sigma=0.1$ cm. 
\pause
We need to calculate 
\[ \begin{array}{rll} 
P(X<5 \mbox{ or } X>6) &= 1-P(5<X<6) \\
&= 1-[P(X<6) - P(X<5)] &\mbox{or}\\
&= P(X<5) + [1-P(X<6)].
\end{array} \]

\pause

<<echo = TRUE>>=
mu = 5.3
sigma = 0.1

pnorm(5, mean = mu, sd = sigma) + ( 1 - pnorm(6, mean = mu, sd = sigma) )
@

\end{frame}

\end{document}
