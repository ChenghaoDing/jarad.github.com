--- 
type: post
status: draft
published: false
title: Bayesians vs non-Bayesians
meta: 
  _edit_last: "2"
  _aktt_hash_meta: "#tt"
  aktt_notify_twitter: "yes"
tags: 
- Bayesian
layout: post
---
From Eric,
<blockquote>So as a Bayesian statistician, are you in the minority or the majority of statisticians? Â I ran across Gelman's 2008 article articulating some stereotypical concerns, now I'm wondering how much of a flash point Bayes is.</blockquote>
Bayesian analysis was certainly a flash point about a decade ago, but is becoming more accepted as it proves itself in practice. We are definitely in the minority, but this minority has only been created due to computational resources. Had we had computers in the early 1900s, I'm convinced we would have been doing Bayesian statistics and perhaps just now would have been talking about p-values and confidence intervals, i.e. the history would have been reversed. The question is where is the future going and, particularly for us, where is the short-term future, i.e. during our lifetime. Clearly the trend is going more Bayesian. Many statistics departments are specifically hiring Bayesians. My department is incorporating Bayesian ideas into its core master's curriculum because it believes that graduate students with a master's degree in statistics without knowledge of Bayesian ideas is doing a disservice to those students. .

<a title="Objections to Bayesian statistics" href="http://ba.stat.cmu.edu/journal/2008/vol03/issue03/gelman.pdf" target="_blank">Gelman's article</a> (<a title="Bayesian analysis" href="http://ba.stat.cmu.edu/vol03is03.php" target="_blank">comments and rejoinder</a>) does a reasonable job of laying down some anti-Bayesian arguments (although most of them he doesn't believe), although the presentation could have been improved, i.e. bullet points. I think the main objection to Bayesian analysis is using a prior because it is `drawn from thin air.' Of course, the likelihood (statistical model) is also drawn from thin air. So really the argument is against an additional assumption, i.e. the prior. In many models an objective prior exists and the results exactly match the MLE and confidence intervals. This is great because now you get interpretation from both perspectives.

There should be an additional article with anti-frequentist ideas. Here are some main issues:
- Limited coherence to frequentist procedures, i.e. p-values depend on your test statistic, confidence intervals depend on your parameterization, etc.
- You require asymptotic normality, how large a sample size is necessary anyway?
- Even in simple examples, you violate the likelihood principle.
- As a scientist what does a p-value mean to me? i.e. if my p-value is 0.05, what is the probability of the null hypotheses being true. (In a <a title="P-value interpretation" href="http://www.isds.duke.edu/~berger/papers/99-13.html" target="_blank">very simple setup you can show that the probability of the null is at least 23%</a>. See also the <a title="Pvalue applet" href="http://www.isds.duke.edu/~berger/applet2/pvalue.html" target="_blank">applet</a> and <a title="applet paper" href="http://www.isds.duke.edu/~berger/applet2/applet.pdf" target="_blank">associated paper</a>.)
- As a scientist, what does being 95% confident mean? I want to know where my parameter is, not what will happen with the interval if I repeat the experiment over and over and over. I'm gonna do the experiment once, maybe twice, shouldn't that be enough to tell me where the parameter is?
