--- 
type: post
status: draft
published: false
title: KL divergence versus Chi-square tests
meta: 
  _edit_last: "2"
  aktt_notify_twitter: "yes"
  _aktt_hash_meta: ""
tags: 
- Statistics
layout: post
---
<div>From the inbox:</div>
<blockquote>
<div>

For the Vector-Item pattern algorithm, we have been finding a distribution histogram of our data and then comparing it against a random distribution using Chi squared tests to determine if the distributions are significantly different.  One of the other graduate students has been looking at the Kullback–Leibler divergence:
<blockquote>Actually I am looking to how can I say that the divergence between two distribution is significant or not? like if I apply KLD(P|Q) and I get a value say 0.0002 nats, So Is this value (0.0002) significant or not.
Like as you said for Chi Squared test, we have a table the we can obtain the P-value to decide significance of the Chi Squared test result. Is there a table for KLD test?</blockquote>
</div></blockquote>
<div>
<div>I'm not sure what you mean by a random distribution, somehow I'm guessing you don't mean a random probability distribution, but perhaps you do. What is the null hypothesis in this case?  Usually we test goodness-of-fit with a probability distribution possibly with unknown parameters. I'm guessing you mean either 1) a random histogram drawn from some distribution or 2) you test versus many distributions and you are calling process by which you obtain these distributions random.</div>
<div>I have no idea what the vector-item pattern algorithm is, but you could certainly create a table for the KLD test. Do a simulation study where you compare two independent realizations from whatever mechanism is generating your random distributions. Calculate the KLD. Repeat this procedure so that you have an entire histogram of KLD values. Then compare the KLD obtained with your data histogram vs this histogram and calculate the percentage of KLD values that are larger.</div>
<blockquote>
<div>Everything I have read so far (mostly Wikipedia) uses divergence in a maximizing or minimizing context.  But I see both Chi squared and Divergence are listed as measures of "statistical difference".</div>
<div>Any thoughts on the pro/con of using Chi Squared vs. KL Divergence as a way to compare actual data with a random distribution to test the null hypothesis?</div></blockquote>
<div>The chi-squared statistic is very general in that it comes up whenever you have a test statistic that has a chi-squared distribution. I'm guessing perhaps you take your histogram and bin it somehow and then have counts in the bins. The sum of the squared differences between the observed and expected divided by expected which gives Pearson's goodness-of-fit test. This test is usually reasonable if bin counts are greater than 5. Why not try both and see when they agree and when they disagree? Perhaps that will give some insight into what is going on.</div>
<blockquote>
<div>I saw on the Wikipedia article that Kullback and Liebler originally defined the divergence as <img src="http://upload.wikimedia.org/wikipedia/en/math/7/f/f/7ff1f435ab3bc3cb4654ab665e7e57a2.png" alt=" D_{\mathrm{KL}}(P\|Q) + D_{\mathrm{KL}}(Q\|P)\, \!" /> to make it symmetric. Any idea why this fell out of favor and is no longer used?  (Just idle curiosity on my part for this question.)</div></blockquote>
<div>Perhaps because you often have a distribution you want to use a reference in which case the symmetry is unimportant?</div>
</div>
